[["0",{"pageContent":"\\--- title: \"API Endpoints\" description: \"Unlock the power of RunPod's API Endpoints, manage models without managing pods, and retrieve results via the status endpoint within 30 minutes for privacy protection; rate limits enforced per user.\" sidebar\\_position: 1 --- :::note You will need a RunPod API key which can be generated under your user settings. This API key will identify you for billing purposes, so guard it well! You must retrieve your results via the status endpoint within 30 minutes.","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["1",{"pageContent":"We don't keep your inputs or outputs longer than that to protect your privacy! ::: API Endpoints are Endpoints managed by RunPod that you can use to interact with your favorite models without managing the pods yourself. These Endpoints are available to all users. ## Overview The API Endpoint implementation works asynchronously as well as synchronous. Let's take a look at the differences between the two different implementations. ### Asynchronous Endpoints Asynchronous endpoints are useful for","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["2",{"pageContent":"long-running jobs that you don't want to wait for. You can submit a job and then check back later to see if it's done. When you fire an Asynchronous request with the API Endpoint, your input parameters are sent to our endpoint and you immediately get a response with a unique job ID. You can then query the response by passing the job ID to the status endpoint. The status endpoint will give you the job results when completed. ### Synchronous Endpoints Synchronous endpoints are useful for","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["3",{"pageContent":"short-running jobs that you want to wait for. You can submit a job and get the results back immediately. Let's take the Stable Diffusion v1 inference endpoint, for example. ### Start your job You would first make a request like the following (remember to replace the \"xxxxxx\"s with your real API key: \\`\\`\\`curl curl -X POST https://api.runpod.ai/v2/stable-diffusion-v1/run \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' \\ -d '{\"input\":","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["4",{"pageContent":"{\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}}' \\`\\`\\` You would get an immediate response that looks like this: \\`\\`\\`json { \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` In this example, your job ID would be \"c80ffee4-f315-4e25-a146-0f3d98cf024b\". You get a new one for each job, and it is a unique identifier for your job. ### Check the status of your job You haven't gotten any output, so you must make an additional call to","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["5",{"pageContent":"the status endpoint after some time. Your status endpoint uses the job ID to route to the correct job status. In this case, the status endpoint is \\`\\`\\`command https://api.runpod.ai/v1/stable-diffusion-v1/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \\`\\`\\` Note how the last part of the URL is your job ID. You could request that endpoint like so. Remember to use your API key for this request too! \\`\\`\\`curl curl","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["6",{"pageContent":"https://api.runpod.ai/v2/stable-diffusion-v1/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' \\`\\`\\` If your job hasn't been completed, you may get something that looks like this back: \\`\\`\\`json { \"delayTime\": 2624, \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\", \"input\": { \"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\" }, \"status\": \"IN\\_PROGRESS\" } \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["7",{"pageContent":"This means to wait a bit longer before you query the status endpoint again. ## Get completed job status Eventually, you will get the final results of your job. They would look something like this: \\`\\`\\`json { \"delayTime\": 123456, // (milliseconds) time in queue \"executionTime\": 1234, // (milliseconds) time it took to complete the job \"gpu\": \"24\", // gpu type used to run the job \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\", \"input\": { \"prompt\": \"a cute magical flying dog, fantasy art drawn by","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["8",{"pageContent":"disney concept artists\" }, \"output\": \\[ { \"image\": \"https://job.results1\", \"seed\": 1 }, { \"image\": \"https://job.results2\", \"seed\": 2 } ], \"status\": \"COMPLETED\" } \\`\\`\\` :::note You must retrieve your results via the status endpoint within 1 hour. We do not keep your inputs or outputs longer than that to protect your privacy! ::: ### Get your stuff Note how you don't get the images directly in the output. The output contains the URLs to the cloud storage that will let you download each image.","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["9",{"pageContent":"You've successfully generated your first images with our Stable Diffusion API! ### Rate Limit Rate limits are enforced on a per-user basis. If you exceed the rate limit, you will receive a \\`429\\` error code. \\`/run\\` - 1000 requests every 10s. \\`/runsync\\` - 2000 requests every 10s.","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["10",{"pageContent":"\\--- title: \"Overview\" slug: \"overview\" description: \"Create custom APIs for specific use cases with RunPod's serverless support. Bring your own container image and let us handle scaling and other aspects. Convert templates to API endpoints and invoke APIs using the 'run' endpoint, with asynchronous results tracked through the 'status' endpoint.\" sidebar\\_position: 1 --- You can create Custom APIs for your specific use cases. If you have a custom use case, you can use RunPod's custom API support","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_overview.md","loc":{"lines":{"from":1,"to":1}}}}],["11",{"pageContent":"to stand up your serverless API. You can bring your own container image, and RunPod will handle the scaling and other aspects. To create a custom API, you can navigate to the RunPod Serverless console and click \"New Template\" to add your container image. Once the template is created, you can convert it into an API endpoint by navigating to the APIs section and clicking \"New API\". Please note that running 1 minimum worker is great for debugging purposes, but you will be charged for that worker","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_overview.md","loc":{"lines":{"from":1,"to":1}}}}],["12",{"pageContent":"whether or not you are making requests to your endpoint. Once everything is set up, you can invoke your API using the \"run\" endpoint on your API dashboard. RunPod services are currently asynchronous, so you must use the \"status\" endpoint to get the status/results of each run using the ID present in the run response payload.","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_overview.md","loc":{"lines":{"from":1,"to":1}}}}],["13",{"pageContent":"\\--- title: \"Using Your API\" slug: \"using-your-api-copy\" excerpt: \"Okay! Now you have everything set up, but how do you use it?\" hidden: false metadata: image: \\[] robots: \"index\" createdAt: \"Thu Jul 27 2023 10:39:12 GMT+0000 (Coordinated Universal Time)\" updatedAt: \"Fri Oct 27 2023 13:53:56 GMT+0000 (Coordinated Universal Time)\" description: \"Invoke the RunPod API using the 'run' endpoint, which offers synchronous and asynchronous run mechanisms, and track job status using the 'status'","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["14",{"pageContent":"endpoint.\" --- Once everything above is configured, you will be able to invoke your API using the \"run\" endpoint on your API dashboard. Our services are currently asynchronous, so you must use the \"status\" endpoint to get the status/results of each run using the ID present in the run response payload. You can also pass in a webhook URL when invoking \"run\" within the JSON body. Our own APIs are built using the same tools, so you can take a look at the RunPod API overview. The only difference is","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["15",{"pageContent":"that your custom API endpoint only accepts requests using your own account's API key, not any RunPod API key. We offer two different kinds of run mechanisms: synchronous responses and asynchronous responses. ## Running your API ### /runsync <!-- dprint-ignore-start --> \\`\\`\\`curl cURL curl -X POST https://api.runpod.ai/v2//runsync \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' \\ -d '{\"input\": { here's a possible example request (taken","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["16",{"pageContent":"from our stable diffusion image) <!-- dprint-ignore-start --> \\`\\`\\`curl cURL curl -X POST https://api.runpod.ai/v2//runsync \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' \\ -d '{\"input\": {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}}' \\`\\`\\` \\`\\`\\`python # this requires the installation of runpod-python # with \\`pip install runpod-python\\` beforehand import runpod runpod.api\\_key =","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["17",{"pageContent":"\"xxxxxxxxxxxxxxxxxxxxxx\" # you can find this in settings endpoint = runpod.Endpoint(\"ENDPOINT\\_ID\") run\\_request = endpoint.run\\_sync( {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"} ) print(run\\_request) \\`\\`\\` <!-- dprint-ignore-end --> this should give a direct response if the code runs for \\\\< 90 seconds, or else it'll give a status response (which you can see below) \\*\\*Sample response\\*\\* \\`\\`\\`json { \"delayTime\": 123456, // (milliseconds) time in queue","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["18",{"pageContent":"\"executionTime\": 1234, // (milliseconds) time it took to complete the job \"gpu\": \"24\", // gpu type used to run the job \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\", \"input\": { \"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\" }, \"output\": \\[ { \"image\": \"https://job.results1\", \"seed\": 1 }, { \"image\": \"https://job.results2\", \"seed\": 2 } ], \"status\": \"COMPLETED\" } \\`\\`\\` ### /run <!-- dprint-ignore-start --> \\`\\`\\`curl cURL curl -X POST https://api.runpod.ai/v2//run","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["19",{"pageContent":"\\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' \\ -d '{\"input\": { here's a possible example request <!-- dprint-ignore-start --> \\`\\`\\`curl cURL curl -X POST https://api.runpod.ai/v2//run \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' \\ -d '{\"input\": {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}}' \\`\\`\\` \\`\\`\\`python # this requires the","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["20",{"pageContent":"installation of runpod-python # with \\`pip install runpod-python\\` beforehand import runpod runpod.api\\_key = \"xxxxxxxxxxxxxxxxxxxxxx\" # you can find this in settings endpoint = runpod.Endpoint(\"ENDPOINT\\_ID\") run\\_request = endpoint.run( {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"} ) print(run\\_request.status()) \\`\\`\\` <!-- dprint-ignore-end --> running your api via \\*\\*/run\\*\\* runs the code asynchronously, here's a sample response \\*\\*sample response","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["21",{"pageContent":"(for curl)\\*\\* \\`\\`\\`json { \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` ### /status \\*\\*Sample request\\*\\* <!-- dprint-ignore-start --> \\`\\`\\`Text cURL curl https://api.runpod.ai/v2//status/ \\`\\`\\` \\`\\`\\`python Start a job and return a status # this requires the installation of runpod-python # with \\`pip install runpod-python\\` beforehand import runpod runpod.api\\_key = \"xxxxxxxxxxxxxxxxxxxxxx\" # you can find this in settings endpoint =","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["22",{"pageContent":"runpod.Endpoint(\"ENDPOINT\\_ID\") run\\_request = endpoint.run( {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"} ) print(run\\_request.status()) \\`\\`\\` \\`\\`\\`python Get the status of a running job # Prerequisite: Install runpod-python using \\`pip install runpod-python\\` import runpod runpod.api\\_key = \"xxxxxxxxxxxxxxxxxxxxxx\" # Replace with your API key client = runpod.endpoint.runner.RunPodClient() job = runpod.endpoint.Job( endpoint\\_id=\"your\\_endpoint\\_id\",","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["23",{"pageContent":"job\\_id=\"your\\_job\\_id\", client=client ) print(job.status()) \\`\\`\\` <!-- dprint-ignore-end --> \\*\\*sample response (for job in progress)\\*\\* \\`\\`\\`json JSON { \"delayTime\": 2624, \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\", \"input\": { \"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\" }, \"status\": \"IN\\_PROGRESS\" } \\`\\`\\` \\*\\*sample response (for completed job)\\*\\* \\`\\`\\`json JSON { \"delayTime\": 123456, // (milliseconds) time in queue \"executionTime\": 1234, //","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["24",{"pageContent":"(milliseconds) time it took to complete the job \"gpu\": \"24\", // gpu type used to run the job \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\", \"input\": { \"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\" }, \"output\": \\[ { \"image\": \"https://job.results1\", \"seed\": 1 }, { \"image\": \"https://job.results2\", \"seed\": 2 } ], \"status\": \"COMPLETED\" } \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":1}}}}],["25",{"pageContent":"\\--- title: Installing runpodctl description: \"Get started with runpodctl, an open-source CLI, to work with Pods and RunPod projects. Install and configure the tool, then verify the installation and API key setup to start using runpodctl.\" sidebar\\_position: 1 --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; runpodctl is an \\[open-source command-line interface (CLI)]\\(https://github.com/runpod/runpodctl). You can use runpodctl to work with Pods and RunPod projects. When","metadata":{"source":"/runpod-docs/docs/cli/install-runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["26",{"pageContent":"you create a Pod, it comes with runpodctl installed and configured with a Pod-scoped API key. You can also run runpodctl locally. To install runpodctl on your local machine, run the appropriate command for your operating system. \\`\\`\\`bash brew install runpod/runpodctl/runpodctl \\`\\`\\` \\`\\`\\`bash wget -qO- cli.runpod.net | sudo bash \\`\\`\\` \\`\\`\\`bash wget https://github.com/runpod/runpodctl/releases/download/v1.12.3/runpodctl-windows-amd64.exe -O runpodctl.exe \\`\\`\\` ## Configuring runpodctl","metadata":{"source":"/runpod-docs/docs/cli/install-runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["27",{"pageContent":"Before you can use runpodctl, you must configure an API key. To create a new API key, complete the following steps: 1. In the web interface, go to your \\[\\*\\*Settings\\*\\*]\\(https://www.runpod.io/console/user/settings). 2. Expand \\*\\*API Keys\\*\\* and click the \\*\\*+ API Key\\*\\* button. 3. Select \\*\\*Read\\*\\* or \\*\\*Read & Write\\*\\* permissions. 4. Click \\*\\*Create\\*\\*. :::note Keep your API key secret. Anyone with the key can gain full access to your account. ::: Now that you've created an API","metadata":{"source":"/runpod-docs/docs/cli/install-runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["28",{"pageContent":"key, run the following command to add it to runpodctl: \\`\\`\\`bash runpodctl config --apiKey your-api-key \\`\\`\\` You should see something similar to the following output: \\`\\`\\`bash saved apiKey into config file: /Users/runpod/.runpod/config.toml \\`\\`\\` Now that you've configured an API key, check that runpodctl installed successfully. Run the following command: \\`\\`\\`bash runpodctl version \\`\\`\\` You should see which version is installed. \\`\\`\\`bash runpodctl v1.13.0 \\`\\`\\` If at any point you","metadata":{"source":"/runpod-docs/docs/cli/install-runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["29",{"pageContent":"need help with a command, you can use the \\`--help\\` flag to see documentation on the command you're running. \\`\\`\\`bash runpodctl --help \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/cli/install-runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["30",{"pageContent":"\\--- title: Get started sidebar\\_position: 2 description: \"Get the IP address of your machine and deploy your code to the RunPod platform with this step-by-step tutorial. Learn how to set up a project environment, run a development server, and interact with your code using the RunPod API.\" --- In this tutorial, we'll explore how to get the IP address of the machine your code is running on and deploy your code to the RunPod platform. You will get the IP address of your local machine, the","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["31",{"pageContent":"development server, and the Serverless Endpoint's server. By the end, you'll have a solid understanding of how to set up a project environment, interact with your code, and deploy your code to a Serverless Endpoint on the RunPod platform. While this project is scoped to getting the IP address of the machine your code is running on, you can use the RunPod platform to deploy any code you want. For larger projects, bundling large packages a Docker image and making code changes requires multiple","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["32",{"pageContent":"steps. With a RunPod development server, you can make changes to your code and test them in a live environment without having to rebuild a Docker image or redeploy your code to the RunPod platform. This tutorial takes advantage of making updates to your code and testing them in a live environment. Let's get started by setting up the project environment. ## Prerequisites Before we begin, you'll need the following: - RunPod CLI - Python 3.8 or later ## Step 1. Set up the project environment In","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["33",{"pageContent":"this first step, you'll set up your project environment using the RunPod CLI. Set your API key in the RunPod CLI configuration file. \\`\\`\\`bash runpodctl config --apiKey $(RUNPOD\\_API\\_KEY) \\`\\`\\` Next, use the RunPod CLI \\`project create\\` command to create a new directory and files for your project. \\`\\`\\`bash runpodctl project create \\`\\`\\` Select the \\*\\*Hello World\\*\\* project and follow the prompts on the screen. ## Step 2. Write the code Next, you'll write the code to get the IP address","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["34",{"pageContent":"of the machine your code is running on. Use \\`httpbin\\` to retrieve the IP address and test the code locally. Change directories to the project directory and open the \\`src/handler.py\\` file in your text editor. \\`\\`\\`bash cd my\\_ip \\`\\`\\` The current code is boiler plate text. Replace the code with the following: \\`\\`\\`python import runpod import requests def get\\_my\\_ip(job): response = requests.get('https://httpbin.org/ip') return response.json()\\['origin'] runpod.serverless.start({\"handler\":","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["35",{"pageContent":"get\\_my\\_ip}) \\`\\`\\` This uses \\`httpbin\\` to get the IP address of the machine your code is running on. Run this code locally to get the IP address of your machine, for example: \\`\\`\\`bash python3 src/handler.py --test\\_input '{\"input\": {\"prompt\": \"\"}}' \\`\\`\\` \\`\\`\\`text --- Starting Serverless Worker | Version 1.6.1 --- INFO | test\\_input set, using test\\_input as job input. DEBUG | Retrieved local job: {'input': {'prompt': ''}, 'id': 'local\\_test'} INFO | local\\_test | Started. DEBUG |","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["36",{"pageContent":"local\\_test | Handler output: 174.21.174.xx DEBUG | local\\_test | run\\_job return: {'output': '174.21.174.xx'} INFO | Job local\\_test completed successfully. INFO | Job result: {'output': '174.21.174.xx'} INFO | Local testing complete, exiting.\\` \\`\\`\\` This testing environment works for smaller projects, but for larger projects, you will want to use the RunPod CLI to deploy your code to run on the RunPod platform. In the next step, you'll see how to deploy your code to the RunPod platform. ##","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["37",{"pageContent":"Step 3. Run a development server Now let's run the code you've written using RunPod's development server. You'll start a development server using the RunPod CLI \\`project dev\\` command. RunPod provides a development server that allows you to quickly make changes to your code and test these changes in a live environment. You don't need to rebuild a Docker image or redeploy your code to the RunPod platform just because you made a small change or added a new dependency. To run a development server,","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["38",{"pageContent":"use the RunPod CLI \\`project dev\\` command and select a Network volume. \\`\\`\\`bash runpodctl project dev \\`\\`\\` This starts a development server on a Pod. The logs shows the status of your Pod as well as the port number your Pod is running on. The development server watches for changes in your code and automatically updates the Pod with changes to your code and files like \\`requirements.txt\\`. When the Pod is running you should see the following logs: \\`\\`\\`text Connect to the API server at:","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["39",{"pageContent":"\\[lug43rcd07ug47] > https://lug43rcd07ug47-8080.proxy.runpod.net \\[lug43rcd07ug47] \\[lug43rcd07ug47] Synced venv to network volume \\[lug43rcd07ug47] --- Starting Serverless Worker | Version 1.6.2 --- \\`\\`\\` The \\`\\[lug43rcd07ug47]\\` is your Worker Id. The \\`https://lug43rcd07ug47-8080.proxy.runpod.net\\` is the URL to access your Pod with the 8080 port exposed. You can interact with this URL like you would any other Endpoint. ## Step 4. Interact with your code In this step, you'll interact with","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["40",{"pageContent":"your code by running a \\`curl\\` command to fetch the IP address from the development server. You'll learn how to include dependencies in your project and how to use the RunPod API to run your code. You might have noticed that the function to get an IP address uses a third-party dependency \\`requests\\`. This means by default it's not included in Python or the RunPod environment. To include this dependency, you need to add it to the \\`requirements.txt\\` file in the root of your project. \\`\\`\\`text","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["41",{"pageContent":"runpod requests \\`\\`\\` When you save your file, notice that the development server automatically updates the Pod with the dependencies. During this sync, your Pod is unable to receive requests. Wait until you see the following logs: \\`\\`\\`text Restarted API server with PID: 701 --- Starting Serverless Worker | Version 1.6.2 --- INFO | Starting API server. \\`\\`\\` Now you can interact with your code. While the Pod is still running, create a new terminal session and run the following command:","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["42",{"pageContent":"\\`\\`\\`bash curl -X 'POST' \\ 'https://${YOUR\\_ENDPOINT}-8080.proxy.runpod.net/runsync' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"input\": {} }' \\`\\`\\` This command uses the \\`runsync\\` method on the RunPod API to run your code synchronously. The previous command returns a response: \\`\\`\\`text { \"id\": \"test-9613c9be-3fed-401f-8cda-6b5f354417f8\", \"status\": \"COMPLETED\", \"output\": \"69.30.85.70\" } \\`\\`\\` The output is the IP address of the Pod your code is running","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["43",{"pageContent":"on and not your local machine. Even though you're executing code locally, you can see that it's running on a Pod. Now, what if you wanted this function to run as a Serverless Endpoint? Meaning, you didn't want to keep the Pod running all the time. You only wanted it to turn on when you sent a request to it. In the next step, you'll learn to deploy your code to the Serverless platform and get the IP address of that machine. ## Step 5. Deploy your code Now that you've tested your code in the","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["44",{"pageContent":"development environment, you'll deploy it to the RunPod platform using the RunPod CLI \\`project deploy\\` command. This will make your code available as a \\[Serverless Endpoint]\\(/serverless/endpoints/overview). Stop the development server by pressing \\`Ctrl + C\\` in the terminal. To deploy your code to the RunPod platform, use the RunPod CLI \\`project deploy\\` command. \\`\\`\\`bash runpodctl project deploy \\`\\`\\` Select your network volume and wait for your Endpoint to deploy. After deployment,","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["45",{"pageContent":"you will see the following logs: \\`\\`\\`text The following URLs are available: - https://api.runpod.ai/v2/${YOUR\\_ENDPOINT}/runsync - https://api.runpod.ai/v2/${YOUR\\_ENDPOINT}/run - https://api.runpod.ai/v2/${YOUR\\_ENDPOINT}/health \\`\\`\\` :::note You can follow the logs to see the status of your deployment. You may notice that the logs show the Pod being created and then the Endpoint being created. ::: ## Step 6. Interact with your Endpoint Finally, you'll interact with your Endpoint by running","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["46",{"pageContent":"a \\`curl\\` command to fetch the IP address from the deployed Serverless function. You'll see how your code runs as expected and tested in the development environment. When the deployment completes, you can interact with your Endpoint as you would any other Endpoint. Replace the previous Endpoint URL and specify the new one and add your API key. Then, run the following command: \\`\\`\\`bash curl -X 'POST' \\ 'https://api.runpod.ai/v2/${YOUR\\_ENDPOINT}/runsync' \\ -H 'accept: application/json' \\ -H","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["47",{"pageContent":"'authorization: ${YOUR\\_API\\_KEY}' \\ -H 'Content-Type: application/json' \\ -d '{ \"input\": {} }' \\`\\`\\` The previous command returns a response: \\`\\`\\`text { \"delayTime\": 249, \"executionTime\": 88, \"id\": \"sync-b2188a79-3f9f-4b99-b4d1-18273db3f428-u1\", \"output\": \"69.30.85.69\", \"status\": \"COMPLETED\" } \\`\\`\\` The output is the IP address of the Pod your code is running on. ## Conclusion In this tutorial, you've learned how to get the IP address of the machine your code is running on and deploy your","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["48",{"pageContent":"code to the RunPod platform. You've also learned how to set up a project environment, run a development server, and interact with your code using the RunPod API. With this knowledge, you can now use this code as a Serverless Endpoint or continue developing your project, testing, and deploying it to the RunPod platform.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["49",{"pageContent":"\\--- title: Managing Projects description: Create and deploy projects on RunPod's infrastructure with ease, using commands like 'runpod project create' and 'runpodctl project deploy' to develop, deploy, and build your project as a serverless endpoint or Dockerfile. sidebar\\_position: 2 --- Projects enable you to develop and deploy endpoints entirely on RunPod's infrastructure. ## Create a project A RunPod project is a folder with everything you need to run a development session on a Pod. 1. To","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":1,"to":1}}}}],["50",{"pageContent":"create a new project, run the following command. \\`\\`\\`command runpod project create \\`\\`\\` 2. Select a starter project. Starter projects include preliminary settings for different kinds of project environments, such as LLM or image diffusion development. 3. Check the \\[base image]\\(https://github.com/runpod/containers/tree/main/official-templates/base) for included dependencies. 4. (Optional) If you need dependencies that are not included or added by your starter project, add them to the","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":1,"to":1}}}}],["51",{"pageContent":"generated \\`requirements.txt\\` file. 5. Save your changes. You've customized your project, and now you're ready to run a development session. ## Run a development session A development session is the active connection between your local environment and the project environment on your Pod. During a development session, local changes to your project propagate to the project environment in real time. 1. To start a development session, run the following command. \\`\\`\\`command runpodctl project dev","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":1,"to":1}}}}],["52",{"pageContent":"\\`\\`\\` 2. When you're done developing, press \\`ctrl\\` + \\`c\\` to end the session. Your Pod will terminate automatically when the session ends. :::tip You can resume developing at any time by running \\`runpodctl project dev\\` again. ::: Now that you've developed your project, you can deploy an endpoint directly to RunPod or build a Dockerfile to create a portable image. ## Deploy a project When you deploy a project, RunPod creates a serverless endpoint with access to saved project data on your","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":1,"to":1}}}}],["53",{"pageContent":"network volume. To deploy a project, run the following command. \\`\\`\\`command runpodctl project deploy \\`\\`\\` Your project is now deployed to a Serverless Endpoint. You can interact with this Endpoint like you would any other Serverless Endpoint. For more information, see \\[Endpoints]\\(/serverless/endpoints/overview). ## Build a project You have the option to build your project instead of deploying it as an endpoint. When you build a project, RunPod emits a Dockerfile. To build a project, run","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":1,"to":1}}}}],["54",{"pageContent":"the following command. \\`\\`\\`command runpodctl project build \\`\\`\\` You can use the generated Dockerfile to build an image, then deploy the image to any API server.","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":1,"to":1}}}}],["55",{"pageContent":"\\--- title: Overview description: \"Streamline your development process with RunPod's Dockerless workflow, enabling you to deploy and manage endpoints without Docker or container image management, perfect for rapid prototyping and testing.\" sidebar\\_position: 1 --- RunPod projects enable you to develop and deploy endpoints entirely on RunPod's infrastructure. That means you can get a worker up and running without knowing Docker or needing to structure handler code. This Dockerless workflow also","metadata":{"source":"/runpod-docs/docs/cli/projects/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["56",{"pageContent":"streamlines the development process: you don't need to rebuild and push container images or edit your endpoint to use the new image each time you change your code. To get started, see \\[Managing Projects]\\(/cli/projects/manage-projects).","metadata":{"source":"/runpod-docs/docs/cli/projects/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["57",{"pageContent":"\\--- title: API keys description: \"Generate API keys with Read and Write or Read permission to authenticate requests to RunPod. Create and revoke keys from the console under Settings > API Keys.\" sidebar\\_position: 4 --- API keys authenticate requests to RunPod. You can generate an API key with \\*\\*Read and Write\\*\\* permission or \\*\\*Read\\*\\* permission. ## Generate To create an API key: 1. From the console, select \\*\\*Settings\\*\\*. 2. Under \\*\\*API Keys\\*\\*, choose \\*\\*+ API Keys\\*\\*. 3.","metadata":{"source":"/runpod-docs/docs/get-started/api-keys.md","loc":{"lines":{"from":1,"to":1}}}}],["58",{"pageContent":"Select the permission and choose \\*\\*Create\\*\\*. :::note Once your API key is generated, keep it secure. Treat it like a password and avoid sharing it in insecure environments. ::: ## Revoke To delete an API key: 1. From the console, select \\*\\*Settings\\*\\*. 2. Under \\*\\*API Keys\\*\\*, select the trash can icon and select \\*\\*Yes\\*\\*.","metadata":{"source":"/runpod-docs/docs/get-started/api-keys.md","loc":{"lines":{"from":1,"to":1}}}}],["59",{"pageContent":"\\--- title: Billing information description: \"Learn about RunPod's billing and payment methods, including credit cards, crypto payments, and business invoicing options, with guidance on common issues and troubleshooting tips.\" sidebar\\_position: 2 --- All billing, including per-hour compute and storage billing, is charged per minute. For more information on billing questions, see \\[Billing FAQ]\\(docs/references/faq/faq.md#billing). ### Payment methods RunPod accepts several payment methods for","metadata":{"source":"/runpod-docs/docs/get-started/billing-information.md","loc":{"lines":{"from":1,"to":1}}}}],["60",{"pageContent":"funding your account: 1. \\*\\*Credit Card\\*\\*: You can use your credit card to fund your RunPod account. However, be aware that card declines are more common than you might think, and the reasons for them might not always be clear. If you're using a prepaid card, it's recommended to deposit in transactions of at least $100 to avoid unexpected blocks due to Stripe's minimums for prepaid cards. For more information, review \\[cards accepted by","metadata":{"source":"/runpod-docs/docs/get-started/billing-information.md","loc":{"lines":{"from":1,"to":1}}}}],["61",{"pageContent":"Stripe]\\(https://stripe.com/docs/payments/cards/supported-card-brands?ref=blog.runpod.io). <!-- [source](https://blog.runpod.io/how-to-manage-funding-your-runpod-account#my-card-keeps-getting-declined) --> 1\\. \\*\\*Crypto Payments\\*\\*: RunPod also accepts crypto payments. It's recommended to set up a \\[crypto.com]\\(https://crypto.com/?ref=blog.runpod.io) account and go through any KYC checks they may require ahead of time. This provides an alternate way of funding your account in case you run","metadata":{"source":"/runpod-docs/docs/get-started/billing-information.md","loc":{"lines":{"from":1,"to":1}}}}],["62",{"pageContent":"into issues with card payment. <!-- [source](https://blog.runpod.io/how-to-manage-funding-your-runpod-account#crypto-payments) --> 3\\. \\*\\*Business Invoicing\\*\\*: For large transactions (over $5,000), RunPod offers business invoicing through ACH, credit card, Coinbase, and local and international wire transfers. <!-- [source](https://blog.runpod.io/how-to-manage-funding-your-runpod-account#invoicing) --> If you're having trouble with your card payments, you can contact \\[RunPod","metadata":{"source":"/runpod-docs/docs/get-started/billing-information.md","loc":{"lines":{"from":1,"to":1}}}}],["63",{"pageContent":"support]\\(https://www.runpod.io/contact) for assistance.","metadata":{"source":"/runpod-docs/docs/get-started/billing-information.md","loc":{"lines":{"from":1,"to":1}}}}],["64",{"pageContent":"\\--- title: Connect to RunPod description: \"Interact with RunPod through multiple interfaces: web, CLI, and SDKs. Access the web interface at runpod.io/console/login, use the CLI runpodctl for management and development, or leverage SDKs for GraphQL, JavaScript, and Python programming languages.\" sidebar\\_position: 5 --- There are many ways to interact with RunPod: - Web interface - CLI - SDKs ## Web interface \\[Create an account]\\(/get-started/connect-to-runpod) and then log into the web","metadata":{"source":"/runpod-docs/docs/get-started/connect-to-runpod.md","loc":{"lines":{"from":1,"to":1}}}}],["65",{"pageContent":"interface at the following address: \\[runpod.io/console/login]\\(https://www.runpod.io/console/login). ## CLI You can use RunPod's CLI \\[runpodctl]\\(https://github.com/runpod/runpodctl) to manage Pods and for development. All Pods come with \\`runpodctl\\` installed with a Pod-scoped API key, which makes managing your Pods easier through the command line. ## SDKs RunPod provides SDKs for the following programming languages: - \\[GraphQL]\\(/sdks/graphql/manage-pods) -","metadata":{"source":"/runpod-docs/docs/get-started/connect-to-runpod.md","loc":{"lines":{"from":1,"to":1}}}}],["66",{"pageContent":"\\[JavaScript]\\(/sdks/javascript/overview) - \\[Python]\\(/sdks/python/overview)","metadata":{"source":"/runpod-docs/docs/get-started/connect-to-runpod.md","loc":{"lines":{"from":1,"to":1}}}}],["67",{"pageContent":"\\--- title: Manage accounts sidebar\\_position: 1 description: \"Create an account or get invited by a team member to use RunPod, then convert personal to team account or invite others to join, with various role options including Basic, Billing, Dev, and Admin for customized access and permissions.\" --- You will need to create an account or get invited by a team member to use RunPod. ## Create an account Sign up for an account at \\[RunPod.io]\\(https://www.runpod.io). ### Convert personal account","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":1,"to":1}}}}],["68",{"pageContent":"to a team account You can convert a personal account to a team account at anytime. 1. From the console, select \\*\\*Convert to Team Account\\*\\*. 2. Set a team name and confirm the conversion. ## Get invited by a team member 1. Accept the link sent by a team member. 2. Select \\*\\*Join Team\\*\\*. For information on how to send an invitation, see \\[Invite a user]\\(#invite-a-user). ### Invite a user To invite a user to your team on RunPod, you'll need a team account. 1. To invite users, navigate to","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":1,"to":1}}}}],["69",{"pageContent":"your Team page, and select the \"Invite New Member\" button at the top of the \"Members\" section. 2. Select the role you want to provide the user. 3. After you create the invite, you can copy the invite link from the pending invites section. 4. Send a user your invite link and they will be able to go to it to join your team. ## Role types The following roles and permissions are available to you: ### Basic role Limited access, primarily for account usage and existing pod connections.","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":1,"to":1}}}}],["70",{"pageContent":"\\*\\*Permissions\\*\\*: - Use the account. - Connect to and use Pods. - Restricted from viewing billing information. - No permissions to start, stop, or create Pods. ### Billing role Specialized role focused on managing billing aspects. \\*\\*Permissions\\*\\*: - Access and manage billing information. - Restricted from other account features and Pod management. ### Dev role Enhanced privileges suitable for development tasks. \\*\\*Permissions\\*\\*: - All \"Basic\" role permissions (use account, connect to","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":1,"to":1}}}}],["71",{"pageContent":"Pods). - Start, stop, and create Pods. - No access to billing information. ### Admin role Full control over the account, ideal for administrators. \\*\\*Permissions\\*\\*: - Complete access to all account features and settings. - Manage billing and access billing information. - Start, stop, and create Pods. - Modify account settings and user permissions. - Full control over all account resources and users. ## Audit logs RunPod includes audit logs to help you understand which actions were used. Go to","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":1,"to":1}}}}],["72",{"pageContent":"the \\[Audit logs]\\(https://www.runpod.io/console/user/audit-logs) settings. You can view and filter the audit logs by date range, user, resource, resource ID, and action.","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":1,"to":1}}}}],["73",{"pageContent":"\\--- title: Referral programs sidebar\\_position: 5 description: \"Learn how to earn additional revenue with RunPod's referral programs and template program, offering credits for referring users and template creators.\" --- RunPod offers two referral programs and a template program that allow users to earn additional revenue in the form of RunPod Credits. This document provides an overview of these programs and instructions on how to participate. ## Referral Programs ### 1. Serverless Referral","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":1,"to":1}}}}],["74",{"pageContent":"Program (BETA) The Serverless Referral Program rewards both the referrer and the referred user with RunPod Credits when the referred user spends a certain amount on Serverless. #### Rewards - Referrer: Earns $500 in RunPod Credits - Referred User: Earns $500 in RunPod Credits #### Eligibility - The referred user must spend $1000 on Serverless. ### 2. Referral Program (BETA) The Referral Program allows users to earn a percentage of the money spent by their referred users for the lifetime of their","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":1,"to":1}}}}],["75",{"pageContent":"account. #### Rewards - Referrer: Earns 2% in RunPod Credits for every penny spent by the referred user. #### Example - If 20 referrals spend $100 each, the referrer earns $40. #### Eligibility - The referrer must have at least 25 referrals and a minimum of $500 in referral spend. - To be eligible, \\[contact RunPod]\\(https://contact.runpod.io/hc/en-us/requests/new) once these criteria are met. ## Template Program (BETA) The Template Program allows users to earn a percentage of the money spent by","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":1,"to":1}}}}],["76",{"pageContent":"users who use their Pod Template. #### Rewards - Template Creator: Earns 1% for runtime in RunPod Credits for every penny spent using their template. #### Example - If 20 users use a Pod Template at $0.54/hr for a week, the template creator earns $18.14. #### Eligibility - The template must have at least 1 day of runtime. ## How to Participate ### Serverless Referral Program - Follow the instructions on the Serverless Referral Program page to refer users. ### Referral Program 1. Access your","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":1,"to":1}}}}],["77",{"pageContent":"\\[referral dashboard]\\(https://www.runpod.io/console/user/referrals). 2. Locate your unique referral link. For example \\`https://runpod.io?ref=5t99c9je\\`. 3. Share your referral link with potential users. ## Support If you have any questions or need assistance with the referral or template programs, \\[please contact]\\(https://contact.runpod.io/hc/en-us/requests/new) the RunPod support team. RunPod allows referrers to keep the earnings made before activation once they meet the eligibility","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":1,"to":1}}}}],["78",{"pageContent":"criteria and are accepted into the program.","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":1,"to":1}}}}],["79",{"pageContent":"\\## Community Cloud GPU instances connect individual compute providers to consumers through a vetted, secure peer-to-peer system. ## Datacenter A data center is a secure location where RunPod's cloud computing services, such as Secure Cloud and GPU Instances, are hosted. These data centers are equipped with redundancy and data backups to ensure the safety and reliability of your data.","metadata":{"source":"/runpod-docs/docs/glossary.md","loc":{"lines":{"from":1,"to":1}}}}],["80",{"pageContent":"\\--- title: \"Burn Testing\" description: \"Before listing a machine on the RunPod platform, thoroughly test it with a burn test, verifying memory, CPU, and disk capabilities, and ensure compatibility with popular templates by self-renting the machine after verifying its performance.\" --- Machines should be thoroughly tested before they are listed on the RunPod platform. Here is a simple guide to running a burn test for a few days. Stop the RunPod agent by running: \\`\\`\\`command sudo systemctl stop","metadata":{"source":"/runpod-docs/docs/hosting/burn-testing.md","loc":{"lines":{"from":1,"to":1}}}}],["81",{"pageContent":"runpod \\`\\`\\` Then you can kick off a gpu-burn run by typing: \\`\\`\\`command docker run --gpus all --rm jorghi21/gpu-burn-test 172800 \\`\\`\\` You should also verify that your memory, CPU, and disk are up to the task. You can use the \\[ngstress library]\\(https://wiki.ubuntu.com/Kernel/Reference/stress-ngstress) to accomplish this. When everything is verified okay, start the RunPod agent again by running \\`\\`\\`command sudo systemctl start runpod \\`\\`\\` Then, on your \\[machine","metadata":{"source":"/runpod-docs/docs/hosting/burn-testing.md","loc":{"lines":{"from":1,"to":1}}}}],["82",{"pageContent":"dashboard]\\(https://www.runpod.io/console/host/machines), self rent your machine to ensure it's working well with most popular templates.","metadata":{"source":"/runpod-docs/docs/hosting/burn-testing.md","loc":{"lines":{"from":1,"to":1}}}}],["83",{"pageContent":"\\--- title: \"Maintenance and reliability\" description: \"Schedule maintenance with at least one-week notice to minimize disruptions. Automated reminders sent to users. RunPod aims for 99.99% uptime, calculating reliability with a 10-minute buffer and rolling 30-day window. Excessive maintenance may result in penalties.\" --- ## Maintenance Hosts must currently schedule maintenance at least one week in advance and are able to program flash maintenance in the case their server is unrented. Users","metadata":{"source":"/runpod-docs/docs/hosting/maintenance-and-reliability.md","loc":{"lines":{"from":1,"to":1}}}}],["84",{"pageContent":"will get email reminders of upcoming maintenance that will occur on their active pods. Please contact RunPod on Discord or Slack if you are scheduling maintenance on more than a few machines so that we are aware of any major impacts to our customers. Here are some things to keep in mind. - Uptime/reliability will not be affected during scheduled maintenance. - ALL other events that may impact customer workloads will result in a reliability score decrease. This includes unlisted machines. - All","metadata":{"source":"/runpod-docs/docs/hosting/maintenance-and-reliability.md","loc":{"lines":{"from":1,"to":1}}}}],["85",{"pageContent":"machines that have maintenance scheduled will be automatically unlisted 4 days prior to the scheduled maintenance start time to minimize disruption for clients. - Excessive maintenance will result in further penalties. - You are allowed to bring down machines that have active users on them provided that you are in a maintenance window. ## Reliability calculations RunPod aims to partner with datacenters that offer \\*\\*99.99%\\*\\* uptime. Reliability is currently calculated as follows: \\`( total","metadata":{"source":"/runpod-docs/docs/hosting/maintenance-and-reliability.md","loc":{"lines":{"from":1,"to":1}}}}],["86",{"pageContent":"minutes + small buffer ) / total minutes in interval\\` This means that if you have 30 minutes of network downtime on the first of the month, your reliability will be calculated as: \\`( 43200 - 30 + 10 ) / 43200 = 99.95%\\` Based on approximately 43200 minutes per month and a 10 minute buffer. We include the buffer because we do incur small single-minute uptime dings once in a while due to agent upgrades and such. It will take an entire month to regenerate back to 100% uptime given no further","metadata":{"source":"/runpod-docs/docs/hosting/maintenance-and-reliability.md","loc":{"lines":{"from":1,"to":1}}}}],["87",{"pageContent":"downtimes in the month, considering it it calculated based on a 30 days rolling window. Machines with less than \\*\\*98%\\*\\* reliability are \\*\\*automatically removed\\*\\* from the available GPU pool and can only be accessed by clients that already had their data on it.","metadata":{"source":"/runpod-docs/docs/hosting/maintenance-and-reliability.md","loc":{"lines":{"from":1,"to":1}}}}],["88",{"pageContent":"\\--- title: Overview description: \"RunPod offers GPU hosting opportunities through proprietary servers and community collaboration, with a 24% service fee, minimum bid pricing, and KYC verification for trusted hosts with at least 20 GPUs, offering customized dashboard management and revenue insights.\" sidebar\\_position: 1 --- ## RunPod GPU hosting opportunity RunPod offers a diverse range of GPUs, made possible through proprietary servers and collaboration with trusted community members. If","metadata":{"source":"/runpod-docs/docs/hosting/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["89",{"pageContent":"you're interested in integrating your hardware into the RunPod ecosystem, follow the steps below. ## How to join as a host 1. Check Eligibility: Make sure you adhere to our \\[minimum requirements]\\(/hosting/partner-requirements). 2. Connect with us: Currently, we onboard hosts through a manual vetting process. If you have high-quality machines that satisfy our hosting requirements, and at least 20 GPUs in total, please fill out \\[this","metadata":{"source":"/runpod-docs/docs/hosting/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["90",{"pageContent":"form]\\(https://share.hsforms.com/1GYpMeNlSQc6n11toAlgNngecykq). ## Additional hosting information - Service Fee: RunPod charges a 24% service fee. This encompasses: - Approximately 4% for Stripe payment fees. - 2% for our referral program and 1% for our template program. For more information, see \\[Refer a friend]\\(https://www.runpod.io/refer-a-friend). - Pricing: While GPU on-demand prices are consistent, hosts can define a minimum bid price for spot rental. Even though we try as much as","metadata":{"source":"/runpod-docs/docs/hosting/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["91",{"pageContent":"possible to maintain stable prices over time, we need to adjust to market trends. - Safety & Trust: We mandate KYC (Know Your Customer) verification for all hosts to safeguard our users and combat fraud. For larger providers, we require a Provider Agreement and a Service Level Agreement to be completed. - Hosting Experience: As one of our trusted providers, you have access to a fully customized dashboard to manage your resources that you can leverage to deploy hardware and plan your expansion. -","metadata":{"source":"/runpod-docs/docs/hosting/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["92",{"pageContent":"Rental Rates: We do not make utilization data publicly available. However, we are more than happy to provide statistics and information about popular GPU models when directly discussing with you. Furthermore, lots of different variables can impact occupancy. We are more than happy to provide you with in-dept data about how different hardware quality levels can impact your revenue.","metadata":{"source":"/runpod-docs/docs/hosting/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["93",{"pageContent":"\\--- title: \"Requirements\" description: \"Meet the minimum requirements for a secure and scalable AI cloud infrastructure: Ubuntu Server 22.04 LTS, remote SSH access, Nvidia drivers, CUDA Toolkit, and more.\" --- The following requirements are minimal and are subject to change. ## Software specifications - Ubuntu Server 22.04 LTS: - Basic Linux proficiency. - Ability to remotely connect via SSH. ### Operating system - Ubuntu Server 22.04 LTS - Use the same file as 22.04, but select HWE during","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["94",{"pageContent":"install. - That way, Kernel 6.5.0-15 is installed (please replace by any more recent production version if available). ### BIOS - For non-VM systems, make sure IOMMU is disabled in the BIOS. - Another good practice is to update the server BIOS to the latest stable version when facing compatibility issues. ### Drivers - Nvidia drivers 550.54.15 (please replace by any more recent production version if available). - Never use beta or new feature branch drivers except if you have been instructed","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["95",{"pageContent":"otherwise. - CUDA 12.4 (please replace by any more recent production version if available). - Nvidia Persistence should be activated for GPUs of 48 GB or more. ### HGX SXM Systems - Nvidia Fabric Manager needs to be installed, activated, running, and tested. - Mandatory: Fabric Manager version = Nvidia drivers version = Kernel drivers headers. - A p2p bandwidth test should be passed. - CUDA Toolkit, Nvidia NSCQ and Nvidia DCGM need to be installed. - Ensure the topology of the NVLINK switch is","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["96",{"pageContent":"right by leveraging nvidia-smi and dcgmi. - Ensure the SXM is performing well leveraging the dcgmi diagnostic tool. ## Minimal Secure Cloud Specifications ### GPU Models - Latest Nvidia GPUs are required with models of at least 30xx or RTX A4000 or more recent. - Demand is highest for SXM 80 GB, PCIe 80 GB, Ada 6000 48 GB, Ada 5000 32 GB, 4090 24 GB, L4 24 GB, A5000 24 GB, and Ada 4000. ### Quantity - \\*\\*Option 1\\*\\*: At least 100 GPUs in total, each with a minimum of 12 GB VRAM. - \\*\\*Option","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["97",{"pageContent":"2\\*\\*: At least 32 GPUs in total, each with a minimum of 80 GB of VRAM. We also require 2 GPU per server at minimum. 8x configuration is recommended. ### CPU - Minimum of 4 Physical CPU Cores per GPU + 2 for system operations. - You should prioritize CPU core clock as fast as possible over more providing more cores. - For example, a 24-cores CPU clocking at 5.0 GHz is preferred to a 128-cores CPU clocking at 3.0 GHz for a 4x GPU configuration. - Genoa CPU are often a good option for these","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["98",{"pageContent":"reasons. ### Bus Bandwidth - Minimum banwitdh per GPU is PCIe 3.0 x16 for 8 GB | 10 GB | 12 GB | 16 GB GPUs. - Minimum banwitdh per GPU is PCIe 4.0 x16 for 20 GB | 24 GB | 32 GB | 40 GB | 48 GB | 80 GB GPUs. - PCIe 5.0 x16 is recommended for 80 GB GPUs. ### Memory - Your RAM should at minimum equals your total VRAM of all GPUs + 12 GB for system operations. - 1024 GB+ of RAM recommended for 8x 80 GB VRAM GPU configurations. - 512 GB+ of RAM recommended for 8x 24 GB VRAM GPU configurations. - 256","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["99",{"pageContent":"GB+ of RAM recommended for 8x 16 GB VRAM GPU configurations. - DDR4 minimum. DDR5 is recommended. - Memory should be ECC compatible. ### Storage - Absolute minimum of 1 TB+ of NVME space per GPU for each server (excluding the OS drives). Recommended storage is 2 TB+ of NVME space per GPU for 24 GB and 48 GB GPU, and is 4 TB+ of NVME space per GPU for 80 GB GPUs. - We recommend 2 smaller NVME disk in RAID 1 for the operating system (2x 500 GB or 2x 1 TB is fine). - For the data drives, keep one","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["100",{"pageContent":"larger NVME unpartitioned and unformatted. If several data drives are provided, you need to create a LVM volume for those. For higher number of drives, Raid 10 is the ideal scenario. When installing RunPod, you will have to mention that LVM or Raid volume. - A read/write speed of 3,000 mbps is required. - PCIe 5.0 x4 NVME SSD are an asset for 80 GB and newer 48 GPUs. - Ability to deploy network storage clusters if needed. ### Networking - 10 gbps Bidirectional Internet Speed as backbone. - ISP","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["101",{"pageContent":"access of minimum 1 gbps symmetrical per server. - Static Public IP. - A single IP can be shared between up between groups of up to 20 servers. - Find how to activate Public IP \\[here]\\(https://www.runpod.io/console/host/docs/config-options). - Access and ability to port forward. - Minimum of 30 ports forwarded per GPU per server. For an 8x GPU server, 300 ports forwarded is recommended. - Minimum interconnect speed of 25 gbps between servers. - Recommended interconnect speed of 50 gbps between","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["102",{"pageContent":"servers. - Recommended interconnect speed of 200 gbps between servers for 80 GB GPUs. - A100 HGX SXM4 80 GB and H100 HGX SXM5 80 GB see higher demand if on high-speed InfiniBand that are 1200 gbps to 3600 gbps. ### Compliance - Abide by Tier III+ Datacenter Standards. - Robust Uninterruptible Power Supply and backup generators. - Switches and PDU redundancy. - Internet Service Provider redundancy. - 24/7 on-site security and technical staff. - All maintenance or downtime need to be scheduled one","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["103",{"pageContent":"week in advance. See more about it \\[here]\\(https://docs.runpod.io/hosting/maintenance-and-reliability). ## Most importantly - The ability to scale GPU supply over time. - Interest for less purely transactional relationship, and eagerness for a partnership centered around building the future of AI Cloud Infrastructure.","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":1}}}}],["104",{"pageContent":"\\--- title: Overview description: \"RunPod is a cloud computing platform for AI, machine learning, and general compute, offering GPU and CPU resources, serverless computing, and a Command Line Interface for easy deployment and development.\" sidebar\\_position: 1 --- This page provides an overview for RunPod and its related features. \\*\\*RunPod\\*\\* is a cloud computing platform designed for AI, machine learning applications, and general compute. Execute your code utilizing both GPU and CPU","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["105",{"pageContent":"resources through \\[Pods]\\(/pods/overview), as well as \\[Serverless]\\(/serverless/overview) computing options. ## What are Pods? \\*\\*Pods\\*\\* allows you to run your code on GPU and CPU instances with containers. Pods are available in two different types: \\[Secure Cloud and Community Cloud]\\(references/faq/#secure-cloud-vs-community-cloud). The Secure Cloud runs in T3/T4 data centers providing high reliability and security, while the Community Cloud connects individual compute providers to","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["106",{"pageContent":"consumers through a vetted, secure peer-to-peer system. ## What is Serverless? \\*\\*Serverless\\*\\* offers pay-per-second serverless computing, bringing autoscaling to your production environment. The Serverless offering allows users to define a Worker, create a REST API Endpoint for it which queue jobs and autoscales to fill demand. This service, part of our Secure Cloud offering, guarantees low cold-start times and stringent security measures. You can get started with: - \\[Quick","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["107",{"pageContent":"deploys]\\(/serverless/quick-deploys) - \\[Build your own]\\(/serverless/workers/overview) ## CLI Additionally, RunPod has developed a Command Line Interface (CLI) tool designed specifically for quickly developing and deploying custom endpoints on the RunPod serverless platform. ### Our mission RunPod is committed to making cloud computing accessible and affordable to all without compromising on features, usability, or experience. We strive to empower individuals and enterprises with cutting-edge","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["108",{"pageContent":"technology, enabling them to unlock the potential of AI and cloud computing. For any general inquiries, we recommend browsing through our documentation. Our team is also available on \\[Discord]\\(https://discord.gg/cUpRmau42V), our support chat, and by \\[email]\\(mailto:support@runpod.io). More information can be found on our \\[contact page]\\(https://www.runpod.io/contact). ## Where do I go next? Learn more about RunPod by: - \\[Create an account]\\(/get-started/manage-accounts) - \\[Add funds to","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["109",{"pageContent":"your account]\\(/get-started/billing-information) - \\[Run your first tutorial]\\(/tutorials/introduction/overview)","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["110",{"pageContent":"\\--- title: Choose a Pod description: \"Choose the right Pod instance for your RunPod deployment by considering VRAM, RAM, vCPU, and storage, both Temporary and Persistent, to ensure optimal performance and efficiency.\" sidebar\\_position: 3 --- Selecting the appropriate Pod instance is a critical step in planning your RunPod deployment. The choice of VRAM, RAM, vCPU, and storage, both Temporary and Persistent, can significantly impact the performance and efficiency of your project. This page","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["111",{"pageContent":"gives guidance on how to choose your Pod configuration. However, these are general guidelines. Keep your specific requirements in mind and plan accordingly. ### Overview It's essential to understand the specific needs of your model. You can normally find detailed information in the model card’s description on platforms like Hugging Face or in the \\`config.json\\` file of your model. There are tools that can help you assess and calculate your model’s specific requirements, such as: - \\[Hugging","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["112",{"pageContent":"Face's Model Memory Usage Calculator]\\(https://huggingface.co/spaces/hf-accelerate/model-memory-usage) - \\[Vokturz’ Can it run LLM calculator]\\(https://huggingface.co/spaces/Vokturz/can-it-run-llm) - \\[Alexander Smirnov’s VRAM Estimator]\\(https://vram.asmirnov.xyz) Using these resources should give you a clearer picture of what to look for in a Pod. When transitioning to the selection of your Pod, you should focus on the following main factors: - \\*\\*GPU\\*\\* - \\*\\*VRAM\\*\\* - \\*\\*Disk Size\\*\\*","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["113",{"pageContent":"Each of these components plays a crucial role in the performance and efficiency of your deployment. By carefully considering these elements along with the specific requirements of your project as shown in your initial research, you will be well-equipped to determine the most suitable Pod instance for your needs. ### GPU The type and power of the GPU directly affect your project's processing capabilities, especially for tasks involving graphics processing and machine learning. ### Importance The","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["114",{"pageContent":"GPU in your Pod plays a vital role in processing complex algorithms, particularly in areas like data science, video processing, and machine learning. A more powerful GPU can significantly speed up computations and enable more complex tasks. ### Selection criteria - \\*\\*Task Requirements\\*\\*: Assess the intensity and nature of the GPU tasks in your project. - \\*\\*Compatibility\\*\\*: Ensure the GPU is compatible with your software and frameworks. - \\*\\*Energy Efficiency\\*\\*: Consider the power","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["115",{"pageContent":"consumption of the GPU, especially for long-term deployments. ### VRAM VRAM (Video RAM) is crucial for tasks that require heavy graphical processing and rendering. It is the dedicated memory used by your GPU to store image data that is displayed on your screen. ### Importance VRAM is essential for intensive tasks. It serves as the memory for the GPU, allowing it to store and access data quickly. More VRAM can handle larger textures and more complex graphics, which is crucial for high-resolution","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["116",{"pageContent":"displays and advanced 3D rendering. ### Selection criteria - \\*\\*Graphics Intensity\\*\\*: More VRAM is needed for graphically intensive tasks such as 3D rendering, gaming, or AI model training that involves large datasets. - \\*\\*Parallel Processing Needs\\*\\*: Tasks that require simultaneous processing of multiple data streams benefit from more VRAM. - \\*\\*Future-Proofing\\*\\*: Opting for more VRAM can make your setup more adaptable to future project requirements. ### Storage Adequate storage, both","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["117",{"pageContent":"temporary and persistent, ensures smooth operation and data management. ### Importance Disk size, including both temporary and persistent storage, is critical for data storage, caching, and ensuring that your project has the necessary space for its operations. ### Selection criteria - \\*\\*Data Volume\\*\\*: Estimate the amount of data your project will generate and process. - \\*\\*Speed Requirements\\*\\*: Faster disk speeds can improve overall system performance. - \\*\\*Data Retention Needs\\*\\*:","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["118",{"pageContent":"Determine the balance between temporary (volatile) and persistent (non-volatile) storage based on your data retention policies.","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["119",{"pageContent":"\\--- title: Export data description: Export RunPod data to various cloud providers, including Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage, Backblaze B2 Cloud Storage, and Dropbox, with secure key and access token management. sidebar\\_position: 6 --- You can export your Pod's data to any of the following cloud providers: - Amazon S3 - Google Cloud Storage - Microsoft Azure Blob Storage - Dropbox - Backblaze B2 Cloud Storage Remember to keep your keys and access tokens","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["120",{"pageContent":"confidential to maintain the security of your resources. ## Amazon S3 You can review a video guide on the process \\[here]\\(https://www.youtube.com/watch?v=2ZuOKwFR9pc\\&t=1s). ### Creating a Bucket within Amazon S3 1. \\*\\*Access the Bucket Creation Form:\\*\\* - Navigate to the Amazon S3 bucket creation form by visiting \\[this link]\\(https://s3.console.aws.amazon.com/s3/bucket/create?region=us-east-1). 2. \\*\\*Name Your Bucket:\\*\\* - Provide a descriptive name for your bucket. Choose a name that is","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["121",{"pageContent":"easy to remember and reflects the contents or purpose of the bucket. 3. \\*\\*Select AWS Region:\\*\\* - Ensure you select your preferred AWS Region. This is important for data storage locations and can affect access speeds. 4. \\*\\*Adjust Public Access Settings:\\*\\* - Uncheck the \\*\\*Block All Public Access\\*\\* option at the bottom of the form if you need your bucket to be publicly accessible. 5. \\*\\*Access Key and Secret Access Key:\\*\\* - Go to Security Credentials in your AWS account. - Create an","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["122",{"pageContent":"Access Key on the Security Credentials page. - Note that your Secret Access Key will be displayed during this process. Keep it secure. !\\[]\\(/img/docs/7fa9781-image.png) ### Sending Data from RunPod to AWS S3 1. \\*\\*Access CloudSync in RunPod:\\*\\* - In RunPod, navigate to the CloudSync section. 2. \\*\\*Enter Key IDs and Bucket Information:\\*\\* - Enter your Access Key and Secret Access Key. - Specify the AWS Region where your bucket is located. - Provide the path of your bucket as shown in the","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["123",{"pageContent":"interface. 3. \\*\\*Initiate Data Transfer:\\*\\* - Select the \\*\\*Copy to AWS S3\\*\\* option. - This action will start copying your pod contents to the specified Amazon S3 bucket. 4. \\*\\*Monitor Transfer:\\*\\* - Once you select Copy, your pod contents should begin copying over to Amazon S3. - You can monitor the transfer process through RunPod’s interface to ensure that the data transfer completes successfully. !\\[]\\(/img/docs/8fec5c5-image.png) Remember to keep your Access Key and Secret Access Key","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["124",{"pageContent":"confidential to maintain the security of your AWS resources. ## Google Cloud Storage ### Creating a Bucket within Google Cloud Storage 1. \\*\\*Access the Bucket Creation Interface:\\*\\* - Navigate to the Google Cloud Storage dashboard and click on \"Buckets -> Create\" to access the bucket creation interface. 2. \\*\\*Name Your Bucket:\\*\\* - Assign a unique, descriptive name to your bucket that reflects its contents or purpose. 3. \\*\\*Configure Bucket Settings:\\*\\* - Leave most options as default.","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["125",{"pageContent":"Important: Uncheck \"Enforce Public Access Prevention On This Bucket\" if you need your bucket to be publicly accessible. 4. \\*\\*Organize Your Bucket:\\*\\* - Once the bucket is created, consider creating a folder within the bucket for better organization, especially if managing multiple pods. !\\[]\\(/img/docs/4450288-image.png) ### Transferring Data from RunPod to Google Cloud Storage 1. \\*\\*Access CloudSync in RunPod:\\*\\* - Within RunPod, go to the CloudSync section and select \"Google Cloud Storage","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["126",{"pageContent":"-> Copy to Google Cloud Storage.\" 2. \\*\\*Service Account JSON Key:\\*\\* - Obtain your Service Account JSON key. If unsure how to do this, consult \\[this guide]\\(https://cloud.google.com/iam/docs/keys-create-delete). - In the provided field on RunPod, paste the entire contents of your Service Account JSON key. 3. \\*\\*Specify Transfer Details:\\*\\* - Enter the destination path in your bucket. - Choose the folder from your pod that you wish to copy. 4. \\*\\*Initiate and Monitor Transfer:\\*\\* - Start","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["127",{"pageContent":"the data transfer process by selecting the relevant options. - Monitor the transfer in the RunPod interface to ensure successful completion. !\\[]\\(/img/docs/a4fcf38-image.png) ### Troubleshooting - If your bucket is not publicly viewable and you encounter errors, refer to \\[Google Cloud Storage's documentation on making data public]\\(https://cloud.google.com/storage/docs/access-control/making-data-public) for necessary adjustments. Remember to keep your Service Account JSON key confidential to","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["128",{"pageContent":"maintain the security of your Google Cloud resources. ## Azure Blob Storage Setup and Data Transfer with RunPod ### Creating a Storage Account in Azure 1. \\*\\*Create a Resource Group in Azure:\\*\\* - Go to \\[Resource Groups]\\(https://portal.azure.com/#view/HubsExtension/BrowseResourceGroups) and click the Create button. - Name the resource group, which will be used to organize your Azure resources. 2. \\*\\*Set Up a Storage Account:\\*\\* - Under \\[Storage","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["129",{"pageContent":"Accounts]\\(https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Storage%2FStorageAccounts), click Create. - Provide a name for your storage account and assign it to the newly created resource group. 3. \\*\\*Retrieve Access Key:\\*\\* - Navigate to Access Keys under Security + Networking in your storage account to get the key needed for authentication. 4. \\*\\*Create a Blob Container:\\*\\* - In the Storage Browser, select Blob Containers, then click Add Container. -","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["130",{"pageContent":"Optionally, create folders within this container for better organization. !\\[]\\(/img/docs/dcc8c23-image.png) ### Transferring Data from RunPod to Azure Blob Storage 1. \\*\\*Access Cloud Sync in RunPod:\\*\\* - Go to your pod in My Pods on RunPod. - Select Cloud Sync, then choose \"Azure Blob Storage\" and \"Copy to Azure Blob Storage.\" 2. \\*\\*Input Storage Details:\\*\\* - Enter your Azure account name and account key. - Specify the desired path in the blob storage where the data will be transferred. 3.","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["131",{"pageContent":"\\*\\*Initiate Transfer:\\*\\* - Click on \"Copy to Azure Blob Storage\" to start the process. - Your RunPod data will begin copying over to the specified location in Azure Blob Storage. !\\[]\\(/img/docs/55e94f0-image.png) Ensure secure handling of your Azure account key to maintain the integrity and security of your data during the transfer process. ## Backblaze B2 Cloud Storage Setup ### Creating a Bucket in Backblaze B2 1. \\*\\*Navigate to Bucket Creation:\\*\\* - Go to \\[B2 Cloud Storage","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["132",{"pageContent":"Buckets]\\(https://secure.backblaze.com/b2\\_buckets.htm) and click \"Create a Bucket.\" - Make sure to set the bucket visibility to Public. 2. \\*\\*Generate Application Key:\\*\\* - Visit \\[App Keys]\\(https://secure.backblaze.com/app\\_keys.htm) to create a new application key. This key will be used for authenticating access to your bucket. !\\[]\\(/img/docs/8aff108-image.png) ### Transferring Data from RunPod to Backblaze B2 1. \\*\\*Access CloudSync in RunPod:\\*\\* - On your My Pods screen in RunPod,","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["133",{"pageContent":"select Cloud Sync, then choose \"Backblaze B2.\" 2. \\*\\*Enter Credentials:\\*\\* - Input your KeyID in the first field. - Enter your applicationKey in the second field. - Specify your bucket name as illustrated in the interface. 3. \\*\\*Initiate Transfer:\\*\\* - Click \"Copy to Backblaze B2\" to start the transfer process. Your pod's contents will begin transferring to the specified Backblaze B2 bucket. !\\[]\\(/img/docs/5c12c2f-image.png) Remember to securely manage your KeyID and applicationKey to","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["134",{"pageContent":"ensure the safety of your data in Backblaze B2 Cloud Storage. ## Dropbox Setup and Data Transfer with RunPod ### Setting Up Dropbox 1. \\*\\*Create an App on Dropbox:\\*\\* - Go to the \\[DBX Platform]\\(https://www.dropbox.com/developers/apps/create) and create an app. - Choose \"Scoped Access\" under API options and \"Full Dropbox\" for the type of access. Then, name your app. 2. \\*\\*Configure App Permissions:\\*\\* - In the Dropbox App Console, under the Permissions tab, make sure to enable the required","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["135",{"pageContent":"checkboxes for reading and writing access. 3. \\*\\*Generate Access Token:\\*\\* - Return to the Settings tab of your app. - In the OAuth2 section, click \"Generate\" under Generated Access Token to create an access key. - Save this key securely, as it is crucial for integrating with RunPod and will not be visible after leaving the page. 4. \\*\\*Create a Dropbox Folder (Optional):\\*\\* - Although not mandatory, it's advisable to create a dedicated folder in Dropbox for organizing the data synced from","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["136",{"pageContent":"RunPod. !\\[]\\(/img/docs/e73bced-image.png) ### Transferring Data from RunPod to Dropbox 1. \\*\\*Access Cloud Sync in RunPod:\\*\\* - In RunPod, navigate to the Cloud Sync option and select Dropbox. 2. \\*\\*Enter Access Token and Path:\\*\\* - Input your Dropbox Access Token. - Specify the remote path in Dropbox where you want to send the data. 3. \\*\\*Start Data Sync:\\*\\* - Click \"Copy to Dropbox\" to initiate the data syncing process. Your RunPod data will begin transferring to the specified location","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["137",{"pageContent":"in Dropbox. !\\[]\\(/img/docs/2281560-image.png) Ensure the safekeeping of your Dropbox Access Token to maintain the security of your data during the sync process.","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":1}}}}],["138",{"pageContent":"\\--- title: Expose ports description: \"Exposing ports on your pod to the outside world: Learn how to expose ports via RunPod's Proxy or TCP Public IP, and discover the benefits and limitations of each method, including symmetrical port mapping requests.\" sidebar\\_position: 8 --- There are a few ways to expose ports on your pod to the outside world. The first thing that you should understand is that the publicly exposed port is most likely NOT going to be the same as the port that you expose on","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":1,"to":1}}}}],["139",{"pageContent":"your container. Let's look at an example to illustrate this. Let's say that I want to run a public API on my pod using uvicorn with the following command: \\`\\`\\` uvicorn main:app --host 0.0.0.0 --port 4000 \\`\\`\\` This means that uvicorn would be listening on all interfaces on port 4000. Let's now expose this port to the public internet using two different methods. ### Through RunPod's Proxy In this case, you would want to make sure that the port you want to expose (4000 in this case) is set on","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":1,"to":1}}}}],["140",{"pageContent":"the \\[Template]\\(https://www.runpod.io/console/user/templates) or \\[Pod]\\(https://www.runpod.io/console/pods) configuration page. You can see here that I have added 4000 to the HTTP port list in my pod config. You can also do this on your template definition. !\\[]\\(/img/docs/1386a3c-image.png) Once you have done this, and your server is running, you should be able to hit your server using the pod's proxy address, which is formed in this programmatic way, where the pod ID is the unique ID of your","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":1,"to":1}}}}],["141",{"pageContent":"pod, and the internal port in this case is 4000: \\`\\`\\`text https://{POD\\_ID}-{INTERNAL\\_PORT}.proxy.runpod.net \\`\\`\\` Keep in mind that this exposed to the public internet. While your pod ID can act as a password of sorts, it's not a replacement for real authentication, which should be implemented at your API level. ### Through TCP Public IP If your pod supports a public IP address, you can also expose your API over public TCP. In this case, you would add the port to the TCP side of the","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":1,"to":1}}}}],["142",{"pageContent":"configuration. !\\[]\\(/img/docs/49ebb9a-image.png) The only difference here is that you will receive an external port mapping and a public IP address to access your service. For example, your connect menu may look something like this: !\\[]\\(/img/docs/5e76c21-image.png) In this case, you would be hitting your service running on 4000 with the following ip:port combination \\`\\`\\`text 73.10.226.56:10027 \\`\\`\\` Be aware that the public IP could potentially change when using Community Cloud, but should","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":1,"to":1}}}}],["143",{"pageContent":"not change when using Secure Cloud. The port will change if your pod gets reset. ### Requesting a Symmetrical Port Mapping For some applications, asymmetrical port mappings are not ideal. In the above case, we have external port 10027 mapping to internal port 4000. If you need to have a symmetrical port mapping, you can request them by putting in ports above 70000 in your TCP port field. !\\[]\\(/img/docs/23c4178-image.png) Of course, 70000 isn't a valid port number, but what this does is it tells","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":1,"to":1}}}}],["144",{"pageContent":"RunPod that you don't care what the actual port number is on launch, but to rather give you a symmetrical mapping. You can inspect the actual mapping via your connect menu: !\\[]\\(/img/docs/92e4f90-image.png) In this case, I have requested two symmetrical ports and they ended up being 10030:10030 and 10031:10031. If you need programmatic access to these in your pod, you can access them via environment variable: \\`\\`\\`text RUNPOD\\_TCP\\_PORT\\_70001=10031 RUNPOD\\_TCP\\_PORT\\_70000=10030 \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":1,"to":1}}}}],["145",{"pageContent":"\\--- title: \"Override public key\" sidebar\\_position: 9 description: \"Configure public key authentication for secure access via terminal, or override at the pod level using the RUNPOD\\_SSH\\_PUBLIC\\_KEY environment variable.\" --- We attempt to inject the public key that you configure in your account's settings page for authentication using basic terminal. If you want to override this at a pod level, you can manually supply a public key as the \\`RUNPOD\\_SSH\\_PUBLIC\\_KEY\\` environment variable.","metadata":{"source":"/runpod-docs/docs/pods/configuration/override-public-keys.md","loc":{"lines":{"from":1,"to":1}}}}],["146",{"pageContent":"\\--- title: \"Use SSH\" sidebar\\_position: 9 description: \"Set up secure SSH access to RunPod using public/private key pairs, ensuring compatibility with ed25519 keys, and troubleshoot common issues like incorrect key copying and file path errors.\" --- The basic terminal SSH access that RunPod exposes is not a full SSH connection and, therefore, does not support commands like SCP. If you want to have full SSH capabilities, then you will need to rent an instance that has public IP support and run a","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["147",{"pageContent":"full SSH daemon in your Pod. ## Setup 1. Generate your public/private SSH key pair on your local machine with \\`ssh-keygen -t ed25519 -C \"your\\_email@example.com\"\\`. This will save your public/private key pair to \\`~/.ssh/id\\_ed25519.pub\\` and \\`~/.ssh/id\\_ed25519\\`, respectively.\\ :::note if you're using command prompt in Windows rather than the Linux terminal or WSL, your public/private key pair will be saved to \\`C:\\users\\\\{yourUserAccount}\\\\.ssh\\id\\_ed25519.pub\\` and","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["148",{"pageContent":"\\`C:\\users\\\\{yourUserAccount}\\\\.ssh\\id\\_ed25519\\`, respectively. ::: !\\[]\\(/img/docs/4655a01-1.png) 2. Add your public key to your \\[RunPod user settings]\\(https://www.runpod.io/console/user/settings). !\\[]\\(/img/docs/4972691-2.png) !\\[]\\(/img/docs/c340553-image.png) 3. Start your Pod. Make sure of the following things: - Your Pod supports a public IP, if you're deploying in Community Cloud. - An SSH daemon is started. If you're using a RunPod official template such as RunPod Stable Diffusion,","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["149",{"pageContent":"you don't need to take any additional steps. If you're using a custom template, make sure your template has TCP port 22 exposed and use the following Docker command. If you have an existing start command, replace \\`sleep infinity\\` at the end with your existing command: \\`\\`\\`bash bash -c 'apt update;DEBIAN\\_FRONTEND=noninteractive apt-get install openssh-server -y;mkdir -p ~/.ssh;cd $\\_;chmod 700 ~/.ssh;echo \"$RUNPOD\\_SSH\\_PUBLIC\\_KEY\" >> authorized\\_keys;chmod 700 authorized\\_keys;service ssh","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["150",{"pageContent":"start;sleep infinity' \\`\\`\\` !\\[]\\(/img/docs/97823c6-image.png) Once your Pod is done initializing, you'll be able to SSH into it by running the SSH over exposed TCP command in the Pod's Connection Options menu on your local machine. :::note - if you're using the Windows Command Prompt rather than the Linux terminal or WSL, and you've used the default key location when generating your public/private key pair (i.e., you didn't specify a different file path when prompted), you'll need to modify","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["151",{"pageContent":"the file path in the provided SSH command after the \\`-i\\` flag to \\`C:\\users\\\\{yourUserAccount}\\\\.ssh\\id\\_ed25519\\`. - If you've saved your key to a location other than the default, specify that path you chose when generating your key pair after the \\`-i\\` flag instead. ::: !\\[]\\(/img/docs/3d51ed8-image.png) !\\[]\\(/img/docs/ff71847-image.png) ## What's the SSH password? If you're being prompted for a password when you attempt to connect, something is amiss. We don't require a password for SSH","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["152",{"pageContent":"connections. Some common mistakes that cause your SSH client to prompt for a password include: - Copying and pasting the key \\_fingerprint\\_ (beginning with \\`SHA256:\\`) into your RunPod user settings instead of the public key itself (the contents of the \\`id\\_ed25519.pub\\` file when viewed from a text editor) - Omitting the encryption type from the beginning of the key when copying and pasting into your RunPod user settings (i.e., copying the random text, but not the \\`ssh-ed25519\\` which","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["153",{"pageContent":"precedes it) - Not separating different public keys in your RunPod user settings with a newline between each one (this would result in the first public/private key pair functioning as expected, but each subsequent key pair would not work) - Specifying an incorrect file path to your private key file: !\\[]\\(/img/docs/10cbfa6-image.png) - Attempting to use a private key that other users on the machine have permissions for: !\\[]\\(/img/docs/7a5cf85-image.png) - Incorrect Private Key being used","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["154",{"pageContent":"locally in SSH config file. There should be a config file on your local machine in your ~/.ssh folder. You want to ensure that the IdentityFile in the config file points to the private key of the public key you used to make this connection. If you are not pointing to the correct private key in the config file, when you make a connection request using your public key, you will get a mismatch and be prompted for a password. Once the correct private key is set in your config file, you can connect","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["155",{"pageContent":"without a password. !\\[private-key-fix]\\(https://github.com/runpod/docs/assets/19496114/1f3db241-72a1-4d29-be36-ea5bab945b0a)","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["156",{"pageContent":"\\--- title: Connect to a Pod sidebar\\_position: 4 --- You can connect to a Pod through various methods, depending on your requirements, preferences, and templates used. ## SSH terminal Connecting to a Pod using an SSH terminal is a secure and reliable method, suitable for long-running processes and critical tasks. Every Pod contains the ability to connect through SSH. To do this, you need to have an SSH client installed on your local machine. 1. Open the terminal on your local machine. 2. Choose","metadata":{"source":"/runpod-docs/docs/pods/connect-to-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["157",{"pageContent":"one of the following commands and then enter it into your machine's terminal: \\`\\`\\`bash # No support for SCP & SFTP ssh @ -i # Supports SCP & SFTP ssh @ -p -i \\`\\`\\` Replace the placeholders with the following: - \\`\\`: Your assigned username for the Pod - \\`\\`: The SSH hostname provided for your Pod - \\`\\`: The IP address of your Pod - \\`\\`: The designated SSH port for your Pod - \\`\\`: The path to your SSH private key file You now have a secure SSH terminal to your Pod. ## Web terminal :::note","metadata":{"source":"/runpod-docs/docs/pods/connect-to-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["158",{"pageContent":"Depending on your Pod's template will provide the ability to connect to the web terminal. ::: The web terminal is a convenient, web-based terminal for quickly connecting to your Pod and running commands. This shouldn't be relied on for long-running process such as training an LLM or other critical tasks. The web terminal is useful for quickly logging in to your Pod and running commands. 1. On your Pod's page, select \\*\\*Connect\\*\\*. 2. Select \\*\\*Start Web Terminal\\*\\* then choose \\*\\*Connect to","metadata":{"source":"/runpod-docs/docs/pods/connect-to-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["159",{"pageContent":"Web Terminal\\*\\* in a new window. 3. Enter the \\*\\*Username\\*\\* and \\*\\*Password\\*\\*.","metadata":{"source":"/runpod-docs/docs/pods/connect-to-a-pod.md","loc":{"lines":{"from":1,"to":1}}}}],["160",{"pageContent":"\\--- title: Access Logs description: \"Get insights into your pods' activities with logs, including container logs and system logs, detailing console output, formation, and status updates, accessible through the Pods dashboard's Logs button.\" sidebar\\_position: 5 --- Pods provide two types of logs. - \\*\\*Container logs\\*\\* include anything typically sent to your console standard out. - \\*\\*System logs\\*\\* include information on your container's formation and current status, including download,","metadata":{"source":"/runpod-docs/docs/pods/logs.md","loc":{"lines":{"from":1,"to":1}}}}],["161",{"pageContent":"extraction, start, and stop. To access your logs, go the Pods dashboard and click the \\*\\*Logs\\*\\* button on your Pod.","metadata":{"source":"/runpod-docs/docs/pods/logs.md","loc":{"lines":{"from":1,"to":1}}}}],["162",{"pageContent":"\\--- title: Manage Pods description: \"Learn how to start, stop, and manage Pods with RunPod, including creating and terminating Pods, and using the command line interface to manage your Pods.\" id: manage-pods sidebar\\_position: 3 --- Learn how to start, stop, and manage Pods with RunPod, including creating and terminating Pods, and using the command line interface to manage your Pods. import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; ### Prerequisites If you are using the","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["163",{"pageContent":"\\[RunPod CLI]\\(/cli/install-runpodctl), you'll need to set your API key in the configuration. \\`\\`\\`bash runpodctl config --apiKey $RUNPOD\\_API\\_KEY \\`\\`\\` Replace \\`$RUNPOD\\_API\\_KEY\\` with your RunPod API key. Once your API key is set, you can manage your infrastructure. If you're not sure which Pod meets your needs, see \\[Choose a Pod]\\(/pods/choose-a-pod). ## Create Pods 1. Navigate to \\[Pods]\\(https://www.dev.runpod.io/console/pods) and select \\*\\*+ Deploy\\*\\*. 2. Choose between \\*\\*GPU\\*\\*","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["164",{"pageContent":"and \\*\\*CPU\\*\\*. 3. Customize your an instance by setting up the following: 1. (optional) Specify a Network volume. 2. Select an instance type. For example, \\*\\*A40\\*\\*. 3. (optional) Provide a template. For example, \\*\\*RunPod Pytorch\\*\\*. 4. (GPU only) Specify your compute count. 4. Review your configuration and select \\*\\*Deploy On-Demand\\*\\*. To create a Pod using the CLI, use the \\`runpodctl create pods\\` command. \\`\\`\\`bash runpodctl create pods \\ --name hello-world \\ --gpuType \"NVIDIA","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["165",{"pageContent":"A40\" \\ --imageName \"runpod/pytorch:3.10-2.0.0-117\" \\ --containerDiskSize 10 \\ --volumeSize 100 \\ --args \"bash -c 'mkdir /testdir1 && /start.sh'\" \\`\\`\\` :::tip RunPod supports custom \\[templates]\\(/pods/templates/overview) that allow you to specify your own Dockerfile. By creating a Dockerfile, you can build a \\[custom Docker image]\\(/tutorials/introduction/containers/overview) with your specific dependencies and configurations. This ensures that your applications are reliable and portable across","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["166",{"pageContent":"different environments. ::: Charges occur after the Pod build is complete. ## Stop a Pod 1. Click the stop icon. 2. Confirm by clicking the \\*\\*Stop Pod\\*\\* button. To stop a Pod, enter the following command. \\`\\`\\`bash runpodctl stop pod $RUNPOD\\_POD\\_ID \\`\\`\\` ### Stop a Pod after a specific time You can also stop a Pod after a specific amount of time. For example, the following command sleeps for 2 hours, and then stops the Pod. Use the following command to stop a Pod after 2 hours:","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["167",{"pageContent":"\\`\\`\\`bash sleep 2h; runpodctl stop pod $RUNPOD\\_POD\\_ID & \\`\\`\\` This command uses sleep to wait for 2 hours before executing the \\`runpodctl stop pod\\` command to stop the Pod. The \\`&\\` at the end runs the command in the background, allowing you to continue using the SSH session. To stop a Pod after 2 hours using the web terminal, enter: \\`\\`\\`bash nohup bash -c \"sleep 2h; runpodctl stop pod $RUNPOD\\_POD\\_ID\" & \\`\\`\\` \\`nohup\\` ensures the process continues running if you close the web","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["168",{"pageContent":"terminal window. :::warning You are charged for storing idle Pods. If you do not need to store your Pod, be sure to terminate it next. ::: ## Start a Pod You can resume a pod that has been stopped. 1. Navigate to the \\*\\*Pods\\*\\* page. 2. Select your Pod you want to resume. 3. Select \\*\\*Start\\*\\*. Your Pod will resume. To start a single Pod, enter the command \\`runpodctl start pod\\`. You can pass the environment variable \\`RUNPOD\\_POD\\_ID\\` to identify each Pod. \\`\\`\\`bash runpodctl start pod","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["169",{"pageContent":"$RUNPOD\\_POD\\_ID \\`\\`\\` ## Terminate a Pod :::danger Terminating a Pod permanently deletes all data outside your \\[Network volume]\\(/pods/storage/create-network-volumes). Be sure you've saved any data you want to access again. ::: 1. Select the hamburger menu at the bottom of the Pod you want to terminate. 2. Click \\*\\*Terminate Pod\\*\\*. 3. Confirm by clicking the \\*\\*Yes\\*\\* button. To remove a single Pod, enter the following command. \\`\\`\\`bash runpodctl remove pod $RUNPOD\\_POD\\_ID \\`\\`\\` You","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["170",{"pageContent":"can also remove Pods in bulk. For example, the following command terminates up to 40 pods with the name \\`my-bulk-task\\`. \\`\\`\\`bash runpodctl remove pods my-bulk-task --podCount 40 \\`\\`\\` ## List Pods If you're using the command line, enter the following command to list your pods. \\`\\`\\`bash runpodctl get pod \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["171",{"pageContent":"\\--- title: Overview description: \"Run containers as Pods with a container registry, featuring compatible architectures, Ubuntu Linux, and persistent storage, with customizable options for GPU type, system disk size, and more.\" sidebar\\_position: 1 --- Pods are running container instances. You can pull an instance from a container registry such as Docker Hub, GitHub Container Registry, Amazon Elastic Container Registry, or another compatible registry. :::note When building an image for RunPod on","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["172",{"pageContent":"a Mac (Apple Silicon), use the flag \\`--platform linux/amd64\\` to ensure your image is compatible with the platform. This flag is necessary because RunPod currently only supports the \\`linux/amd64\\` architecture. ::: ### Understanding Pod components and configuration A Pod is a server container created by you to access the hardware, with a dynamically generated assigned identifier. For example, \\`2s56cp0pof1rmt\\` identifies the instance. A Pod comprises a container volume with the operating","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["173",{"pageContent":"system and temporary storage, a disk volume for permanent storage, an Ubuntu Linux container, allocated vCPU and system RAM, optional GPUs or CPUs for specific workloads, a pre-configured template for easy software access, and a proxy connection for web access. Each Pod encompasses a variety of components: - A container volume that houses the operating system and temporary storage. - This storage is volatile and will be lost if the Pod is halted or rebooted. - A disk volume for permanent","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["174",{"pageContent":"storage, preserved for the duration of the Pod's lease, akin to a hard disk. - This storage is persistent and will be available even if the Pod is halted or rebooted. - Network storage, similar to a volume but can be moved between machines. - When using network storage, you can only delete the Pod. - An Ubuntu Linux container, capable of running almost any software that can be executed on Ubuntu. - Assigned vCPU and system RAM dedicated to the container and any processes it runs. - Optional GPUs","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["175",{"pageContent":"or CPUs, tailored for specific workloads like CUDA or AI/ML tasks, though not mandatory for starting the container. - A pre-configured template that automates the installation of software and settings upon Pod creation, offering straightforward, one-click access to various packages. - A proxy connection for web access, allowing connectivity to any open port on the container. - For example, \\`https://\\[pod-id]-\\[port number].proxy.runpod.net\\`, or","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["176",{"pageContent":"\\`https://2s56cp0pof1rmt-7860.proxy.runpod.net/\\`). To get started, see how to \\[Choose a Pod]\\(/pods/choose-a-pod) then see the instructions on \\[Manage Pods]\\(/pods/manage-pods). ## Learn more You can jump straight to a running Pod by starting from a \\[template]\\(/pods/templates/overview). For more customization, you can configure the following: - \\[GPU Type]\\(/references/gpu-types) and quantity - System Disk Size - Start Command - \\[Environment","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["177",{"pageContent":"Variables]\\(/pods/references/environment-variables) - \\[Expose HTTP/TCP ports]\\(/pods/configuration/expose-ports) - \\[Persistent Storage Options]\\(/category/storage) To get started, see how to \\[Choose a Pod]\\(/pods/choose-a-pod) then see the instructions on \\[Manage Pods]\\(/pods/manage-pods).","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["178",{"pageContent":"\\--- title: Pod environment variables description: \"Configure and manage your pods with these essential environment variables, including pod ID, API key, host name, GPU and CPU count, public IP, SSH port, data center ID, volume ID, CUDA version, current working directory, PyTorch version, and public SSH key.\" sidebar\\_position: 4 --- You can store the following environment variables in your Pods. | Variable | Description | | : | : | | \\`RUNPOD\\_POD\\_ID\\` | The unique identifier for your pod. | |","metadata":{"source":"/runpod-docs/docs/pods/references/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["179",{"pageContent":"\\`RUNPOD\\_API\\_KEY\\` | Used to make RunPod API calls to the specific pod. It's limited in scope to only the pod. | | \\`RUNPOD\\_POD\\_HOSTNAME\\` | Name of the host server the pod is running on. | | \\`RUNPOD\\_GPU\\_COUNT\\` | Number of GPUs available to the pod. | | \\`RUNPOD\\_CPU\\_COUNT\\` | Number of CPUs available to the pod. | | \\`RUNPOD\\_PUBLIC\\_IP\\` | If available, the publicly accessible IP for the pod. | | \\`RUNPOD\\_TCP\\_PORT\\_22\\` | The public port SSH port 22. | | \\`RUNPOD\\_DC\\_ID\\` | The","metadata":{"source":"/runpod-docs/docs/pods/references/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["180",{"pageContent":"data center where the pod is located. | | \\`RUNPOD\\_VOLUME\\_ID\\` | The ID of the volume connected to the pod. | | \\`CUDA\\_VERSION\\` | The installed CUDA version. | | \\`PWD\\` | Current working directory. | | \\`PYTORCH\\_VERSION\\` | Installed PyTorch Version. | | \\`PUBLIC\\_KEY\\` | The SSH public keys to access the pod over SSH. |","metadata":{"source":"/runpod-docs/docs/pods/references/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["181",{"pageContent":"\\--- title: \"Savings plans\" description: \"Maximize your RunPod experience with Savings Plans, a cost-saving feature that offers upfront discounts on uninterrupted instances, flexible savings, instant activation, easy management, and clear visibility.\" sidebar\\_position: 9 --- Savings Plans are a powerful cost-saving feature designed to optimize your RunPod experience. Take advantage of upfront payments to unlock discounts on uninterrupted instances, maximize cost efficiency, and get the most out","metadata":{"source":"/runpod-docs/docs/pods/savings-plans.md","loc":{"lines":{"from":1,"to":1}}}}],["182",{"pageContent":"of specific card types. ## Get started To start saving with RunPod's Savings Plans, ensure you have sufficient RunPod credits in your account. There are two ways to create a savings plan. - Add a Savings Plan to your existing Pod from the Pod dashboard. - Initiate a Savings Plan during Pod deployment. Regularly check the \\*\\*Savings Plans\\*\\* section to track your Savings Plans and associated pods. !\\[]\\(/img/docs/f58bad9-image.png) !\\[]\\(/img/docs/0eb087a-image.png) ## Benefits \\*\\*Reduced","metadata":{"source":"/runpod-docs/docs/pods/savings-plans.md","loc":{"lines":{"from":1,"to":1}}}}],["183",{"pageContent":"Costs\\*\\*: By paying upfront for a Savings Plan, you can enjoy discounted rates on uninterrupted instances. This means significant cost savings for your RunPod deployments. \\*\\*Flexible Savings\\*\\*: When you stop a Pod, the Savings Plan associated with it applies to your next deployment of the same card. This means you continue to benefit from your savings commitment even after temporary pauses in your pod usage. :::warning Stopping your Pod(s) does not extend your Savings Plan. Each Savings","metadata":{"source":"/runpod-docs/docs/pods/savings-plans.md","loc":{"lines":{"from":1,"to":1}}}}],["184",{"pageContent":"Plan has a fixed expiration date, which you set at when you buy the plan. ::: \\*\\*Instant Activation\\*\\*: Savings Plans kick in immediately upon activation and remain active for the duration of your committed period. You can start saving from the moment you initiate a Savings Plan. \\*\\*Easy Management\\*\\*: Adding a Savings Plan to your existing running Pod is a breeze through the Pod dashboard. Alternately, Savings Plans are automatically started when you deploy a new Pod, simplifying the","metadata":{"source":"/runpod-docs/docs/pods/savings-plans.md","loc":{"lines":{"from":1,"to":1}}}}],["185",{"pageContent":"process and ensuring you don't miss out on potential savings. \\*\\*Clear Visibility\\*\\*: Stay on top of your savings commitments and associated Pods by navigating to the \\*\\*Savings Plan\\*\\* page. This shows a comprehensive overview of your Savings Plans for effective monitoring and management. Empower your RunPod deployments with Savings Plans and unlock the potential for cost optimization and enhanced savings. Begin maximizing your RunPod experience today!","metadata":{"source":"/runpod-docs/docs/pods/savings-plans.md","loc":{"lines":{"from":1,"to":1}}}}],["186",{"pageContent":"\\--- title: \"Create a network volume\" description: \"Create a network volume in Secure Cloud to access high-performance storage and flexibility for multiple pods, with options for data center selection, naming, and size allocation, and enjoy cost-effective storage solutions with robust infrastructure and NVME SSDs.\" sidebar\\_position: 9 --- Network Volumes are a feature specific to Secure Cloud that allows you to create a volume that multiple pods can interact with. This gives you an extra amount","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":1,"to":1}}}}],["187",{"pageContent":"of flexibility to keep working--especially if you are working with a high demand GPU pool that may not always be available--as you can simply create a pod in a different pool while you wait for an option to free up. This can also save you time by downloading frequently used models or other large files to a volume and holding them for later use, rather than having to re-download them every time you spin up a new pod. \\*\\*How to Create a Volume\\*\\* Under the Secure Cloud page, click the option to","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":1,"to":1}}}}],["188",{"pageContent":"create a volume. !\\[]\\(/img/docs/797cfdc-image.png) The pricing for the volume will be shown, and you can select a data center and provide a name and the requested size. !\\[]\\(/img/docs/596a4f5-image.png) Once you create the volume, it will appear in your list. !\\[]\\(/img/docs/c4eea31-image.png) Once you click the Deploy option, your container size will be locked to the size of your network volume. Note that there will be a nominal cost for network volume storage, in lieu of the disk cost","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":1,"to":1}}}}],["189",{"pageContent":"normally quoted. Also note that you can link many pods to one singular network volume, and you will enjoy an overall cost saving even with just two pods sharing one volume (as opposed to setting up two pods with separate volumes), despite the presence of this additional cost. !\\[]\\(/img/docs/8f69d41-image.png) \\*\\*What's the infrastructure behind Network Volume\\*\\* When you choose to create a Network Volume, you gain access to our robust infrastructure, which includes state-of-the-art storage","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":1,"to":1}}}}],["190",{"pageContent":"servers located in the same datacenters where you rent GPU servers from us. These servers are connected via a high-speed 25Gbps local network, up to 200Gbps in some locations, guaranteeing efficient data transfer and minimal latency. Everything is stored on high-speed NVME SSDs to ensure best performance. If you're interested in harnessing the advantages of Network Volume and its cost-effective storage solutions, we invite you to read our detailed \\[blog","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":1,"to":1}}}}],["191",{"pageContent":"article]\\(https://blog.runpod.io/four-reasons-to-set-up-a/). It explores the benefits and features of Network Volume, helping you make an informed decision about your storage needs. \\*\\*Please note that if your machine-based storage or network volume is terminated due to lack of funds, that disk space is immediately freed up for use by other clients, and RunPod is unable to assist in recovering lost storage.\\*\\*","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":1,"to":1}}}}],["192",{"pageContent":"\\--- title: \"Sync a volume to a cloud provider\" sidebar\\_position: 9 description: \"Sync your volume to a cloud provider by clicking 'Cloud Sync' on your My Pods page, then follow provider-specific instructions from the dropdown menu.\" --- You can sync your volume to a cloud provider by clicking the Cloud Sync option under your My Pods page. Click the dropdown on the left to get specific instructions for your specific provider.","metadata":{"source":"/runpod-docs/docs/pods/storage/sync-volumes.md","loc":{"lines":{"from":1,"to":1}}}}],["193",{"pageContent":"\\--- title: \"Transfer files with SCP\" sidebar\\_position: 9 description: \"Transfer files to and from your Pod using SCP and rsync commands. Prerequisites include a Linux or WSL instance, SSH configured, and rsync installed. Follow syntax guides for secure file transfer and option flags for customization.\" --- Learn to transfer files to and from RunPod with Secure Copy Protocol (SCP). ## Prerequisites - Make sure your Pod is configured to use real SSH. For more information, see \\[use","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["194",{"pageContent":"SSH]\\(/pods/configuration/use-ssh). - If you intend to use rsync, make sure it's installed on both your local machine and your Pod with \\`apt install rsync\\`. - Note the public IP address and external port from the SSH over exposed TCP command (you'll need these for the SCP/rsync commands). ## Transferring with SCP The general syntax for sending files to a Pod with SCP is as follows (execute this on your local machine, and replace the x's with your Pod's external TCP port and IP; for this","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["195",{"pageContent":"example, they are 43201 and 194.26.196.6, respectively): \\`\\`\\`shell scp -P 43201 -i ~/.ssh/id\\_ed25519 /local/file/path root@194.26.196.6:/destination/file/path \\`\\`\\` :::note If your private key file is in a location other than \\`~/.ssh/id\\_ed25519\\` or you're using the Windows Command Prompt, make sure you update this path accordingly in your command. ::: Example of sending a file to a Pod: \\`\\`\\`shell scp -P 43201 -i ~/.ssh/id\\_ed25519 ~/documents/example.txt","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["196",{"pageContent":"root@194.26.196.6:/root/example.txt \\`\\`\\` If you want to receive a file from your Pod, switch the source and destination arguments: \\`\\`\\`shell scp -P 43201 -i ~/.ssh/id\\_ed25519 root@194.26.196.6:/root/example.txt ~/documents/example.txt \\`\\`\\` If you need to transfer a directory, use the \\`-r\\` flag to recursively copy files and subdirectories (this will follow any symbolic links encountered as well): \\`\\`\\`shell scp -r -P 43201 -i ~/.ssh/id\\_ed25519 ~/documents/example\\_dir","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["197",{"pageContent":"root@194.26.196.6:/root/example\\_dir \\`\\`\\` ## Transferring with rsync :::note Your local machine must be running Linux or a \\[WSL instance]\\(https://learn.microsoft.com/en-us/windows/wsl/about) in order to use rsync. ::: The general syntax for sending files to a Pod with rsync is as follows (execute this on your local machine, and replace the x's with your Pod's external TCP port and IP): \\`\\`\\`shell rsync -e \"ssh -p 43201\" /source/file/path root@194.26.196.6:/destination/file/path \\`\\`\\` Some","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["198",{"pageContent":"helpful flags include: - \\`-a\\`/\\`--archive\\` - archive mode (ensures that permissions, timestamps, and other attributes are preserved during the transfer; use this when transferring directories or their contents) - \\`-d\\`/\\`--delete\\` - deletes files in the destination directory that are not present in the source - \\`-p\\`/\\`--progress\\` - displays file transfer progress - \\`-v\\`/\\`--verbose\\` - verbose output - \\`-z\\`/\\`--compress\\` - compresses data as it's being sent and uncompresses as it's","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["199",{"pageContent":"received (heavier on your CPU, but easier on your network connection) Example of sending a file to a Pod using rsync: \\`\\`\\`shell rsync -avz -e \"ssh -p 43201\" ~/documents/example.txt root@194.26.196.6:/root/example.txt \\`\\`\\` If you want to receive a file from your Pod, switch the source and destination arguments: \\`\\`\\`shell rsync -avz -e \"ssh -p 43201\" root@194.26.196.6:/root/example.txt ~/documents/example.txt \\`\\`\\` To transfer the contents of a directory (without transferring the directory","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["200",{"pageContent":"itself), use a trailing slash in the file path: \\`\\`\\`shell rsync -avz -e \"ssh -p 43201\" ~/documents/example\\_dir/ root@194.26.196.6:/root/example\\_dir/ \\`\\`\\` Without a trailing slash, the directory itself is transferred: \\`\\`\\`shell rsync -avz -e \"ssh -p 43201\" ~/documents/example\\_dir root@194.26.196.6:/root/ \\`\\`\\` An advantage of rsync is that files that already exist at the destination aren't transferred again if you attempt to copy them twice (note the minimal data transfer after the","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["201",{"pageContent":"second execution): \\`\\`\\`shell rsync -avz -e \"ssh -p 43201\" ~/documents/example.txt root@194.26.196.6:/root/example.txt sending incremental file list example.txt 119 100% 0.00kB/s 0:00:00 (xfr#1, to-chk=0/1) sent 243 bytes received 35 bytes 185.33 bytes/sec total size is 119 speedup is 0.43 $ rsync -avz -e \"ssh -p 43201\" ~/documents/example.txt root@194.26.196.6:/root/example.txt sending incremental file list sent 120 bytes received 12 bytes 88.00 bytes/sec total size is 119 speedup is 0.90","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["202",{"pageContent":"\\`\\`\\`","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":1}}}}],["203",{"pageContent":"\\--- title: Storage types sidebar\\_position: 8 --- The following section describes the different types of storage and volume options. ## Container volume A container volume is a type of storage that houses the operating system and provides temporary storage for a Pod. It is created when a Pod is launched and is tightly coupled with the Pod's lifecycle. \\*\\*Key characteristics:\\*\\* - Volatile storage that is lost if the Pod is halted or rebooted - Suitable for storing temporary data or files that","metadata":{"source":"/runpod-docs/docs/pods/storage/types.md","loc":{"lines":{"from":1,"to":1}}}}],["204",{"pageContent":"are not required to persist beyond the Pod's lifecycle - Capacity is determined by the selected Pod configuration - Provides fast read and write speeds as it is locally attached to the Pod ## Disk volume A disk volume is a type of persistent storage that is preserved for the duration of the Pod's lease. It functions similarly to a hard disk, allowing you to store data that needs to be retained even if the Pod is halted or rebooted. \\*\\*Key characteristics:\\*\\* - Persistent storage that remains","metadata":{"source":"/runpod-docs/docs/pods/storage/types.md","loc":{"lines":{"from":1,"to":1}}}}],["205",{"pageContent":"available throughout the Pod's lease period - Suitable for storing data, models, or files that need to be preserved across Pod restarts or reconfigurations - Capacity can be selected based on storage requirements - Provides reliable data persistence but may have slightly slower read and write speeds compared to container volumes ## Network storage Network storage is a type of storage that is similar to a disk volume but offers the flexibility to be moved between different machines. It provides a","metadata":{"source":"/runpod-docs/docs/pods/storage/types.md","loc":{"lines":{"from":1,"to":1}}}}],["206",{"pageContent":"way to store and access data across multiple Pods or instances. \\*\\*Key characteristics:\\*\\* - Persistent storage that can be attached to different Pods or machines - Suitable for scenarios where data needs to be shared or accessed by multiple Pods - Allows for data portability and facilitates collaboration between different instances - Provides data persistence and the ability to move storage between Pods - When using network storage, you can only delete the Pod, as the storage is managed","metadata":{"source":"/runpod-docs/docs/pods/storage/types.md","loc":{"lines":{"from":1,"to":1}}}}],["207",{"pageContent":"separately","metadata":{"source":"/runpod-docs/docs/pods/storage/types.md","loc":{"lines":{"from":1,"to":1}}}}],["208",{"pageContent":"\\--- title: Volumes description: \"Explore the concept of volumes in computing, where data can be stored as persistent or ephemeral resources, each with its own unique characteristics and applications.\" --- Volumes can be: ## Persistent ## Ephemeral","metadata":{"source":"/runpod-docs/docs/pods/storage/_volume.md","loc":{"lines":{"from":1,"to":1}}}}],["209",{"pageContent":"\\--- title: Manage Pod Templates description: \"Discover and create custom templates for your pods, define environment variables, and use RunPod's API to launch and manage your applications with ease.\" sidebar\\_position: 3 --- ## Explore Templates You can explore Templates managed by RunPod and Community Templates in the \\*\\*\\[Explore]\\(https://www.runpod.io/console/explore)\\*\\* section of the Web interface. You can explore Templates managed by you or your team in the","metadata":{"source":"/runpod-docs/docs/pods/templates/manage-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["210",{"pageContent":"\\*\\*\\[Templates]\\(https://www.runpod.io/console/user/templates)\\*\\* section of the Web interface. Learn to create your own Template in the following section. ## Creating a Template Templates are used to launch images as a Pod: within a template, you define the required container disk size, volume, volume path, and ports needed. ### Web interface !\\[]\\(/img/docs/8418b2b-image.png) ### cURL You can also create or modify a template using the RunPod API. \\`\\`\\`curl curl --request POST \\ --header","metadata":{"source":"/runpod-docs/docs/pods/templates/manage-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["211",{"pageContent":"'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \\\\\"sleep infinity\\\\\", env: \\[ { key: \\\\\"key1\\\\\", value: \\\\\"value1\\\\\" }, { key: \\\\\"key2\\\\\", value: \\\\\"value2\\\\\" } ], imageName: \\\\\"ubuntu:latest\\\\\", name: \\\\\"Generated Template\\\\\", ports: \\\\\"8888/http,22/tcp\\\\\", readme: \\\\\"## Hello, World!\\\\\", volumeInGb: 15, volumeMountPath: \\\\\"/workspace\\\\\" }) {","metadata":{"source":"/runpod-docs/docs/pods/templates/manage-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["212",{"pageContent":"containerDiskInGb dockerArgs env { key value } id imageName name ports readme volumeInGb volumeMountPath } }\"}' \\`\\`\\` ### Environment variables Environment variables in RunPod templates are key-value pairs that are accessible within your pod. Define a variable by setting a name with the \\`key\\` and then what it should contain with the \\`value\\`. Use environment variables to pass configuration settings and secrets to your container. For example, environment variables can store the path to a","metadata":{"source":"/runpod-docs/docs/pods/templates/manage-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["213",{"pageContent":"database or API keys used by your application. !\\[]\\(/img/docs/b7670dd-image.png) RunPod also provides a set of predefined \\[environment variables]\\(/pods/references/environment-variables) that provide information about the pod, such as the unique identifier for your pod (\\`RUNPOD\\_POD\\_ID\\`), the API key used to make RunPod API calls to the specific pod (\\`RUNPOD\\_API\\_KEY\\`), the name of the host server the pod is running on (\\`RUNPOD\\_POD\\_HOSTNAME\\`), and more. You can references","metadata":{"source":"/runpod-docs/docs/pods/templates/manage-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["214",{"pageContent":"\\[Secrets]\\(/pods/templates/secrets) in your Pod templates.","metadata":{"source":"/runpod-docs/docs/pods/templates/manage-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["215",{"pageContent":"\\--- title: Overview description: \"Docker templates: pre-configured images with customizable settings for deploying Pods, environment variables, and port management, with options for official, community, and custom templates.\" sidebar\\_position: 2 --- Templates are Docker containers images paired with a configuration. They are used to launch images as Pods, define the required container disk size, volume, volume paths, and ports needed. You can also define environment variables within the","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["216",{"pageContent":"Template. ## Template types There a few types of Templates: - \\*\\*Managed by RunPod\\*\\*: Also known as offical Templates; these templates are created and maintained by RunPod. - \\*\\*Custom Templates\\*\\*: - \\*\\*Community Templates\\*\\*: Custom Templates shared by the community. - \\*\\*Private Templates\\*\\*: Custom Templates created by you or if using a team account, shared inside your team. ### Custom Templates ### Customizing Container Start Command You can customize the Docker command to run","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["217",{"pageContent":"additional commands or modify the default behavior. The Docker command is specified in the \\*\\*Container Start Command\\*\\* field. \\*\\*Default Docker Command\\*\\* The default Docker command is: \\`\\`\\`bash bash -c '/start.sh' \\`\\`\\` This command runs the \\`/start.sh\\` script at the end of the container startup process. You can customize the Docker command to run additional commands or modify the default behavior. For example, you can add a command to run before \\`/start.sh\\`: \\`\\`\\`bash bash -c","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["218",{"pageContent":"'mkdir /testdir1 && /start.sh' \\`\\`\\` This command creates a directory \\`/testdir1\\` before running \\`/start.sh\\`. \\*\\*Using the \\`entrypoint\\` Field\\*\\* You can also specify a JSON string with \\`cmd\\` and \\`entrypoint\\` as the keys. The \\`entrypoint\\` field allows you to specify a command to run at the beginning of the container startup process. For example: \\`\\`\\`json { \"cmd\": \\[\"echo foo && /start.sh\"], \"entrypoint\": \\[\"bash\", \"-c\"] } \\`\\`\\` This command runs the \\`echo\\` command and then","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["219",{"pageContent":"runs \\`/start.sh\\`. \\*\\*Important Considerations\\*\\* When using the \\`entrypoint\\` field, be aware that the command will run twice: once as the entrypoint and again as part of the \\`cmd\\` field. This can cause issues if the command errors when run a second time. For example: \\`\\`\\`json { \"cmd\": \\[\"mkdir /testdir11 && /start.sh\"], \"entrypoint\": \\[\"bash\", \"-c\"] } \\`\\`\\` This command will run \\`mkdir\\` twice, which can cause errors if the directory already exists. \\*\\*Tips and Examples\\*\\* Here are","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["220",{"pageContent":"some working examples to try in dev: - Command only: \\`bash -c 'mkdir /testdir1 && /start.sh'\\` - Command only: \\`{\"cmd\": \\[\"bash\", \"-c\", \"mkdir /testdir8 && /start.sh\"]}\\` - Command and Entrypoint: \\`{\"cmd\": \\[\"test-echo-test-echo\"], \"entrypoint\": \\[\"echo\"]}\\` - Cmmand and Entrypoint: \\`{\"cmd\": \\[\"mkdir -p /testdir12 && /start.sh\"], \"entrypoint\": \\[\"bash\", \"-c\"]}\\` :::note Remember to use \\`mkdir -p\\` to avoid errors when creating directories. :::","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["221",{"pageContent":"\\--- title: Secrets description: \"Manage sensitive data with RunPod Secrets, encrypted strings for storing passwords, API keys, and more, via the Web interface or API, with options to create, modify, view, and delete Secrets for secure use in Pods and templates.\" sidebar\\_position: 4 --- You can add Secrets to your Pods and templates. Secrets are encrypted strings of text that are used to store sensitive information, such as passwords, API keys, and other sensitive data. ## Create a Secret You","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":1,"to":1}}}}],["222",{"pageContent":"can create a Secret using the RunPod Web interface or the RunPod API. 1. Login into the RunPod Web interface and select \\[Secrets]\\(https://www.runpod.io/console/user/secrets). 2. Choose \\*\\*Create Secret\\*\\* and provide the following: 1. \\*\\*Secret Name\\*\\*: The name of the Secret. 2. \\*\\*Secret Value\\*\\*: The value of the Secret. 3. \\*\\*Description\\*\\*: (optional) A description of the Secret. 3. Select \\*\\*Create Secret\\*\\*. :::note Once a Secret is created, its value cannot be viewed. If you","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":1,"to":1}}}}],["223",{"pageContent":"need to change the Secret, you must create a new one or \\[modify the Secret Value]\\(#modify-a-secret). ::: ## Modify a Secret You can modify an existing Secret using the RunPod Web interface. 1. Login into the RunPod Web interface and select \\[Secrets]\\(https://www.runpod.io/console/user/secrets). 2. Select the name of the Secret you want to modify. 3. Select the configuration icon and choose \\*\\*Edit Secret Value\\*\\*. 1. Enter your new Secret Value. 4. Select \\*\\*Save Changes\\*\\*. ## View","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":1,"to":1}}}}],["224",{"pageContent":"Secret details You can view the details of an existing Secret using the RunPod Web interface. You can't view the Secret Value. 1. Login into the RunPod Web interface and select \\[Secrets]\\(https://www.runpod.io/console/user/secrets). 2. Select the name of the Secret you want to view. 3. Select the configuration icon and choose \\*\\*View Secret\\*\\*. ## Use a Secret in a Pod With your Secrets setup, you can now reference them in your Pods. You can reference your Secret directly or select it from","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":1,"to":1}}}}],["225",{"pageContent":"the Web interface when creating or modifying a Pod template. \\*\\*Reference your Secret directly\\*\\* You can reference your Secret directly in the \\[Environment Variables]\\(/pods/references/environment-variables) section of your Pod template. To reference your Secret, reference it's key appended to the \\`RUNPOD\\_SECRET\\_\\` prefix. For example: \\`\\`\\`yml {{ RUNPOD\\_SECRET\\_hello\\_world }} \\`\\`\\` Where \\`hello\\_world\\` is the value of your Secret Name. \\*\\*Select your Secret from the Web","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":1,"to":1}}}}],["226",{"pageContent":"interface\\*\\* Alternatively, you can select your Secret from the Web interface when creating or modifying a Pod template. ## Delete a Secret You can delete an existing Secret using the RunPod Web interface. 1. Login into the RunPod Web interface and select \\[Secrets]\\(https://www.runpod.io/console/user/secrets). 2. Select the name of the Secret you want to delete. 3. Select the configuration icon and choose \\*\\*Delete Secret\\*\\*. 4. Enter the name of the Secret to confirm deletion. 5. Select","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":1,"to":1}}}}],["227",{"pageContent":"\\*\\*Confirm Delete\\*\\*.","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":1,"to":1}}}}],["228",{"pageContent":"\\--- title: Serverless CPU types --- The following list contains all CPU types available on RunPod. <!-- Table last generated: 2024-06-04 --> | displayName | cores | threadsPerCore | | : | ----: | : | | 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz | 6 | 2 | | 11th Gen Intel(R) Core(TM) i5-11400F @ 2.60GHz | 6 | 2 | | 11th Gen Intel(R) Core(TM) i7-11700 @ 2.50GHz | 8 | 2 | | 11th Gen Intel(R) Core(TM) i7-11700F @ 2.50GHz | 8 | 2 | | 11th Gen Intel(R) Core(TM) i7-11700K @ 3.60GHz | 8 | 2 | | 11th","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["229",{"pageContent":"Gen Intel(R) Core(TM) i7-11700KF @ 3.60GHz | 8 | 2 | | 11th Gen Intel(R) Core(TM) i9-11900K @ 3.50GHz | 8 | 2 | | 11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz | 8 | 2 | | 12th Gen Intel(R) Core(TM) i3-12100 | 4 | 2 | | 12th Gen Intel(R) Core(TM) i7-12700F | 12 | 1 | | 12th Gen Intel(R) Core(TM) i7-12700K | 12 | 1 | | 13th Gen Intel(R) Core(TM) i3-13100F | 4 | 2 | | 13th Gen Intel(R) Core(TM) i5-13600K | 14 | 1 | | 13th Gen Intel(R) Core(TM) i7-13700K | 16 | 1 | | 13th Gen Intel(R) Core(TM)","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["230",{"pageContent":"i7-13700KF | 16 | 1 | | 13th Gen Intel(R) Core(TM) i9-13900F | 24 | 1 | | 13th Gen Intel(R) Core(TM) i9-13900K | 24 | 1 | | 13th Gen Intel(R) Core(TM) i9-13900KF | 24 | 1 | | AMD EPYC 7251 8-Core Processor | 8 | 2 | | AMD EPYC 7252 8-Core Processor | 8 | 2 | | AMD EPYC 7272 12-Core Processor | 12 | 2 | | AMD EPYC 7281 16-Core Processor | 16 | 2 | | AMD EPYC 7282 16-Core Processor | 16 | 2 | | AMD EPYC 7302 16-Core Processor | 16 | 2 | | AMD EPYC 7302P 16-Core Processor | 16 | 2 | | AMD EPYC 7313","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["231",{"pageContent":"16-Core Processor | 16 | 2 | | AMD EPYC 7313P 16-Core Processor | 16 | 2 | | AMD EPYC 7343 16-Core Processor | 16 | 2 | | AMD EPYC 7351P 16-Core Processor | 16 | 2 | | AMD EPYC 7352 24-Core Processor | 24 | 2 | | AMD EPYC 7402 24-Core Processor | 24 | 2 | | AMD EPYC 7402P 24-Core Processor | 24 | 2 | | AMD EPYC 7413 24-Core Processor | 24 | 2 | | AMD EPYC 7443 24-Core Processor | 48 | 1 | | AMD EPYC 7443P 24-Core Processor | 24 | 2 | | AMD EPYC 7452 32-Core Processor | 32 | 2 | | AMD EPYC 7453","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["232",{"pageContent":"28-Core Processor | 28 | 1 | | AMD EPYC 7502 32-Core Processor | 32 | 1 | | AMD EPYC 7502P 32-Core Processor | 32 | 1 | | AMD EPYC 7513 32-Core Processor | 32 | 2 | | AMD EPYC 7532 32-Core Processor | 32 | 2 | | AMD EPYC 7542 32-Core Processor | 32 | 2 | | AMD EPYC 7543 32-Core Processor | 28 | 1 | | AMD EPYC 7543P 32-Core Processor | 32 | 2 | | AMD EPYC 7551 32-Core Processor | 32 | 2 | | AMD EPYC 7551P 32-Core Processor | 32 | 2 | | AMD EPYC 7552 48-Core Processor | 48 | 2 | | AMD EPYC 75F3","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["233",{"pageContent":"32-Core Processor | 32 | 2 | | AMD EPYC 7601 32-Core Processor | 32 | 2 | | AMD EPYC 7642 48-Core Processor | 48 | 2 | | AMD EPYC 7643 48-Core Processor | 48 | 2 | | AMD EPYC 7663 56-Core Processor | 56 | 2 | | AMD EPYC 7702 64-Core Processor | 64 | 2 | | AMD EPYC 7702P 64-Core Processor | 64 | 2 | | AMD EPYC 7713 64-Core Processor | 64 | 1 | | AMD EPYC 7742 64-Core Processor | 64 | 2 | | AMD EPYC 7763 64-Core Processor | 64 | 2 | | AMD EPYC 7773X 64-Core Processor | 64 | 2 | | AMD EPYC 7B12","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["234",{"pageContent":"64-Core Processor | 64 | 2 | | AMD EPYC 7B13 64-Core Processor | 64 | 1 | | AMD EPYC 7F32 8-Core Processor | 8 | 2 | | AMD EPYC 7F72 24-Core Processor | 24 | 2 | | AMD EPYC 7H12 64-Core Processor | 64 | 2 | | AMD EPYC 7R32 48-Core Processor | 48 | 2 | | AMD EPYC 7T83 64-Core Processor | 127 | 1 | | AMD EPYC 7V13 64-Core Processor | 24 | 1 | | AMD EPYC 9124 16-Core Processor | 16 | 2 | | AMD EPYC 9254 24-Core Processor | 24 | 2 | | AMD EPYC 9354 32-Core Processor | 32 | 2 | | AMD EPYC 9354P","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["235",{"pageContent":"32-Core Processor | 32 | 2 | | AMD EPYC 9374F 32-Core Processor | 32 | 1 | | AMD EPYC 9474F 48-Core Processor | 48 | 2 | | AMD EPYC 9554 64-Core Processor | 126 | 1 | | AMD EPYC 9654 96-Core Processor | 96 | 2 | | AMD EPYC 9754 128-Core Processor | 128 | 2 | | AMD EPYC Processor | 1 | 1 | | AMD EPYC Processor (with IBPB) | 16 | 1 | | AMD EPYC-Rome Processor | 16 | 1 | | AMD Eng Sample: 100-000000053-04\\_32/20\\_N | 48 | 1 | | AMD Ryzen 3 2200G with Radeon Vega Graphics | 4 | 1 | | AMD Ryzen 3","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["236",{"pageContent":"3200G with Radeon Vega Graphics | 4 | 1 | | AMD Ryzen 3 4100 4-Core Processor | 4 | 2 | | AMD Ryzen 5 1600 Six-Core Processor | 6 | 2 | | AMD Ryzen 5 2600 Six-Core Processor | 6 | 2 | | AMD Ryzen 5 2600X Six-Core Processor | 6 | 2 | | AMD Ryzen 5 3600 6-Core Processor | 6 | 2 | | AMD Ryzen 5 3600X 6-Core Processor | 6 | 2 | | AMD Ryzen 5 5500 | 6 | 2 | | AMD Ryzen 5 5600G with Radeon Graphics | 6 | 2 | | AMD Ryzen 5 7600 6-Core Processor | 6 | 2 | | AMD Ryzen 5 PRO 2600 Six-Core Processor | 6 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["237",{"pageContent":"2 | | AMD Ryzen 7 1700 Eight-Core Processor | 8 | 2 | | AMD Ryzen 7 1700X Eight-Core Processor | 8 | 2 | | AMD Ryzen 7 5700G with Radeon Graphics | 8 | 2 | | AMD Ryzen 7 5700X 8-Core Processor | 8 | 2 | | AMD Ryzen 7 5800X 8-Core Processor | 8 | 2 | | AMD Ryzen 7 7700 8-Core Processor | 8 | 2 | | AMD Ryzen 9 3900X 12-Core Processor | 12 | 2 | | AMD Ryzen 9 5950X 16-Core Processor | 16 | 2 | | AMD Ryzen 9 7950X 16-Core Processor | 16 | 2 | | AMD Ryzen Threadripper 1900X 8-Core Processor | 8 | 2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["238",{"pageContent":"| AMD Ryzen Threadripper 1920X 12-Core Processor | 12 | 2 | | AMD Ryzen Threadripper 1950X 16-Core Processor | 16 | 2 | | AMD Ryzen Threadripper 2920X 12-Core Processor | 12 | 2 | | AMD Ryzen Threadripper 2950X 16-Core Processor | 16 | 2 | | AMD Ryzen Threadripper 2970WX 24-Core Processor | 24 | 1 | | AMD Ryzen Threadripper 2990WX 32-Core Processor | 32 | 2 | | AMD Ryzen Threadripper 3960X 24-Core Processor | 24 | 2 | | AMD Ryzen Threadripper PRO 3975WX 32-Cores | 32 | 2 | | AMD Ryzen","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["239",{"pageContent":"Threadripper PRO 3995WX 64-Cores | 64 | 2 | | AMD Ryzen Threadripper PRO 5945WX 12-Cores | 12 | 2 | | AMD Ryzen Threadripper PRO 5955WX 16-Cores | 16 | 2 | | AMD Ryzen Threadripper PRO 5965WX 24-Cores | 24 | 2 | | AMD Ryzen Threadripper PRO 5975WX 32-Cores | 32 | 2 | | AMD Ryzen Threadripper PRO 5995WX 64-Cores | 18 | 1 | | AMD Ryzen Threadripper PRO 7985WX 64-Cores | 112 | 1 | | Common KVM processor | 28 | 1 | | Genuine Intel(R) CPU $0000%@ | 24 | 2 | | Genuine Intel(R) CPU @ 2.20GHz | 14 | 2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["240",{"pageContent":"| Intel Xeon Processor (Icelake) | 40 | 2 | | Intel(R) Celeron(R) CPU G3900 @ 2.80GHz | 2 | 1 | | Intel(R) Celeron(R) G5905 CPU @ 3.50GHz | 2 | 1 | | Intel(R) Core(TM) i3-10100F CPU @ 3.60GHz | 4 | 2 | | Intel(R) Core(TM) i3-10105F CPU @ 3.70GHz | 4 | 2 | | Intel(R) Core(TM) i3-6100 CPU @ 3.70GHz | 2 | 2 | | Intel(R) Core(TM) i3-9100F CPU @ 3.60GHz | 4 | 1 | | Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz | 6 | 2 | | Intel(R) Core(TM) i5-10400F CPU @ 2.90GHz | 6 | 2 | | Intel(R) Core(TM) i5-10600 CPU","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["241",{"pageContent":"@ 3.30GHz | 6 | 2 | | Intel(R) Core(TM) i5-4570 CPU @ 3.20GHz | 4 | 1 | | Intel(R) Core(TM) i5-6400 CPU @ 2.70GHz | 4 | 1 | | Intel(R) Core(TM) i5-6500 CPU @ 3.20GHz | 4 | 1 | | Intel(R) Core(TM) i5-7400 CPU @ 3.00GHz | 4 | 1 | | Intel(R) Core(TM) i5-9400F CPU @ 2.90GHz | 6 | 1 | | Intel(R) Core(TM) i7-10700F CPU @ 2.90GHz | 8 | 2 | | Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz | 8 | 2 | | Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz | 4 | 2 | | Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz | 4 | 2 | |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["242",{"pageContent":"Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz | 4 | 2 | | Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz | 4 | 2 | | Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz | 6 | 2 | | Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz | 4 | 2 | | Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz | 6 | 2 | | Intel(R) Core(TM) i7-9700 CPU @ 3.00GHz | 8 | 1 | | Intel(R) Core(TM) i9-10940X CPU @ 3.30GHz | 14 | 2 | | Intel(R) Core(TM) i9-14900K | 24 | 1 | | Intel(R) Pentium(R) CPU G3260 @ 3.30GHz | 2 | 1 | | Intel(R) Pentium(R) CPU G4560 @","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["243",{"pageContent":"3.50GHz | 2 | 2 | | Intel(R) Xeon(R) Bronze 3204 CPU @ 1.90GHz | 6 | 1 | | Intel(R) Xeon(R) CPU X5660 @ 2.80GHz | 6 | 2 | | Intel(R) Xeon(R) CPU E3-1220 v3 @ 3.10GHz | 4 | 1 | | Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz | 4 | 1 | | Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz | 6 | 2 | | Intel(R) Xeon(R) CPU E5-2603 v3 @ 1.60GHz | 6 | 1 | | Intel(R) Xeon(R) CPU E5-2609 0 @ 2.40GHz | 4 | 1 | | Intel(R) Xeon(R) CPU E5-2609 v3 @ 1.90GHz | 1 | 1 | | Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz | 8 | 2 | |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["244",{"pageContent":"Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz | 6 | 2 | | Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz | 6 | 2 | | Intel(R) Xeon(R) CPU E5-2637 v2 @ 3.50GHz | 4 | 2 | | Intel(R) Xeon(R) CPU E5-2643 0 @ 3.30GHz | 4 | 1 | | Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz | 10 | 2 | | Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz | 12 | 2 | | Intel(R) Xeon(R) CPU E5-2660 v2 @ 2.20GHz | 10 | 2 | | Intel(R) Xeon(R) CPU E5-2667 v2 @ 3.30GHz | 1 | 1 | | Intel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz | 8 | 2 | | Intel(R)","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["245",{"pageContent":"Xeon(R) CPU E5-2667 v4 @ 3.20GHz | 1 | 1 | | Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz | 8 | 2 | | Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz | 10 | 2 | | Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz | 20 | 2 | | Intel(R) Xeon(R) CPU E5-2678 v3 @ 2.50GHz | 12 | 2 | | Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz | 12 | 2 | | Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz | 14 | 2 | | Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz | 16 | 2 | | Intel(R) Xeon(R) CPU E5-2690 0 @ 2.90GHz | 8 | 2 | | Intel(R) Xeon(R)","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["246",{"pageContent":"CPU E5-2690 v4 @ 2.60GHz | 14 | 2 | | Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz | 18 | 2 | | Intel(R) Xeon(R) CPU E5-2696 v3 @ 2.30GHz | 18 | 2 | | Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz | 22 | 2 | | Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz | 16 | 2 | | Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz | 20 | 2 | | Intel(R) Xeon(R) CPU E5-2699 v3 @ 2.30GHz | 1 | 1 | | Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz | 22 | 2 | | Intel(R) Xeon(R) CPU E5-4667 v3 @ 2.00GHz | 16 | 2 | | Intel(R) Xeon(R) Gold","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["247",{"pageContent":"5118 CPU @ 2.30GHz | 12 | 2 | | Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz | 20 | 2 | | Intel(R) Xeon(R) Gold 5320 CPU @ 2.20GHz | 26 | 2 | | Intel(R) Xeon(R) Gold 6133 CPU @ 2.50GHz | 40 | 1 | | Intel(R) Xeon(R) Gold 6136 CPU @ 3.00GHz | 12 | 2 | | Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz | 20 | 2 | | Intel(R) Xeon(R) Gold 6150 CPU @ 2.70GHz | 18 | 2 | | Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz | 12 | 2 | | Intel(R) Xeon(R) Gold 6238R CPU @ 2.20GHz | 28 | 2 | | Intel(R) Xeon(R) Gold 6240R CPU @","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["248",{"pageContent":"2.40GHz | 24 | 2 | | Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz | 16 | 1 | | Intel(R) Xeon(R) Gold 6252 CPU @ 2.10GHz | 24 | 1 | | Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz | 22 | 2 | | Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz | 24 | 2 | | Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz | 28 | 2 | | Intel(R) Xeon(R) Gold 6448Y | 32 | 2 | | Intel(R) Xeon(R) Platinum 8160 CPU @ 2.10GHz | 24 | 2 | | Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz | 26 | 2 | | Intel(R) Xeon(R) Platinum 8173M CPU @ 2.00GHz |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["249",{"pageContent":"28 | 2 | | Intel(R) Xeon(R) Platinum 8176M CPU @ 2.10GHz | 28 | 2 | | Intel(R) Xeon(R) Platinum 8180 CPU @ 2.50GHz | 28 | 2 | | Intel(R) Xeon(R) Platinum 8352Y CPU @ 2.20GHz | 32 | 2 | | Intel(R) Xeon(R) Platinum 8452Y | 36 | 2 | | Intel(R) Xeon(R) Platinum 8468 | 48 | 2 | | Intel(R) Xeon(R) Platinum 8470 | 52 | 2 | | Intel(R) Xeon(R) Platinum 8480+ | 56 | 2 | | Intel(R) Xeon(R) Platinum 8480C | 56 | 2 | | Intel(R) Xeon(R) Platinum 8480CL | 56 | 2 | | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["250",{"pageContent":"10 | 2 | | Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz | 10 | 2 | | Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz | 24 | 1 | | Intel(R) Xeon(R) Silver 4310T CPU @ 2.30GHz | 10 | 2 | | Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz | 16 | 2 | | Intel(R) Xeon(R) W-2223 CPU @ 3.60GHz | 4 | 2 | | QEMU Virtual CPU version 2.5+ | 16 | 1 | | Ryzen 5 5600X | 6 | 2 | | Ryzen 9 5900X | 12 | 2 | | Ryzen Threadripper PRO 3955WX | 16 | 2 | | unknown | 0 | 0 | | nan | nan | nan | | nan | nan | nan |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["251",{"pageContent":"\\--- title: \"FAQ\" description: \"RunPod offers two cloud computing services: Secure Cloud and Community Cloud. Secure Cloud provides high-reliability, while Community Cloud offers peer-to-peer GPU computing. On-Demand Pods run continuously, while Spot Pods use spare compute capacity.\" --- ## Secure Cloud vs Community Cloud RunPod provides two cloud computing services: \\[Secure Cloud]\\(https://www.runpod.io/console/gpu-secure-cloud) and \\[Community Cloud.]\\(https://www.runpod.io/console/gpu-cloud)","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["252",{"pageContent":"\\*\\*Secure Cloud\\*\\* runs in T3/T4 data centers by our trusted partners. Our close partnership comes with high-reliability with redundancy, security, and fast response times to mitigate any downtimes. For any sensitive and enterprise workloads, we highly recommend Secure Cloud. \\*\\*Community Cloud\\*\\* brings power in numbers and diversity spanning the whole world. Through our decentralized platform, we can offer peer-to-peer GPU computing that connects individual compute providers to compute","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["253",{"pageContent":"consumers. Our Community Cloud hosts are invite-only and vetted by us, and still have to abide by our standards. Even though their associated infrastructure might not offer as much redundancy for power and networking, they still offer good servers that combine quality and affordability. Both solutions offer far more competitive prices than large cloud providers such as AWS or GCP. ## On-Demand vs. Spot Pod \\*\\*On-Demand Pods\\*\\* can run forever without interruptions with resources dedicated to","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["254",{"pageContent":"your Pod. They do incur higher costs than Spot Pods. \\*\\*Spot Pods\\*\\* use spare compute capacity, allowing you to bid for those compute resources. Resources are dedicated to your Pod, but someone else can bid higher or start an On-Demand Pod that will stop your Pod. When this happens, your Pod is given a signal to stop 5 seconds prior with SIGTERM, and eventually, the kill signal SIGKILL after 5 seconds. You can use volumes to save any data to the disk in that 5s period or push data to the","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["255",{"pageContent":"cloud periodically. ### How does RunPod work? RunPod leverages technologies like \\[Docker]\\(https://www.docker.com/) to containerize and isolate guest workloads on a host machine. We have built a decentralized platform where thousands of servers can be connected to offer a seamless experience for all users. ### Where can I go for help? We'd be happy to help! Join our community on \\[Discord]\\(https://discord.gg/pJ3P2DbUUq), message us in our support chat, or email us at","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["256",{"pageContent":"\\[help@runpod.io]\\(mailto:help@runpod.io). ### What is RunPod's policy on refunds and credits? If you aren't sure if RunPod is for you, feel free to hang out in our \\[Discord]\\(https://discord.gg/cUpRmau42V) to ask questions or email \\[help@runpod.io]\\(mailto:help@runpod.io) You can load as little as $10 into your account to try things out. We don't currently offer refunds or trial credits due to the overhead of processing these requests. Please plan accordingly! ## What are Pods? --- ### What","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["257",{"pageContent":"is an On-Demand instance? \\*\\*On-Demand instances\\*\\* are for non-interruptible workloads. You pay the On-Demand price and cannot be displaced by other users if you have funds to keep your Pod running. ### What is a Spot instance? A \\*\\*Spot instance\\*\\* is an interruptible instance that can generally be rented much cheaper than an On-Demand one. Spot instances are great for stateless workloads like an API or for workloads you can periodically save to a volume disk. Your volume disk is retained","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["258",{"pageContent":"even if your Spot instance is interrupted. ### What is a Savings Plan? Savings Plans are a way for you to pay up-front and get a discount for it. This is great for when you know you will need prolonged access to compute. You can learn more on the about \\[Savings Plans here]\\(/pods/savings-plans). ## Billing All billing, including per-hour compute and storage billing, is charged per minute. ### How does Pod billing work? Every Pod has an hourly cost based on GPU type. Your RunPod credits are","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["259",{"pageContent":"charged for the Pod every minute as long as the Pod is running. If you ever run out of credits, your Pods will be automatically stopped, and you will get an email notification. Eventually, Pods will be terminated if you don't refill your credit. \\*\\*We pre-emptively stop all of your Pods if you get down to 10 minutes of remaining run time. This gives your account enough balance to keep your data volumes around in the case you need access to your data. Please plan accordingly.\\*\\* Once a balance","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["260",{"pageContent":"has been completely drained, all pods are subject to deletion at the discretion of the service. An attempt will be made to hold the pods for as long as possible, but this should not be relied upon! We highly recommend setting up \\[automatic payments]\\(https://www.runpod.io/console/user/billing) to ensure balances are automatically topped up as needed. :::note You must have at least one hour's worth of time in your balance to rent a Pod at your given spec. If your balance is insufficient to rent","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["261",{"pageContent":"a Pod, then consider renting the Pod on Spot, depositing additional funds, or lowering your GPU spec requirements. ::: ### How does storage billing work? We currently charge $0.10 GB per month for all storage on running Pods and $0.20 GB per month for volume storage on stopped Pods. Storage is tied to compute servers, and we want to ensure active users have enough space to run their workloads. Storage is charged per minute, and we never charge users if the host machine is down or unavailable","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["262",{"pageContent":"from the public internet. ### How does Network Volume billing work? For storage requirements below 1TB, we charge a competitive rate of $0.07/GB/Month. If your storage requirements exceed 1TB, we provide a cost-effective pricing of $0.05/GB/Month. This ensures that you receive significant savings as your data storage scales. When you choose to create a Network Volume, you gain access to our robust infrastructure, which includes state-of-the-art storage servers located in the same datacenters","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["263",{"pageContent":"where you rent GPU servers from us. These servers are connected via a high-speed 25Gbps local network, up to 200 Gbps in some locations, guaranteeing efficient data transfer and minimal latency. Everything is stored on high-speed NVME SSDs to ensure best performance. Network volumes are billed on a per-hour basis. Please note that if your machine-based storage or network volume is terminated due to lack of funds, that disk space is immediately freed up for use by other clients, and RunPod is","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["264",{"pageContent":"unable to assist in recovering lost storage. RunPod is also not designed to be a cloud storage system; storage is provided in the pursuit of running tasks using its GPUs, and not meant to be a long-term backup solution. It is highly advisable to continually back up anything you want to save offsite locally or to a cloud provider. ## Security --- ### Is my data protected from other clients? Yes. Your data is run in a multi-tenant environment where other clients can't access your pod. For","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["265",{"pageContent":"sensitive workloads requiring the best security, please use Secure Cloud. ### Is my data protected from the host of the machine my Pod is running on? Data privacy is important to us at RunPod. Our Terms of Service prohibit hosts from trying to inspect your Pod data or usage patterns in any way. If you want the highest level of security, use Secure Cloud. ## Usability --- ### What can I do in a RunPod Pod? You can run any Docker container available on any publicly reachable container registry. If","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["266",{"pageContent":"you are not well versed in containers, we recommend sticking with the default run templates like our RunPod PyTorch template. However, if you know what you are doing, you can do a lot more! ### Can I run my own Docker daemon on RunPod? You can't currently spin up your own instance of Docker, as we run Docker for you! Unfortunately, this means that you cannot currently build Docker containers on RunPod or use things like Docker Compose. Many use cases can be solved by creating a custom template","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["267",{"pageContent":"with the Docker image that you want to run. ### My Pod is stuck on initializing. What gives? Usually, this happens for one of several reasons. If you can't figure it out, \\[contact us]\\(https://www.runpod.io/contact), and we'll gladly help you. 1. You are trying to run a Pod to SSH into, but you did not give the Pod an idle job to run like \"sleep infinity.\" 2. You have given your Pod a command that it doesn't know how to run. Check the logs to make sure that you don't have any syntax errors,","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["268",{"pageContent":"etc. ### Can I run Windows? We don't currently support Windows. We want to do this in the future, but we do not have a solid timeframe for Windows support. ### How do I find a reliable server in Community Cloud? RunPod needs to provide you with reliable servers. All of our listed servers must meet minimum reliability, and most are running in a data center! However, if you want the highest level of reliability and security, use Secure Cloud. RunPod calculates server reliability by maintaining a","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["269",{"pageContent":"heartbeat with each server in real-time. ### Why do I have zero GPUs assigned to my Pod? If you want to avoid this, using network volumes is the best choice. \\[Read about it here.]\\(https://blog.runpod.io/four-reasons-to-set-up-a/) \\[Learn how to use them here]\\(https://docs.runpod.io/docs/create-a-network-volume). Most of our machines have between 4 and 8 GPUs per physical machine. When you start a Pod, it is locked to a specific physical machine. If you keep it running (On-Demand), then that","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["270",{"pageContent":"GPU cannot be taken from you. However, if you stop your Pod, it becomes available for a different user to rent. When you want to start your Pod again, your specific machine may be wholly occupied! In this case, we give you the option to spin up your Pod with zero GPUs so you can retain access to your data. Remember that this does not mean there are no more GPUs of that type available, just none on the physical machine that specific Pod is locked to. Note that transfer Pods have limited computing","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["271",{"pageContent":"capabilities, so transferring files using a UI may be difficult, and you may need to resort to terminal access or cloud sync options. #### What are Network Volumes? Network volumes allow you to share data between Pods and generally be more mobile with your important data. This feature is only available in specific secure cloud data centers, but we are actively rolling it out to more and more of our secure cloud footprint. If you use network volumes, you should rarely run into situations where","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["272",{"pageContent":"you cannot use your data with a GPU without a file transfer! \\[Read about it here]\\(https://blog.runpod.io/four-reasons-to-set-up-a/). ## What if? --- ### What if I run out of funds? All your Pods are stopped automatically when you don't have enough funds to keep your Pods running for at least ten more minutes. When your Pods are stopped, your container disk data will be lost, but your volume data will be preserved. Pods are scheduled for removal if adequate credit balance is not maintained. If","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["273",{"pageContent":"you fail to do so, your Pods will be terminated, and Pod volumes will be removed. After you add more funds to your account, you can start your Pod if you wish (assuming enough GPUs are available on the host machine). ### What if the machine that my Pod is running loses power? If the host machine loses power, it will attempt to start your Pod again when it returns online. Your volume data will be preserved, and your container will run the same command as it ran the first time you started renting","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["274",{"pageContent":"it. Your container disk and anything in memory will be lost! ### What if my Pod loses internet connectivity? The host machine continues to run your Pod to the best of its ability, even if it is not connected to the internet. If your job requires internet connectivity, then it will not function. You will not be charged if the host loses internet connectivity, even if it continues to run your job. You may, of course, request that your Pod exit while the host is offline, and it will exit your Pod","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["275",{"pageContent":"when it regains network connectivity. ### What if it says that my spending limit has been exceeded? We implement a spending limit for newer accounts that will grow over time. This is because we have found that sometimes scammers try to interfere with the natural workings of the platform. We believe that this limit should not impact normal usage. We would be delighted to up your spending limit if you \\[contact us]\\(https://www.runpod.io/contact) and share your use case. ## Legal --- ### Do you","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["276",{"pageContent":"have some legal stuff I can look at? Sure, do! Take a look at our \\[legal page]\\(https://www.runpod.io/legal). ## GDPR Compliance At Runpod, we take data protection and privacy seriously. We have implemented robust policies, procedures, and technical measures to ensure compliance with the GDPR requirements. ### Is RunPod compliant with GDPR for data processed in Europe? Yes, RunPod is fully compliant with the General Data Protection Regulation (GDPR) requirements for any data processed within","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["277",{"pageContent":"our European data center regions. ### What measures does RunPod take to ensure GDPR compliance? For servers hosted in GDPR-compliant regions like the European Union, we ensure: - \\*\\*Data processing procedures\\*\\*: We have established clear procedures for the collection, storage, processing, and deletion of personal data, ensuring transparency and accountability in our data processing activities. - \\*\\*Data protection measures\\*\\*: We have implemented appropriate technical and organizational","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["278",{"pageContent":"measures to safeguard personal data against unauthorized access, disclosure, alteration, and destruction. - \\*\\*Consent mechanisms\\*\\*: We obtain and record consent from individuals for the processing of their personal data in accordance with GDPR requirements, and we provide mechanisms for individuals to withdraw consent if desired. - \\*\\*Rights of data subjects\\*\\*: We facilitate the rights of data subjects under the GDPR, including the right to access, rectify, erase, or restrict the","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["279",{"pageContent":"processing of their personal data, and we handle data subject requests promptly and efficiently. - \\*\\*Data transfer mechanisms\\*\\*: We ensure lawful and secure transfer of personal data outside the EU, where applicable, in compliance with GDPR requirements, utilizing appropriate mechanisms such as adequacy decisions, standard contractual clauses, or binding corporate rules. - \\*\\*Compliance monitoring\\*\\*: We regularly monitor and review our GDPR compliance to ensure ongoing effectiveness and","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["280",{"pageContent":"adherence to regulatory requirements, conducting data protection impact assessments and internal audits as needed. For any inquiries or concerns regarding our GDPR compliance or our data protection practices, reach out to our team through email at \\[support@runpod.io]\\(mailto:support@runpod.io).","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":1}}}}],["281",{"pageContent":"\\--- title: Manage Payment Card Declines description: \"RunPod helps global clients with card declines due to international transactions being flagged as potential fraud. Follow steps to minimize interruptions: keep a balanced account, contact the issuing bank, and consider account risk profiles.\" --- RunPod is a US-based organization that serves clients all across the world. However, credit card processors have in general keyed into international transactions as a potential vector for fraud and","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":1,"to":1}}}}],["282",{"pageContent":"tend to apply more stringent standards for blocking transactions. If your card is declined, don't panic! To minimize potential interruptions to your service, you'll want to follow these steps. \\*\\*Keep your balance topped up\\*\\* To avoid any potential issues with your balance being overrun, it's best to refresh your balance at least a few days before you're due to run out so you have a chance to address any last minute delays. Also be aware that there is an option to automatically refresh your","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":1,"to":1}}}}],["283",{"pageContent":"balance when you run low under the Billing \\[page]\\(https://www.runpod.io/console/user/billing): !\\[]\\(/img/docs/739337f-image.png) \\*\\*Call the bank that issued your card\\*\\* Once you do experience a card decline, the first step you'll want to do is to contact your issuing bank to see why a card is declined. Due to consumer/merchant privacy standards in the US, we are not provided with a reason that the card is declined, only that the transaction was not processed. Only your issuing bank can","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":1,"to":1}}}}],["284",{"pageContent":"specifically tell you why a payment was declined. Many times, declines are for completely innocent reasons, such as your bank's anti-fraud protection tripping; just the same, RunPod is unable to assist with blocks put in place by your bank. It's important that you call your bank for the initial decline before trying a different card, because the processor may block \\_all\\_ funding attempts from an account if it seems declines from multiple cards for the same account, even if these attempts would","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":1,"to":1}}}}],["285",{"pageContent":"have otherwise not had any problems. These account blocks generally clear after 24 hours, but it may be difficult to load the account until then. \\*\\*Other potential reasons for card blocks\\*\\* Our payment processor may block cards for specific users based on their risk profile, so certain use patterns may trigger a block. If you use several different cards within a short period time, or have had disputed transactions in the past, this may also cause cards to decline. To see a list of supported","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":1,"to":1}}}}],["286",{"pageContent":"cards on Stripe, \\[click here]\\(https://stripe.com/docs/payments/cards/supported-card-brands>). \\*\\*Contact us for support\\*\\* If all else fails, then feel free to contact \\[RunPod support]\\(https://www.runpod.io/contact) if you are still having trouble loading your account. We ask that you check with your bank first, but if everything checks out on your end, we will be glad to help!","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":1,"to":1}}}}],["287",{"pageContent":"\\--- title: GPU types --- The following list contains all GPU types available on RunPod. For more information, see \\[GPU pricing]\\(https://www.runpod.io/gpu-instance/pricing). <!-- Table last generated: 2024-06-04 --> | GPU ID | Display Name | Memory (GB) | | | | | | NVIDIA A100 80GB PCIe | A100 PCIe | 80 | | NVIDIA A100-SXM4-80GB | A100 SXM | 80 | | NVIDIA A30 | A30 | 24 | | NVIDIA A40 | A40 | 48 | | NVIDIA H100 NVL | H100 NVL | 94 | | NVIDIA H100 PCIe | H100 PCIe | 80 | | NVIDIA H100 80GB HBM3","metadata":{"source":"/runpod-docs/docs/references/gpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["288",{"pageContent":"| H100 SXM | 80 | | NVIDIA L4 | L4 | 24 | | NVIDIA L40 | L40 | 48 | | NVIDIA L40S | L40S | 48 | | AMD Instinct MI300X OAM | MI300X | 192 | | NVIDIA GeForce RTX 3070 | RTX 3070 | 8 | | NVIDIA GeForce RTX 3080 | RTX 3080 | 10 | | NVIDIA GeForce RTX 3080 Ti | RTX 3080 Ti | 12 | | NVIDIA GeForce RTX 3090 | RTX 3090 | 24 | | NVIDIA GeForce RTX 3090 Ti | RTX 3090 Ti | 24 | | NVIDIA RTX 4000 Ada Generation | RTX 4000 Ada | 20 | | NVIDIA RTX 4000 SFF Ada Generation | RTX 4000 Ada SFF | 20 | | NVIDIA","metadata":{"source":"/runpod-docs/docs/references/gpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["289",{"pageContent":"GeForce RTX 4070 Ti | RTX 4070 Ti | 12 | | NVIDIA GeForce RTX 4080 | RTX 4080 | 16 | | NVIDIA GeForce RTX 4090 | RTX 4090 | 24 | | NVIDIA RTX 5000 Ada Generation | RTX 5000 Ada | 32 | | NVIDIA RTX 6000 Ada Generation | RTX 6000 Ada | 48 | | NVIDIA RTX A2000 | RTX A2000 | 6 | | NVIDIA RTX A4000 | RTX A4000 | 16 | | NVIDIA RTX A4500 | RTX A4500 | 20 | | NVIDIA RTX A5000 | RTX A5000 | 24 | | NVIDIA RTX A6000 | RTX A6000 | 48 | | Tesla V100-PCIE-16GB | Tesla V100 | 16 | | Tesla V100-FHHL-16GB | V100","metadata":{"source":"/runpod-docs/docs/references/gpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["290",{"pageContent":"FHHL | 16 | | Tesla V100-SXM2-16GB | V100 SXM2 | 16 | | Tesla V100-SXM2-32GB | V100 SXM2 32GB | 32 |","metadata":{"source":"/runpod-docs/docs/references/gpu-types.md","loc":{"lines":{"from":1,"to":1}}}}],["291",{"pageContent":"\\--- title: \"runpodctl\" --- You can use RunPod's CLI \\[runpodctl]\\(https://github.com/runpod/runpodctl) to manage Pods. The runpodctl is a tool for managing your Pods on RunPod. All Pods come with \\`runpodctl\\` installed with a Pod-scoped API key, which makes managing your Pods easier through the command line. Choose one of the following methods to install the RunPod CLI. ### MacOs \\*\\*ARM\\*\\* \\`\\`\\`bash wget --quiet --show-progress","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["292",{"pageContent":"https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-darwin-arm64 -O runpodctl && chmod +x runpodctl && sudo mv runpodctl /usr/local/bin/runpodctl \\`\\`\\` \\*\\*AMD\\*\\* \\`\\`\\`bash wget --quiet --show-progress https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-darwin-amd64 -O runpodctl && chmod +x runpodctl && sudo mv runpodctl /usr/local/bin/runpodctl \\`\\`\\` ### Linux \\`\\`\\`bash wget --quiet --show-progress","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["293",{"pageContent":"https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd -O runpodctl && chmod +x runpodctl && sudo cp runpodctl /usr/bin/runpodctl \\`\\`\\` ### Windows (powershell) \\`\\`\\`bash wget https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-win-amd -O runpodctl.exe \\`\\`\\` ### Google Collab \\`\\`\\`bash !wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd -O runpodctl !chmod +x runpodctl !cp runpodctl","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["294",{"pageContent":"/usr/bin/runpodctl \\`\\`\\` ### Jupyter notebook \\`\\`\\`bash !wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd -O runpodctl !chmod +x runpodctl !cp runpodctl /usr/bin/runpodctl \\`\\`\\` ## runpodctl CLI for runpod.io ### Synopsis CLI tool to manage your pods for runpod.io ### Options \\`\\`\\` -h, --help help for runpodctl \\`\\`\\` ### SEE ALSO - \\[runpodctl config]\\(runpodctl\\_config.md) - CLI Config - \\[runpodctl","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["295",{"pageContent":"create]\\(runpodctl\\_create.md) - create a resource - \\[runpodctl get]\\(runpodctl\\_get.md) - get resource - \\[runpodctl project]\\(runpodctl\\_project.md) - Manage RunPod projects - \\[runpodctl receive]\\(runpodctl\\_receive.md) - receive file(s), or folder - \\[runpodctl remove]\\(runpodctl\\_remove.md) - remove a resource - \\[runpodctl send]\\(runpodctl\\_send.md) - send file(s), or folder - \\[runpodctl ssh]\\(runpodctl\\_ssh.md) - SSH keys and commands - \\[runpodctl start]\\(runpodctl\\_start.md) - start a","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["296",{"pageContent":"resource - \\[runpodctl stop]\\(runpodctl\\_stop.md) - stop a resource - \\[runpodctl update]\\(runpodctl\\_update.md) - update runpodctl - \\[runpodctl version]\\(runpodctl\\_version.md) - runpodctl version","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl.md","loc":{"lines":{"from":1,"to":1}}}}],["297",{"pageContent":"\\--- title: \"Config\" --- ## runpodctl config CLI Config ### Synopsis RunPod CLI Config Settings \\`\\`\\` runpodctl config \\[flags] \\`\\`\\` ### Options \\`\\`\\` --apiKey string RunPod API key --apiUrl string RunPod API URL (default \"https://api.runpod.io/graphql\") -h, --help help for config \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_config.md","loc":{"lines":{"from":1,"to":1}}}}],["298",{"pageContent":"\\--- title: \"Create\" --- ## runpodctl create create a resource ### Synopsis create a resource in runpod.io ### Options \\`\\`\\` -h, --help help for create \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io - \\[runpodctl create pod]\\(runpodctl\\_create\\_pod.md) - start a pod - \\[runpodctl create pods]\\(runpodctl\\_create\\_pods.md) - create a group of pods","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create.md","loc":{"lines":{"from":1,"to":1}}}}],["299",{"pageContent":"\\--- title: \"Create Pod\" --- ## runpodctl create pod start a pod ### Synopsis start a pod from runpod.io \\`\\`\\` runpodctl create pod \\[flags] \\`\\`\\` ### Options \\`\\`\\` --args string container arguments --communityCloud create in community cloud --containerDiskSize int container disk size in GB (default 20) --cost float32 $/hr price ceiling, if not defined, pod will be created with lowest price available --env strings container arguments --gpuCount int number of GPUs for the pod (default 1)","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pod.md","loc":{"lines":{"from":1,"to":1}}}}],["300",{"pageContent":"--gpuType string gpu type id, e.g. 'NVIDIA GeForce RTX 3090' -h, --help help for pod --imageName string container image name --mem int minimum system memory needed (default 20) --name string any pod name for easy reference --ports strings ports to expose; max only 1 http and 1 tcp allowed; e.g. '8888/http' --secureCloud create in secure cloud --templateId string templateId to use with the pod --vcpu int minimum vCPUs needed (default 1) --volumePath string container volume path (default","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pod.md","loc":{"lines":{"from":1,"to":1}}}}],["301",{"pageContent":"\"/runpod\") --volumeSize int persistent volume disk size in GB (default 1) --networkVolumeId string network volume id \\`\\`\\` ### SEE ALSO - \\[runpodctl create]\\(runpodctl\\_create.md) - create a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pod.md","loc":{"lines":{"from":1,"to":1}}}}],["302",{"pageContent":"\\--- title: \"Create Pods\" --- ## runpodctl create pods create a group of pods ### Synopsis create a group of pods on runpod.io \\`\\`\\` runpodctl create pods \\[flags] \\`\\`\\` ### Options \\`\\`\\` --args string container arguments --communityCloud create in community cloud --containerDiskSize int container disk size in GB (default 20) --cost float32 $/hr price ceiling, if not defined, pod will be created with lowest price available --env strings container arguments --gpuCount int number of GPUs for","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pods.md","loc":{"lines":{"from":1,"to":1}}}}],["303",{"pageContent":"the pod (default 1) --gpuType string gpu type id, e.g. 'NVIDIA GeForce RTX 3090' -h, --help help for pods --imageName string container image name --mem int minimum system memory needed (default 20) --name string any pod name for easy reference --podCount int number of pods to create with the same name (default 1) --ports strings ports to expose; max only 1 http and 1 tcp allowed; e.g. '8888/http' --secureCloud create in secure cloud --vcpu int minimum vCPUs needed (default 1) --volumePath string","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pods.md","loc":{"lines":{"from":1,"to":1}}}}],["304",{"pageContent":"container volume path (default \"/runpod\") --volumeSize int persistent volume disk size in GB (default 1) \\`\\`\\` ### SEE ALSO - \\[runpodctl create]\\(runpodctl\\_create.md) - create a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pods.md","loc":{"lines":{"from":1,"to":1}}}}],["305",{"pageContent":"\\--- title: \"Get\" --- ## runpodctl get get resource ### Synopsis get resources for pods ### Options \\`\\`\\` -h, --help help for get \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io - \\[runpodctl get cloud]\\(runpodctl\\_get\\_cloud.md) - get all cloud gpus - \\[runpodctl get pod]\\(runpodctl\\_get\\_pod.md) - get all pods","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_get.md","loc":{"lines":{"from":1,"to":1}}}}],["306",{"pageContent":"\\--- title: \"Get Cloud\" --- ## runpodctl get cloud get all cloud gpus ### Synopsis get all cloud gpus available on runpod.io \\`\\`\\` runpodctl get cloud \\[gpuCount] \\[flags] \\`\\`\\` ### Options \\`\\`\\` -c, --community show listings from community cloud only --disk int minimum disk size in GB you need -h, --help help for cloud --mem int minimum sys memory size in GB you need -s, --secure show listings from secure cloud only --vcpu int minimum vCPUs you need \\`\\`\\` ### SEE ALSO - \\[runpodctl","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_get_cloud.md","loc":{"lines":{"from":1,"to":1}}}}],["307",{"pageContent":"get]\\(runpodctl\\_get.md) - get resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_get_cloud.md","loc":{"lines":{"from":1,"to":1}}}}],["308",{"pageContent":"\\--- title: \"Get Pod\" --- ## runpodctl get pod get all pods ### Synopsis get all pods or specify pod id \\`\\`\\` runpodctl get pod \\[podId] \\[flags] \\`\\`\\` ### Options \\`\\`\\` -a, --allfields include all fields in output -h, --help help for pod \\`\\`\\` ### SEE ALSO - \\[runpodctl get]\\(runpodctl\\_get.md) - get resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_get_pod.md","loc":{"lines":{"from":1,"to":1}}}}],["309",{"pageContent":"\\--- title: \"Project\" --- ## runpodctl project Manage RunPod projects ### Synopsis Develop and deploy projects entirely on RunPod's infrastructure ### Options \\`\\`\\` -h, --help help for project \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io - \\[runpodctl project build]\\(runpodctl\\_project\\_build.md) - builds Dockerfile for current project - \\[runpodctl project create]\\(runpodctl\\_project\\_create.md) - creates a new project - \\[runpodctl project","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project.md","loc":{"lines":{"from":1,"to":1}}}}],["310",{"pageContent":"deploy]\\(runpodctl\\_project\\_deploy.md) - deploys your project as an endpoint - \\[runpodctl project dev]\\(runpodctl\\_project\\_dev.md) - starts a development session for the current project","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project.md","loc":{"lines":{"from":1,"to":1}}}}],["311",{"pageContent":"\\--- title: \"Project Build\" --- ## runpodctl project build builds Dockerfile for current project ### Synopsis builds a local Dockerfile for the project in the current folder. You can use this Dockerfile to build an image and deploy it to any API server. \\`\\`\\` runpodctl project build \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for build --include-env include environment variables from runpod.toml in generated Dockerfile \\`\\`\\` ### SEE ALSO - \\[runpodctl project]\\(runpodctl\\_project.md) -","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_build.md","loc":{"lines":{"from":1,"to":1}}}}],["312",{"pageContent":"Manage RunPod projects","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_build.md","loc":{"lines":{"from":1,"to":1}}}}],["313",{"pageContent":"\\--- title: \"Project Create\" --- ## runpodctl project create creates a new project ### Synopsis creates a new RunPod project folder on your local machine \\`\\`\\` runpodctl project create \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for create -i, --init use the current directory as the project directory -n, --name string project name \\`\\`\\` ### SEE ALSO - \\[runpodctl project]\\(runpodctl\\_project.md) - Manage RunPod projects","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_create.md","loc":{"lines":{"from":1,"to":1}}}}],["314",{"pageContent":"\\--- title: \"Project Deploy\" --- ## runpodctl project deploy deploys your project as an endpoint ### Synopsis deploys a serverless endpoint for the RunPod project in the current folder \\`\\`\\` runpodctl project deploy \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for deploy \\`\\`\\` ### SEE ALSO - \\[runpodctl project]\\(runpodctl\\_project.md) - Manage RunPod projects","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["315",{"pageContent":"\\--- title: \"Project Dev\" --- ## runpodctl project dev starts a development session for the current project ### Synopsis connects your local environment and the project environment on your Pod. Changes propagate to the project environment in real time. \\`\\`\\` runpodctl project dev \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for dev --prefix-pod-logs prefix logs from project Pod with Pod ID (default true) --select-volume select a new default network volume for current project \\`\\`\\` ###","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_dev.md","loc":{"lines":{"from":1,"to":1}}}}],["316",{"pageContent":"SEE ALSO - \\[runpodctl project]\\(runpodctl\\_project.md) - Manage RunPod projects","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_dev.md","loc":{"lines":{"from":1,"to":1}}}}],["317",{"pageContent":"\\--- title: \"Receive\" --- ## runpodctl receive receive file(s), or folder ### Synopsis receive file(s), or folder from pod or any computer \\`\\`\\` runpodctl receive \\[code] \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for receive \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_receive.md","loc":{"lines":{"from":1,"to":1}}}}],["318",{"pageContent":"\\--- title: \"Remove\" --- ## runpodctl remove remove a resource ### Synopsis remove a resource in runpod.io ### Options \\`\\`\\` -h, --help help for remove \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io - \\[runpodctl remove pod]\\(runpodctl\\_remove\\_pod.md) - remove a pod - \\[runpodctl remove pods]\\(runpodctl\\_remove\\_pods.md) - remove all pods using name","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_remove.md","loc":{"lines":{"from":1,"to":1}}}}],["319",{"pageContent":"\\--- title: \"Remove Pod\" --- ## runpodctl remove pod remove a pod ### Synopsis remove a pod from runpod.io \\`\\`\\` runpodctl remove pod \\[podId] \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for pod \\`\\`\\` ### SEE ALSO - \\[runpodctl remove]\\(runpodctl\\_remove.md) - remove a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_remove_pod.md","loc":{"lines":{"from":1,"to":1}}}}],["320",{"pageContent":"\\--- title: \"Remove Pods\" --- ## runpodctl remove pods remove all pods using name ### Synopsis remove all pods using name from runpod.io \\`\\`\\` runpodctl remove pods \\[name] \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for pods --podCount int number of pods to remove with the same name (default 1) \\`\\`\\` ### SEE ALSO - \\[runpodctl remove]\\(runpodctl\\_remove.md) - remove a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_remove_pods.md","loc":{"lines":{"from":1,"to":1}}}}],["321",{"pageContent":"\\--- title: \"Send\" --- ## runpodctl send send file(s), or folder ### Synopsis send file(s), or folder to pod or any computer \\`\\`\\` runpodctl send \\[filename(s) or folder] \\[flags] \\`\\`\\` ### Options \\`\\`\\` --code string codephrase used to connect -h, --help help for send \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_send.md","loc":{"lines":{"from":1,"to":1}}}}],["322",{"pageContent":"\\--- title: \"Ssh\" --- ## runpodctl ssh SSH keys and commands ### Synopsis SSH key management and connection to pods ### Options \\`\\`\\` -h, --help help for ssh \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io - \\[runpodctl ssh add-key]\\(runpodctl\\_ssh\\_add-key.md) - Adds an SSH key to the current user account - \\[runpodctl ssh list-keys]\\(runpodctl\\_ssh\\_list-keys.md) - List all SSH keys","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_ssh.md","loc":{"lines":{"from":1,"to":1}}}}],["323",{"pageContent":"\\--- title: \"Ssh Add Key\" --- ## runpodctl ssh add-key Adds an SSH key to the current user account ### Synopsis Adds an SSH key to the current user account. If no key is provided, one will be generated. \\`\\`\\` runpodctl ssh add-key \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for add-key --key string The public key to add. --key-file string The file containing the public key to add. \\`\\`\\` ### SEE ALSO - \\[runpodctl ssh]\\(runpodctl\\_ssh.md) - SSH keys and commands","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_ssh_add-key.md","loc":{"lines":{"from":1,"to":1}}}}],["324",{"pageContent":"\\--- title: \"Ssh List Keys\" --- ## runpodctl ssh list-keys List all SSH keys ### Synopsis List all the SSH keys associated with the current user's account. \\`\\`\\` runpodctl ssh list-keys \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for list-keys \\`\\`\\` ### SEE ALSO - \\[runpodctl ssh]\\(runpodctl\\_ssh.md) - SSH keys and commands","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_ssh_list-keys.md","loc":{"lines":{"from":1,"to":1}}}}],["325",{"pageContent":"\\--- title: \"Start\" --- ## runpodctl start start a resource ### Synopsis start a resource in runpod.io ### Options \\`\\`\\` -h, --help help for start \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io - \\[runpodctl start pod]\\(runpodctl\\_start\\_pod.md) - start a pod","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_start.md","loc":{"lines":{"from":1,"to":1}}}}],["326",{"pageContent":"\\--- title: \"Start Pod\" --- ## runpodctl start pod start a pod ### Synopsis start a pod from runpod.io \\`\\`\\` runpodctl start pod \\[podId] \\[flags] \\`\\`\\` ### Options \\`\\`\\` --bid float32 bid per gpu for spot price -h, --help help for pod \\`\\`\\` ### SEE ALSO - \\[runpodctl start]\\(runpodctl\\_start.md) - start a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_start_pod.md","loc":{"lines":{"from":1,"to":1}}}}],["327",{"pageContent":"\\--- title: \"Stop\" --- ## runpodctl stop stop a resource ### Synopsis stop a resource in runpod.io ### Options \\`\\`\\` -h, --help help for stop \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io - \\[runpodctl stop pod]\\(runpodctl\\_stop\\_pod.md) - stop a pod","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_stop.md","loc":{"lines":{"from":1,"to":1}}}}],["328",{"pageContent":"\\--- title: \"Stop Pod\" --- ## runpodctl stop pod stop a pod ### Synopsis stop a pod from runpod.io \\`\\`\\` runpodctl stop pod \\[podId] \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for pod \\`\\`\\` ### SEE ALSO - \\[runpodctl stop]\\(runpodctl\\_stop.md) - stop a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_stop_pod.md","loc":{"lines":{"from":1,"to":1}}}}],["329",{"pageContent":"\\--- title: \"Update\" --- ## runpodctl update update runpodctl ### Synopsis update runpodctl to the latest version \\`\\`\\` runpodctl update \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for update \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_update.md","loc":{"lines":{"from":1,"to":1}}}}],["330",{"pageContent":"\\--- title: \"Version\" --- ## runpodctl version runpodctl version ### Synopsis runpodctl version \\`\\`\\` runpodctl version \\[flags] \\`\\`\\` ### Options \\`\\`\\` -h, --help help for version \\`\\`\\` ### SEE ALSO - \\[runpodctl]\\(runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_version.md","loc":{"lines":{"from":1,"to":1}}}}],["331",{"pageContent":"\\--- title: \"502 Errors\" id: \"troubleshooting-502-errors\" description: \"Troubleshoot 502 errors in your deployed pod by checking GPU attachment, pod logs, and official template instructions to resolve issues and enable seamless access.\" --- 502 errors can occur when users attempt to access a program running on a specific port of a deployed pod and the program isn't running or has encountered an error. This document provides guidance to help you troubleshoot this error. ### Check Your Pod's GPU","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":1,"to":1}}}}],["332",{"pageContent":"The first step to troubleshooting a 502 error is to check whether your pod has a GPU attached. 1. \\*\\*Access your pod's settings\\*\\*: Click on your pod's settings in the user interface to access detailed information about your pod. 2. \\*\\*Verify GPU attachment\\*\\*: Here, you should be able to see if your pod has a GPU attached. If it does not, you will need to attach a GPU. If a GPU is attached, you will see it under the Pods screen (e.g. 1 x A6000). If a GPU is not attached, this number will be","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":1,"to":1}}}}],["333",{"pageContent":"0. RunPod does allow you to spin up a pod with 0 GPUs so that you can connect to it via a Terminal or CloudSync to access data. However, the options to connect to RunPod via the web interface will be nonfunctional, even if they are lit up. !\\[]\\(/img/docs/fb4c0dd-image.png) ### Check Your Pod's Logs After confirming that your pod has a GPU attached, the next step is to check your pod's logs for any errors. 1. \\*\\*Access your pod's logs\\*\\*: You can view the logs from the pod's settings in the","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":1,"to":1}}}}],["334",{"pageContent":"user interface. 2. !\\[]\\(/img/docs/3500eba-image.png)\\ \\*\\*Look for errors\\*\\*: Browse through the logs to find any error messages that may provide clues about why you're experiencing a 502 error. ### Verify Additional Steps for Official Templates In some cases, for our official templates, the user interface does not work right away and may require additional steps to be performed by the user. 1. \\*\\*Access the template's ReadMe\\*\\*: Navigate to the template's page and open the ReadMe file. 2.","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":1,"to":1}}}}],["335",{"pageContent":"\\*\\*Follow additional steps\\*\\*: The ReadMe file should provide instructions on any additional steps you need to perform to get the UI functioning properly. Make sure to follow these instructions closely. Remember, each template may have unique requirements or steps for setup. It is always recommended to thoroughly review the documentation associated with each template. If you continue to experience 502 errors after following these steps, please contact our support team. We're here to help","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":1,"to":1}}}}],["336",{"pageContent":"ensure that your experience on our platform is as seamless as possible.","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":1,"to":1}}}}],["337",{"pageContent":"\\--- title: \"Video resources\" description: \"Explore comprehensive video tutorials on RunPod, covering topics like image generation, text-to-image, and Linux commands, including StableDiffusion, DeepFloyd, ControlNet, Automatic111, and more, with step-by-step instructions and expert guidance.\" --- ## RunPod Usage | Video Link | Topics Covered | | : | : | | \\[How to redeem your RunPod Coupon]\\(https://www.youtube.com/watch?v=IYqEKwpuyWk\\&ab\\_channel=OpenCVCourses) | RunPod | | \\[RunPod","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":1,"to":1}}}}],["338",{"pageContent":"Introduction and Tour]\\(https://www.youtube.com/watch?v=6O1oM\\_N6pcw\\&ab\\_channel=OpenCVCourses) | General | --- ## Tutorials | Video Link | Topics Covered | | : | : | | \\[Generate Stable Diffusion Images FAST with RunPod]\\(https://www.youtube.com/watch?v=susnjHSWFq0\\&t=32s\\&ab\\_channel=BillMeeks) | StableDiffusion, Automatic111 | | \\[Generate Text On Images with DeepFloyd IF]\\(https://www.youtube.com/watch?v=Px7Vv9WYl88\\&t=2s\\&ab\\_channel=BillMeeks) | DeepFloyd | | \\[Remix Your Pics With Stable","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":1,"to":1}}}}],["339",{"pageContent":"Diffusion and ControlNet]\\(https://www.youtube.com/watch?v=BqdIdk9LU4w\\&t=1s\\&ab\\_channel=BillMeeks) | StableDiffusion, ControlNet | | \\[Using Automatic1111 WebUI on RunPod]\\(https://www.youtube.com/watch?v=R6HUQOtsVic\\&ab\\_channel=OpenCVCourses) | WebUI, Automatic1111 | | \\[RUN TextGen AI WebUI LLM On Runpod & Colab!]\\(https://www.youtube.com/watch?v=TP2yID7Ubr4\\&ab\\_channel=Aitrepreneur) | GoogleColab, TextGeneration | | \\[Make Your Renders 10x Faster With","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":1,"to":1}}}}],["340",{"pageContent":"Runpod]\\(https://www.youtube.com/watch?v=sJ-Diy93TAg\\&ab\\_channel=RahulAhire) | Rendering, Blender | --- ## General/Generic Linux | Video Link | Topics Covered | | : | : | | \\[How to Use the rsync Command \\\\| Linux Essentials Tutorial]\\(https://www.youtube.com/watch?v=2PnAohLS-Q4\\&ab\\_channel=AkamaiDeveloper) | File Transfer | | \\[SSH Key Authentication \\\\| How to Create SSH Key Pairs]\\(https://www.youtube.com/watch?v=33dEcCKGBO4\\&ab\\_channel=AkamaiDeveloper) | SSH Authentication |","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":1,"to":1}}}}],["341",{"pageContent":"\\--- title: API Endpoints id: api-endpoints hide\\_table\\_of\\_contents: true sidebar\\_position: 1 description: \"Integrate API documentation into your project with the @theme/ApiDocMdx library, enabling seamless documentation management for improved user experience.\" --- import ApiDocMdx from '@theme/ApiDocMdx';","metadata":{"source":"/runpod-docs/docs/references/_api-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["342",{"pageContent":"\\--- title: Endpoints --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; Interacting with RunPod's Endpoints is a core feature of the SDK, enabling the execution of tasks and the retrieval of results. This section covers the synchronous and asynchronous execution methods, along with checking the status of operations. ## Prerequisites Before using the RunPod Go SDK, ensure that you have: - \\[Installed the RunPod Go SDK]\\(/sdks/go/overview#install). - Configured your API","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["343",{"pageContent":"key. ## Set your Endpoint Id Set your RunPod API key and your Endpoint Id as environment variables. \\`\\`\\`go package main import ( \"log\" \"os\" \"github.com/runpod/go-sdk/pkg/sdk\" \"github.com.runpod/go-sdk/pkg/sdk/config\" rpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { // Retrieve the API key and base URL from environment variables apiKey := os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") // Check if environment variables are set if apiKey == \"\" {","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["344",{"pageContent":"log.Fatalf(\"Environment variable RUNPOD\\_API\\_KEY is not set\") } if baseURL == \"\" { log.Fatalf(\"Environment variable RUNPOD\\_BASE\\_URL is not set\") } // Use the endpoint object // ... } \\`\\`\\` This allows all calls to pass through your Endpoint Id with a valid API key. The following are actions you use on the - \\[RunSync]\\(#run-sync) - \\[Run]\\(#run-async) - \\[Stream]\\(#stream) - \\[Health]\\(#health-check) - \\[Status]\\(#status) - \\[Cancel]\\(#cancel) - \\[Purge Queue]\\(#purge-queue) Here is the","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["345",{"pageContent":"revised documentation based on the Go Sample: ## Run the Endpoint {#run} Run the Endpoint using either the asynchronous \\`run\\` or synchronous \\`runSync\\` method. Choosing between asynchronous and synchronous execution hinges on your task's needs and application design. ### Run synchronously {#run-sync} To execute an endpoint synchronously and wait for the result, use the \\`runSync\\` method on your endpoint. This method blocks the execution until the endpoint run is complete or until it times","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["346",{"pageContent":"out. \\`\\`\\`go package main import ( \"encoding/json\" \"fmt\" \"log\" \"os\" \"github.com/runpod/go-sdk/pkg/sdk\" \"github.com.runpod/go-sdk/pkg/sdk/config\" rpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey := os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err := rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { log.Fatalf(\"Failed to create endpoint: %v\", err) } jobInput :=","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["347",{"pageContent":"rpEndpoint.RunSyncInput{ JobInput: \\&rpEndpoint.JobInput{ Input: map\\[string]interface{}{ \"prompt\": \"Hello World\", }, }, Timeout: sdk.Int(120), } output, err := endpoint.RunSync(\\&jobInput) if err != nil { panic(err) } data, \\_ := json.Marshal(output) fmt.Printf(\"output: %s\\n\", data) } \\`\\`\\` \\`\\`\\`json { \"delayTime\": 18, \"executionTime\": 36595, \"id\": \"sync-d050a3f6-791a-4aff-857a-66c759db4a06-u1\", \"output\": \\[ { \"choices\": \\[], \"usage\": {} } ], \"status\": \"COMPLETED\", \"started\": true,","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["348",{"pageContent":"\"completed\": true, \"succeeded\": true } \\`\\`\\` ## Run asynchronously {#run-async} Asynchronous execution allows for non-blocking operations, enabling your code to perform other tasks while waiting for an operation to complete. For non-blocking operations, use the \\`run\\` method on the endpoint. This method allows you to start an endpoint run and then check its status or wait for its completion at a later time. \\`\\`\\`go package main import ( \"encoding/json\" \"fmt\" \"log\" \"os\"","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["349",{"pageContent":"\"github.com/runpod/go-sdk/pkg/sdk\" \"github.com/runpod/go-sdk/pkg/sdk/config\" rpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey := os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err := rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { log.Fatalf(\"Failed to create endpoint: %v\", err) } jobInput := rpEndpoint.RunInput{ JobInput: \\&rpEndpoint.JobInput{ Input:","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["350",{"pageContent":"map\\[string]interface{}{ \"mock\\_delay\": 95, }, }, RequestTimeout: sdk.Int(120), } output, err := endpoint.Run(\\&jobInput) if err != nil { panic(err) } data, \\_ := json.Marshal(output) fmt.Printf(\"output: %s\\n\", data) } \\`\\`\\` \\`\\`\\`json { \"id\": \"d4e960f6-073f-4219-af24-cbae6b532c31-u1\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` ### Get results from an asynchronous run The following example shows how to get the results of an asynchronous run. \\`\\`\\`go package main import ( \"encoding/json\" \"fmt\" \"log\" \"os\"","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["351",{"pageContent":"\"time\" \"github.com/runpod/go-sdk/pkg/sdk\" \"github.com/runpod/go-sdk/pkg/sdk/config\" rpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey := os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err := rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { log.Fatalf(\"Failed to create endpoint: %v\", err) } // Initiate the asynchronous run jobInput := rpEndpoint.RunInput{ JobInput:","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["352",{"pageContent":"\\&rpEndpoint.JobInput{ Input: map\\[string]interface{}{\"mock\\_delay\": 95}, }, RequestTimeout: sdk.Int(120), } runOutput, err := endpoint.Run(\\&jobInput) if err != nil { log.Fatalf(\"Failed to initiate the run: %v\", err) } // Extract the ID from the run output runID := \\*runOutput.Id fmt.Printf(\"Run ID: %s\\n\", runID) // Prepare the input for status polling statusInput := rpEndpoint.StatusInput{ Id: sdk.String(runID), } // Poll the status until it completes or fails var statusOutput","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["353",{"pageContent":"\\*rpEndpoint.StatusOutput for { statusOutput, err = endpoint.Status(\\&statusInput) if err != nil { log.Printf(\"Error checking status: %v\", err) time.Sleep(5 \\* time.Second) continue } statusJSON, \\_ := json.Marshal(statusOutput) fmt.Printf(\"Status: %s\\n\", statusJSON) if \\*statusOutput.Status == \"COMPLETED\" || \\*statusOutput.Status == \"FAILED\" { break } time.Sleep(5 \\* time.Second) } // Retrieve the final result (assuming it's available in the status output) if \\*statusOutput.Status ==","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["354",{"pageContent":"\"COMPLETED\" { fmt.Println(\"Run completed successfully!\") // Handle the completed run's output if needed } else { fmt.Println(\"Run failed!\") // Handle the failed run if needed } } \\`\\`\\` \\`\\`\\`json Run ID: 353b1e99-2f35-43a8-8a8b-001d59df8aa1-u1 Status: {\"id\":\"353b1e99-2f35-43a8-8a8b-001d59df8aa1-u1\",\"status\":\"IN\\_QUEUE\"} Status: {\"delayTime\":536,\"executionTime\":239,\"id\":\"353b1e99-2f35-43a8-8a8b-001d59df8aa1-u1\",\"output\":\"69.30.85.167\",\"status\":\"COMPLETED\"} Run completed successfully! \\`\\`\\` ##","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["355",{"pageContent":"Stream {#stream} Stream allows you to stream the output of an Endpoint run. To enable streaming, your handler must support the \\`\"return\\_aggregate\\_stream\": True\\` option on the \\`start\\` method of your Handler. Once enabled, use the \\`stream\\` method to receive data as it becomes available. \\`\\`\\`go package main import ( \"encoding/json\" \"fmt\" \"github.com/runpod/go-sdk/pkg/sdk/config\" rpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey := os.Getenv(\"RUNPOD\\_API\\_KEY\")","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["356",{"pageContent":"baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err := rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { panic(err) } request, err := endpoint.Run(\\&rpEndpoint.RunInput{ JobInput: \\&rpEndpoint.JobInput{ Input: map\\[string]interface{}{ \"prompt\": \"Hello World\", }, }, }) if err != nil { panic(err) } streamChan := make(chan rpEndpoint.StreamResult, 100) err = endpoint.Stream(\\&rpEndpoint.StreamInput{Id: request.Id}, streamChan) if","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["357",{"pageContent":"err != nil { // timeout reached, if we want to get the data that has been streamed if err.Error() == \"ctx timeout reached\" { for data := range streamChan { dt, \\_ := json.Marshal(data) fmt.Printf(\"output:%s\\n\", dt) } } panic(err) } for data := range streamChan { dt, \\_ := json.Marshal(data) fmt.Printf(\"output:%s\\n\", dt) } } \\`\\`\\` \\`\\`\\`json { id: 'cb68890e-436f-4234-955d-001db6afe972-u1', status: 'IN\\_QUEUE' } { \"output\": \"H\" } { \"output\": \"e\" } { \"output\": \"l\" } { \"output\": \"l\" } { \"output\":","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["358",{"pageContent":"\"o\" } { \"output\": \",\" } { \"output\": \" \" } { \"output\": \"W\" } { \"output\": \"o\" } { \"output\": \"r\" } { \"output\": \"l\" } { \"output\": \"d\" } { \"output\": \"!\" } done streaming \\`\\`\\` You must define your handler to support the \\`\"return\\_aggregate\\_stream\": True\\` option on the \\`start\\` method. \\`\\`\\`python from time import sleep import runpod def handler(job): job\\_input = job\\[\"input\"]\\[\"prompt\"] for i in job\\_input: sleep(1) # sleep for 1 second for effect yield i runpod.serverless.start( { \"handler\":","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["359",{"pageContent":"handler, \"return\\_aggregate\\_stream\": True, # Ensures aggregated results are streamed back } ) \\`\\`\\` ## Health check Monitor the health of an endpoint by checking its status, including jobs completed, failed, in progress, in queue, and retried, as well as the status of workers. \\`\\`\\`go package main import ( \"encoding/json\" \"fmt\" \"github.com/runpod/go-sdk/pkg/sdk\" \"github.com/runpod/go-sdk/pkg/sdk/config\" rpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey :=","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["360",{"pageContent":"os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err := rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { panic(err) } healthInput := rpEndpoint.StatusInput{ Id: sdk.String(\"20aad8ef-9c86-4fcd-a349-579ce38e8bfd-u1\"), } output, err := endpoint.Status(\\&healthInput) if err != nil { panic(err) } healthData, \\_ := json.Marshal(output) fmt.Printf(\"health output: %s\\n\", healthData) } \\`\\`\\` \\`\\`\\`json {","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["361",{"pageContent":"\"jobs\": { \"completed\": 72, \"failed\": 1, \"inProgress\": 6, \"inQueue\": 0, \"retried\": 1 }, \"workers\": { \"idle\": 4, \"initializing\": 0, \"ready\": 4, \"running\": 1, \"throttled\": 0 } } \\`\\`\\` ## Status Use the \\`status\\` method and specify the \\`id\\` of the run to get the status of a run. \\`\\`\\`go package main import ( \"encoding/json\" \"fmt\" \"log\" \"os\" \"github.com/runpod/go-sdk/pkg/sdk\" \"github.com/runpod/go-sdk/pkg/sdk/config\" rpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["362",{"pageContent":":= os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err := rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { log.Fatalf(\"Failed to create endpoint: %v\", err) } input := rpEndpoint.StatusInput{ Id: sdk.String(\"5efff030-686c-4179-85bb-31b9bf97b944-u1\"), } output, err := endpoint.Status(\\&input) if err != nil { panic(err) } dt, \\_ := json.Marshal(output) fmt.Printf(\"output:%s\\n\", dt) } \\`\\`\\` \\`\\`\\`json","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["363",{"pageContent":"{ \"delayTime\": 18, \"id\": \"792b1497-b2c8-4c95-90bf-4e2a6a2a37ff-u1\", \"status\": \"IN\\_PROGRESS\", \"started\": true, \"completed\": false, \"succeeded\": false } \\`\\`\\` ## Cancel You can cancel a Job request by using the \\`cancel()\\` function on the run request. You might want to cancel a Job because it's stuck with a status of \\`IN\\_QUEUE\\` or \\`IN\\_PROGRESS\\`, or because you no longer need the result. \\`\\`\\`go package main import ( \"encoding/json\" \"fmt\" \"github.com/runpod/go-sdk/pkg/sdk\"","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["364",{"pageContent":"\"github.com/runpod/go-sdk/pkg/sdk/config\" rpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey := os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err := rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { panic(err) } cancelInput := rpEndpoint.CancelInput{ Id: sdk.String(\"00edfd03-8094-46da-82e3-ea47dd9566dc-u1\"), } output, err := endpoint.Cancel(\\&cancelInput) if err != nil {","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["365",{"pageContent":"panic(err) } healthData, \\_ := json.Marshal(output) fmt.Printf(\"health output: %s\\n\", healthData) } \\`\\`\\` \\`\\`\\`json { \"id\": \"5fb6a8db-a8fa-41a1-ad81-f5fad9755f9e-u1\", \"status\": \"CANCELLED\" } \\`\\`\\` ### Timeout You can set the maximum time to wait for a response from the endpoint using the \\`RequestTimeout\\` field in the \\`RunInput\\` struct. \\`\\`\\`go package main import ( \"encoding/json\" \"fmt\" \"log\" \"os\" \"github.com/runpod/go-sdk/pkg/sdk\" \"github.com/runpod/go-sdk/pkg/sdk/config\" rpEndpoint","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["366",{"pageContent":"\"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey := os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err := rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { log.Fatalf(\"Failed to create endpoint: %v\", err) } jobInput := rpEndpoint.RunInput{ JobInput: \\&rpEndpoint.JobInput{ Input: map\\[string]interface{} RequestTimeout: sdk.Int(120), } output, err := endpoint.Run(\\&jobInput) if err","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["367",{"pageContent":"!= nil { panic(err) } data, \\_ := json.Marshal(output) fmt.Printf(\"output: %s\\n\", data) } \\`\\`\\` \\`\\`\\`json { \"id\": \"43309f93-0422-4eac-92cf-e385dee36e99-u1\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` ### Execution policy You can specify the TTL (Time-to-Live) and ExecutionTimeout values for the job using the \\`Input\\` map of the \\`JobInput\\` struct. \\`\\`\\`go package main import ( \"encoding/json\" \"fmt\" \"log\" \"os\" \"github.com/runpod/go-sdk/pkg/sdk\" \"github.com/runpod/go-sdk/pkg/sdk/config\" rpEndpoint","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["368",{"pageContent":"\"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey := os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err := rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { log.Fatalf(\"Failed to create endpoint: %v\", err) } jobInput := rpEndpoint.RunInput{ JobInput: \\&rpEndpoint.JobInput{ Input: map\\[string]interface{}{ \"ttl\": 3600, // Set the TTL value, e.g., 3600 seconds (1 hour)","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["369",{"pageContent":"\"execution\\_timeout\": 300, // Set the ExecutionTimeout value, e.g., 300 seconds (5 minutes) }, }, RequestTimeout: sdk.Int(120), } output, err := endpoint.Run(\\&jobInput) if err != nil { panic(err) } data, \\_ := json.Marshal(output) fmt.Printf(\"output: %s\\n\", data) } \\`\\`\\` \\`\\`\\`json { \"id\": \"21bd3763-dcbf-4091-84ee-85b80907a020-u1\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` For more information, see \\[Execution policy]\\(/serverless/endpoints/send-requests#execution-policies). ## Purge Queue Create an","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["370",{"pageContent":"instance of the \\`PurgeQueueInput\\` struct and set the desired values. Call the \\`PurgeQueue\\` method of the Endpoint with the \\`PurgeQueueInput\\` instance. \\`PurgeQueue()\\` doesn't affect Jobs in progress. \\`\\`\\`go package main import ( \"fmt\" \"log\" \"os\" \"github.com/runpod/go-sdk/pkg/sdk\" \"github.com/runpod/go-sdk/pkg/sdk/config\" rpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\" ) func main() { apiKey := os.Getenv(\"RUNPOD\\_API\\_KEY\") baseURL := os.Getenv(\"RUNPOD\\_BASE\\_URL\") endpoint, err","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["371",{"pageContent":":= rpEndpoint.New( \\&config.Config{ApiKey: \\&apiKey}, \\&rpEndpoint.Option{EndpointId: \\&baseURL}, ) if err != nil { log.Fatalf(\"Failed to create endpoint: %v\", err) } purgeQueueInput := rpEndpoint.PurgeQueueInput{ RequestTimeout: sdk.Int(5), // Set the request timeout to 5 seconds } purgeQueueOutput, err := endpoint.PurgeQueue(\\&purgeQueueInput) if err != nil { panic(err) } fmt.Printf(\"Status: %s\\n\", \\*purgeQueueOutput.Status) fmt.Printf(\"Removed: %d\\n\", \\*purgeQueueOutput.Removed) } \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["372",{"pageContent":"\\`\\`\\`json Status: completed Removed: 1 \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["373",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 tags: - go --- Get started with setting up your RunPod projects using Go. Whether you're building web applications, server-side implementations, or automating tasks, the RunPod Go SDK provides the tools you need. This guide outlines the steps to get your development environment ready and integrate RunPod into your Go projects. ## Prerequisites Before you begin, ensure that you have the following: - Go installed on your machine (version 1.16 or later) - A","metadata":{"source":"/runpod-docs/docs/sdks/go/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["374",{"pageContent":"RunPod account with an API key and Endpoint Id ## Install the RunPod SDK {#install} Before integrating RunPod into your project, you'll need to install the SDK. To install the RunPod SDK, run the following \\`go get\\` command in your project directory. \\`\\`\\`command go get github.com/runpod/go-sdk \\`\\`\\` This command installs the \\`runpod-sdk\\` package. Then run the following command to install the dependcies: \\`\\`\\`command go mod tidy \\`\\`\\` For more details about the package, visit the \\[Go","metadata":{"source":"/runpod-docs/docs/sdks/go/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["375",{"pageContent":"package page]\\(https://pkg.go.dev/github.com/runpod/go-sdk/pkg/sdk) or the \\[GitHub repository]\\(https://github.com/runpod/go-sdk). ## Add your API key To use the RunPod SDK in your project, you first need to import it and configure it with your API key and endpoint ID. Ensure these values are securely stored, preferably as environment variables. Below is a basic example of how to initialize and use the RunPod SDK in your Go project. \\`\\`\\`go func main() { endpoint, err := rpEndpoint.New(","metadata":{"source":"/runpod-docs/docs/sdks/go/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["376",{"pageContent":"\\&config.Config{ApiKey: sdk.String(os.Getenv(\"RUNPOD\\_API\\_KEY\"))}, \\&rpEndpoint.Option{EndpointId: sdk.String(os.Getenv(\"RUNPOD\\_BASE\\_URL\"))}, ) if err != nil { panic(err) } // Use the endpoint object // ... } \\`\\`\\` This snippet demonstrates how to import the SDK, initialize it with your API key, and reference a specific endpoint using its ID. ### Secure your API key When working with the RunPod SDK, it's essential to secure your API key. Storing the API key in environment variables is","metadata":{"source":"/runpod-docs/docs/sdks/go/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["377",{"pageContent":"recommended, as shown in the initialization example. This method keeps your key out of your source code and reduces the risk of accidental exposure. :::note Use environment variables or secure secrets management solutions to handle sensitive information like API keys. ::: For more information, see the following: - \\[RunPod SDK Go Package]\\(https://pkg.go.dev/github.com/runpod/go-sdk/pkg/sdk) - \\[RunPod GitHub Repository]\\(https://github.com/runpod/go-sdk) - \\[Endpoints]\\(/sdks/go/endpoints)","metadata":{"source":"/runpod-docs/docs/sdks/go/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["378",{"pageContent":"\\--- title: \"Configurations\" sidebar\\_position: 1 description: \"Configure your environment with essential arguments: containerDiskInGb, dockerArgs, env, imageName, name, and volumeInGb, to ensure correct setup and operation of your container.\" --- For details on queries, mutations, fields, and inputs, see the \\[RunPod GraphQL Spec]\\(https://graphql-spec.runpod.io/). When configuring your environment, certain arguments are essential to ensure the correct setup and operation. Below is a detailed","metadata":{"source":"/runpod-docs/docs/sdks/graphql/configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["379",{"pageContent":"overview of each required argument: ### \\`containerDiskInGb\\` - \\*\\*Description\\*\\*: Specifies the size of the disk allocated for the container in gigabytes. This space is used for the operating system, installed applications, and any data generated or used by the container. - \\*\\*Type\\*\\*: Integer - \\*\\*Example\\*\\*: \\`10\\` for a 10 GB disk size. ### \\`dockerArgs\\` - \\*\\*Description\\*\\*: If specified, overrides the \\[container start","metadata":{"source":"/runpod-docs/docs/sdks/graphql/configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["380",{"pageContent":"command]\\(https://docs.docker.com/engine/reference/builder/#cmd). If this argument is not provided, it will rely on the start command provided in the docker image. - \\*\\*Type\\*\\*: String - \\*\\*Example\\*\\*: \\`sleep infinity\\` to run the container in the background. <!-- Contains additional arguments that are passed directly to Docker when starting the container. This can include mount points, network settings, or any other Docker command-line arguments. --> \\### \\`env\\` - \\*\\*Description\\*\\*: A","metadata":{"source":"/runpod-docs/docs/sdks/graphql/configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["381",{"pageContent":"set of environment variables to be set within the container. These can configure application settings, external service credentials, or any other configuration data required by the software running in the container. - \\*\\*Type\\*\\*: Dictionary or Object - \\*\\*Example\\*\\*: \\`{\"DATABASE\\_URL\": \"postgres://user:password@localhost/dbname\"}\\`. ### \\`imageName\\` - \\*\\*Description\\*\\*: The name of the Docker image to use for the container. This should include the repository name and tag, if applicable.","metadata":{"source":"/runpod-docs/docs/sdks/graphql/configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["382",{"pageContent":"- \\*\\*Type\\*\\*: String - \\*\\*Example\\*\\*: \\`\"nginx:latest\"\\` for the latest version of the Nginx image. ### \\`name\\` - \\*\\*Description\\*\\*: The name assigned to the container instance. This name is used for identification and must be unique within the context it's being used. - \\*\\*Type\\*\\*: String - \\*\\*Example\\*\\*: \\`\"my-app-container\"\\`. ### \\`volumeInGb\\` - \\*\\*Description\\*\\*: Defines the size of an additional persistent volume in gigabytes. This volume is used for storing data that needs","metadata":{"source":"/runpod-docs/docs/sdks/graphql/configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["383",{"pageContent":"to persist between container restarts or redeployments. - \\*\\*Type\\*\\*: Integer - \\*\\*Example\\*\\*: \\`5\\` for a 5GB persistent volume. Ensure that these arguments are correctly specified in your configuration to avoid errors during deployment. Optional arguments may also be available, providing additional customization and flexibility for your setup.","metadata":{"source":"/runpod-docs/docs/sdks/graphql/configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["384",{"pageContent":"\\--- title: \"Manage Endpoints\" description: \"Create, modify, or delete serverless endpoints using GraphQL queries and mutations with RunPod API, specifying GPU IDs, template IDs, and other endpoint settings.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; \\`gpuIds\\`, \\`name\\`, and \\`templateId\\` are required arguments; all other arguments are optional, and default values will be used if unspecified. ## Create a new Serverless Endpoint \\`\\`\\`curl curl --request POST \\","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["385",{"pageContent":"--header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { saveEndpoint(input: { gpuIds: \\\\\"AMPERE\\_16\\\\\", idleTimeout: 5, locations: \\\\\"US\\\\\", name: \\\\\"Generated Endpoint -fb\\\\\", networkVolumeId: \\\\\"\\\\\", scalerType: \\\\\"QUEUE\\_DELAY\\\\\", scalerValue: 4, templateId: \\\\\"xkhgg72fuo\\\\\", workersMax: 3, workersMin: 0 }) { gpuIds id idleTimeout locations name scalerType scalerValue templateId workersMax workersMin } }\"}'","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["386",{"pageContent":"\\`\\`\\` \\`\\`\\`graphql mutation { saveEndpoint(input: { # options for gpuIds are \"AMPERE\\_16,AMPERE\\_24,AMPERE\\_48,AMPERE\\_80,ADA\\_24\" gpuIds: \"AMPERE\\_16\", idleTimeout: 5, # leave locations as an empty string or null for any region # options for locations are \"CZ,FR,GB,NO,RO,US\" locations: \"US\", # append -fb to your endpoint's name to enable FlashBoot name: \"Generated Endpoint -fb\", # uncomment below and provide an ID to mount a network volume to your workers # networkVolumeId: \"\", scalerType:","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["387",{"pageContent":"\"QUEUE\\_DELAY\", scalerValue: 4, templateId: \"xkhgg72fuo\", workersMax: 3, workersMin: 0 }) { gpuIds id idleTimeout locations name # networkVolumeId scalerType scalerValue templateId workersMax workersMin } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"saveEndpoint\": { \"gpuIds\": \"AMPERE\\_16\", \"id\": \"i02xupws21hp6i\", \"idleTimeout\": 5, \"locations\": \"US\", \"name\": \"Generated Endpoint -fb\", \"scalerType\": \"QUEUE\\_DELAY\", \"scalerValue\": 4, \"templateId\": \"xkhgg72fuo\", \"workersMax\": 3, \"workersMin\": 0 } } } \\`\\`\\` ##","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["388",{"pageContent":"Modify an existing Serverless Endpoint \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { saveEndpoint(input: { id: \\\\\"i02xupws21hp6i\\\\\", gpuIds: \\\\\"AMPERE\\_16\\\\\", name: \\\\\"Generated Endpoint -fb\\\\\", templateId: \\\\\"xkhgg72fuo\\\\\", workersMax: 0 }) { id gpuIds name templateId workersMax } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { saveEndpoint(input: { id: \"i02xupws21hp6i\", gpuIds:","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["389",{"pageContent":"\"AMPERE\\_16\", name: \"Generated Endpoint -fb\", templateId: \"xkhgg72fuo\", # Modify your template options here (or above, if applicable). # For this example, we've modified the endpoint's max workers. workersMax: 0 }) { id gpuIds name templateId # You can include what you've changed here, too. workersMax } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"saveEndpoint\": { \"id\": \"i02xupws21hp6i\", \"gpuIds\": \"AMPERE\\_16\", \"name\": \"Generated Endpoint -fb\", \"templateId\": \"xkhgg72fuo\", \"workersMax\": 0 } } } \\`\\`\\` # View","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["390",{"pageContent":"your Endpoints \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"query Endpoints { myself { endpoints { gpuIds id idleTimeout locations name networkVolumeId pods { desiredStatus } scalerType scalerValue templateId workersMax workersMin } serverlessDiscount { discountFactor type expirationDate } } }\"}' \\`\\`\\` \\`\\`\\`graphql query Endpoints { myself { endpoints { gpuIds id idleTimeout","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["391",{"pageContent":"locations name networkVolumeId pods { desiredStatus } scalerType scalerValue templateId workersMax workersMin } serverlessDiscount { discountFactor type expirationDate } } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"myself\": { \"endpoints\": \\[ { \"gpuIds\": \"AMPERE\\_16\", \"id\": \"i02xupws21hp6i\", \"idleTimeout\": 5, \"locations\": \"US\", \"name\": \"Generated Endpoint -fb\", \"networkVolumeId\": null, \"pods\": \\[], \"scalerType\": \"QUEUE\\_DELAY\", \"scalerValue\": 4, \"templateId\": \"xkhgg72fuo\", \"workersMax\": 0, \"workersMin\": 0","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["392",{"pageContent":"} ], \"serverlessDiscount\": null } } } \\`\\`\\` # Delete a Serverless Endpoints Note that your endpoint's min and max workers must both be set to zero for your call to work. \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { deleteEndpoint(id: \\\\\"i02xupws21hp6i\\\\\") }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { deleteEndpoint(id: \"i02xupws21hp6i\") } \\`\\`\\` \\`\\`\\`json { \"data\": {","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["393",{"pageContent":"\"deleteEndpoint\": null } } \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["394",{"pageContent":"\\--- title: \"Manage Templates\" sidebar\\_position: 3 description: Create, modify, and delete templates in RunPod using GraphQL API with various parameters for container disk size, Docker arguments, environment variables, and more. --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; Requrired arguments: - \\`containerDiskInGb\\` - \\`dockerArgs\\` - \\`env\\` - \\`imageName\\` - \\`name\\` - \\`volumeInGb\\` All other arguments are optional. If your container image is private, you can","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["395",{"pageContent":"also specify Docker login credentials with a \\`containerRegistryAuthId\\` argument, which takes the ID (\\_not\\_ the name) of the container registry credentials you saved in your RunPod user settings as a string. :::note Template names must be unique as well; if you try to create a new template with the same name as an existing one, your call will fail. ::: ## Create a Pod Template ### Create GPU Template \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["396",{"pageContent":"'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \\\\\"sleep infinity\\\\\", env: \\[ { key: \\\\\"key1\\\\\", value: \\\\\"value1\\\\\" }, { key: \\\\\"key2\\\\\", value: \\\\\"value2\\\\\" } ], imageName: \\\\\"ubuntu:latest\\\\\", name: \\\\\"Generated Template\\\\\", ports: \\\\\"8888/http,22/tcp\\\\\", readme: \\\\\"## Hello, World!\\\\\", volumeInGb: 15, volumeMountPath: \\\\\"/workspace\\\\\" }) { containerDiskInGb dockerArgs env { key value } id","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["397",{"pageContent":"imageName name ports readme volumeInGb volumeMountPath } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \"sleep infinity\", env: \\[ { key: \"key1\", value: \"value1\" }, { key: \"key2\", value: \"value2\" } ], imageName: \"ubuntu:latest\", name: \"Generated Template\", ports: \"8888/http,22/tcp\", readme: \"## Hello, World!\", volumeInGb: 15, volumeMountPath: \"/workspace\" }) { containerDiskInGb dockerArgs env { key value } id imageName name ports readme volumeInGb","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["398",{"pageContent":"volumeMountPath } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"saveTemplate\": { \"containerDiskInGb\": 5, \"dockerArgs\": \"sleep infinity\", \"env\": \\[ { \"key\": \"key1\", \"value\": \"value1\" }, { \"key\": \"key2\", \"value\": \"value2\" } ], \"id\": \"wphkv67a0p\", \"imageName\": \"ubuntu:latest\", \"name\": \"Generated Template\", \"ports\": \"8888/http,22/tcp\", \"readme\": \"## Hello, World!\", \"volumeInGb\": 15, \"volumeMountPath\": \"/workspace\" } } } \\`\\`\\` ### Create Serverless Template For Serverless templates, always pass \\`0\\` for","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["399",{"pageContent":"\\`volumeInGb\\`, since Serverless workers don't have persistent storage (other than those with network volumes). \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \\\\\"python handler.py\\\\\", env: \\[ { key: \\\\\"key1\\\\\", value: \\\\\"value1\\\\\" }, { key: \\\\\"key2\\\\\", value: \\\\\"value2\\\\\" } ], imageName:","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["400",{"pageContent":"\\\\\"runpod/serverless-hello-world:latest\\\\\", isServerless: true, name: \\\\\"Generated Serverless Template\\\\\", readme: \\\\\"## Hello, World!\\\\\", volumeInGb: 0 }) { containerDiskInGb dockerArgs env { key value } id imageName isServerless name readme } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \"python handler.py\", env: \\[ { key: \"key1\", value: \"value1\" }, { key: \"key2\", value: \"value2\" } ], imageName: \"runpod/serverless-hello-world:latest\",","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["401",{"pageContent":"isServerless: true, name: \"Generated Serverless Template\", readme: \"## Hello, World!\", volumeInGb: 0 }) { containerDiskInGb dockerArgs env { key value } id imageName isServerless name readme } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"saveTemplate\": { \"containerDiskInGb\": 5, \"dockerArgs\": \"python handler.py\", \"env\": \\[ { \"key\": \"key1\", \"value\": \"value1\" }, { \"key\": \"key2\", \"value\": \"value2\" } ], \"id\": \"xkhgg72fuo\", \"imageName\": \"runpod/serverless-hello-world:latest\", \"isServerless\": true, \"name\":","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["402",{"pageContent":"\"Generated Serverless Template\", \"readme\": \"## Hello, World!\" } } } \\`\\`\\` ## Modify a Template ### Modify a GPU Pod Template \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { saveTemplate(input: { id: \\\\\"wphkv67a0p\\\\\", containerDiskInGb: 5, dockerArgs: \\\\\"sleep infinity\\\\\", env: \\[ { key: \\\\\"key1\\\\\", value: \\\\\"value1\\\\\" }, { key: \\\\\"key2\\\\\", value: \\\\\"value2\\\\\" } ],","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["403",{"pageContent":"imageName: \\\\\"ubuntu:latest\\\\\", name: \\\\\"Generated Template\\\\\", volumeInGb: 15, readme: \\\\\"## Goodbye, World!\\\\\" }) { id containerDiskInGb dockerArgs env { key value } imageName name volumeInGb readme } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { saveTemplate(input: { id: \"wphkv67a0p\", containerDiskInGb: 5, dockerArgs: \"sleep infinity\", env: \\[ { key: \"key1\", value: \"value1\" }, { key: \"key2\", value: \"value2\" } ], imageName: \"ubuntu:latest\", name: \"Generated Template\", volumeInGb: 15, # Modify your","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["404",{"pageContent":"template options here (or above, if applicable). # For this example, we've modified the template's README. readme: \"## Goodbye, World!\" }) { id containerDiskInGb dockerArgs env { key value } imageName name volumeInGb # You can include what you've changed here, too. readme } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"saveTemplate\": { \"id\": \"wphkv67a0p\", \"containerDiskInGb\": 5, \"dockerArgs\": \"sleep infinity\", \"env\": \\[ { \"key\": \"key1\", \"value\": \"value1\" }, { \"key\": \"key2\", \"value\": \"value2\" } ],","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["405",{"pageContent":"\"imageName\": \"ubuntu:latest\", \"name\": \"Generated Template\", \"volumeInGb\": 15, \"readme\": \"## Goodbye, World!\" } } } \\`\\`\\` ### Modify a Serverless Template \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { saveTemplate(input: { id: \\\\\"xkhgg72fuo\\\\\", containerDiskInGb: 5, dockerArgs: \\\\\"python handler.py\\\\\", env: \\[ { key: \\\\\"key1\\\\\", value: \\\\\"value1\\\\\" }, { key: \\\\\"key2\\\\\",","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["406",{"pageContent":"value: \\\\\"value2\\\\\" } ], imageName: \\\\\"runpod/serverless-hello-world:latest\\\\\", name: \\\\\"Generated Serverless Template\\\\\", volumeInGb: 0, readme: \\\\\"## Goodbye, World!\\\\\" }) { id containerDiskInGb dockerArgs env { key value } imageName name readme } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { saveTemplate(input: { id: \"xkhgg72fuo\", containerDiskInGb: 5, dockerArgs: \"python handler.py\", env: \\[ { key: \"key1\", value: \"value1\" }, { key: \"key2\", value: \"value2\" } ], imageName:","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["407",{"pageContent":"\"runpod/serverless-hello-world:latest\", name: \"Generated Serverless Template\", volumeInGb: 0, # Modify your template options here (or above, if applicable). # For this example, we've modified the template's README. readme: \"## Goodbye, World!\" }) { id containerDiskInGb dockerArgs env { key value } imageName name # You can include what you've changed here, too. readme } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"saveTemplate\": { \"id\": \"xkhgg72fuo\", \"containerDiskInGb\": 5, \"dockerArgs\": \"python handler.py\",","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["408",{"pageContent":"\"env\": \\[ { \"key\": \"key1\", \"value\": \"value1\" }, { \"key\": \"key2\", \"value\": \"value2\" } ], \"imageName\": \"runpod/serverless-hello-world:latest\", \"name\": \"Generated Serverless Template\", \"readme\": \"## Goodbye, World!\" } } } \\`\\`\\` ## Delete a template Note that the template you'd like to delete must not be in use by any Pods or assigned to any Serverless endpoints. It can take up to 2 minutes to be able to delete a template after its most recent use by a Pod or Serverless endpoint, too. The same","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["409",{"pageContent":"mutation is used for deleting both Pod and Serverless templates. \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { deleteTemplate(templateName: \\\\\"Generated Template\\\\\") }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { deleteTemplate(templateName: \"Generated Template\") } \\`\\`\\` \\`\\`\\`json { \"data\": { \"deleteTemplate\": null } } \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":1}}}}],["410",{"pageContent":"\\--- title: \"Manage Pods\" sidebar\\_position: 2 description: \"Manage and deploy high-performance computing resources with RunPod's API, including authentication, GraphQL queries, and management of pods, GPUs, and storage.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; ## Authentication RunPod uses API Keys for all API requests. Go to \\[Settings]\\(https://www.runpod.io/console/user/settings) to manage your API keys. ## GraphQL API Spec If you need detailed queries,","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["411",{"pageContent":"mutations, fields, and inputs, look at the \\[GraphQL Spec]\\(https://graphql-spec.runpod.io/). ## Create Pods A Pod consists of the following resources: - 0 or more GPUs - A pod can be started with 0 GPUs for the purposes of accessing data, though GPU-accelerated functions and web services will fail to work. - vCPU - System RAM - Container Disk - It's temporary and removed when the pod is stopped or terminated. - You only pay for the container disk when the pod is running. - Instance Volume -","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["412",{"pageContent":"Data persists even when you reset or stop a Pod. Volume is removed when the Pod is terminated. - You pay for volume storage even when the Pod is stopped. ### Create On-Demand Pod \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { podFindAndDeployOnDemand( input: { cloudType: ALL, gpuCount: 1, volumeInGb: 40, containerDiskInGb: 40, minVcpuCount: 2, minMemoryInGb: 15,","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["413",{"pageContent":"gpuTypeId: \\\\\"NVIDIA RTX A6000\\\\\", name: \\\\\"RunPod Tensorflow\\\\\", imageName: \\\\\"runpod/tensorflow\\\\\", dockerArgs: \\\\\"\\\\\", ports: \\\\\"8888/http\\\\\", volumeMountPath: \\\\\"/workspace\\\\\", env: \\[{ key: \\\\\"JUPYTER\\_PASSWORD\\\\\", value: \\\\\"rn51hunbpgtltcpac3ol\\\\\" }] } ) { id imageName env machineId machine { podHostId } } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { podFindAndDeployOnDemand( input: { cloudType: ALL gpuCount: 1 volumeInGb: 40 containerDiskInGb: 40 minVcpuCount: 2 minMemoryInGb: 15 gpuTypeId:","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["414",{"pageContent":"\"NVIDIA RTX A6000\" name: \"RunPod Tensorflow\" imageName: \"runpod/tensorflow\" dockerArgs: \"\" ports: \"8888/http\" volumeMountPath: \"/workspace\" env: \\[{ key: \"JUPYTER\\_PASSWORD\", value: \"rn51hunbpgtltcpac3ol\" }] } ) { id imageName env machineId machine { podHostId } } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"podFindAndDeployOnDemand\": { \"id\": \"50qynxzilsxoey\", \"imageName\": \"runpod/tensorflow\", \"env\": \\[ \"JUPYTER\\_PASSWORD=rn51hunbpgtltcpac3ol\" ], \"machineId\": \"hpvdausak8xb\", \"machine\": { \"podHostId\":","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["415",{"pageContent":"\"50qynxzilsxoey-64410065\" } } } } \\`\\`\\` ### Create Spot Pod \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { podRentInterruptable( input: { bidPerGpu: 0.2, cloudType: SECURE, gpuCount: 1, volumeInGb: 40, containerDiskInGb: 40, minVcpuCount: 2, minMemoryInGb: 15, gpuTypeId: \\\\\"NVIDIA RTX A6000\\\\\", name: \\\\\"RunPod Pytorch\\\\\", imageName: \\\\\"runpod/pytorch\\\\\", dockerArgs:","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["416",{"pageContent":"\\\\\"\\\\\", ports: \\\\\"8888/http\\\\\", volumeMountPath: \\\\\"/workspace\\\\\", env: \\[{ key: \\\\\"JUPYTER\\_PASSWORD\\\\\", value: \\\\\"vunw9ybnzqwpia2795p2\\\\\" }] } ) { id imageName env machineId machine { podHostId } } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { podRentInterruptable( input: { bidPerGpu: 0.2 cloudType: SECURE gpuCount: 1 volumeInGb: 40 containerDiskInGb: 40 minVcpuCount: 2 minMemoryInGb: 15 gpuTypeId: \"NVIDIA RTX A6000\" name: \"RunPod Pytorch\" imageName: \"runpod/pytorch\" dockerArgs: \"\" ports: \"8888/http\"","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["417",{"pageContent":"volumeMountPath: \"/workspace\" env: \\[{ key: \"JUPYTER\\_PASSWORD\", value: \"vunw9ybnzqwpia2795p2\" }] } ) { id imageName env machineId machine { podHostId } } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"podRentInterruptable\": { \"id\": \"fkjbybgpwuvmhk\", \"imageName\": \"runpod/pytorch\", \"env\": \\[ \"JUPYTER\\_PASSWORD=vunw9ybnzqwpia2795p2\" ], \"machineId\": \"hpvdausak8xb\", \"machine\": { \"podHostId\": \"fkjbybgpwuvmhk-64410065\" } } } } \\`\\`\\` ## Start Pods ### Start On-Demand Pod \\`\\`\\`curl curl --request POST \\ --header","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["418",{"pageContent":"'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { podResume( input: { podId: \\\\\"inzk6tzuz833h5\\\\\", gpuCount: 1 } ) { id desiredStatus imageName env machineId machine { podHostId } } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { podResume(input: {podId: \"inzk6tzuz833h5\", gpuCount: 1}) { id desiredStatus imageName env machineId machine { podHostId } } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"podResume\": { \"id\": \"inzk6tzuz833h5\",","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["419",{"pageContent":"\"desiredStatus\": \"RUNNING\", \"imageName\": \"runpod/tensorflow\", \"env\": \\[ \"JUPYTER\\_PASSWORD=ywm4c9r15j1x6gfrds5n\" ], \"machineId\": \"hpvdausak8xb\", \"machine\": { \"podHostId\": \"inzk6tzuz833h5-64410065\" } } } } \\`\\`\\` ### Start Spot Pod \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { podBidResume( input: { podId: \\\\\"d62t7qg9n5vtan\\\\\", bidPerGpu: 0.2, gpuCount: 1 } ) { id","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["420",{"pageContent":"desiredStatus imageName env machineId machine { podHostId } } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { podBidResume(input: {podId: \"d62t7qg9n5vtan\", bidPerGpu: 0.2, gpuCount: 1}) { id desiredStatus imageName env machineId machine { podHostId } } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"podBidResume\": { \"id\": \"d62t7qg9n5vtan\", \"desiredStatus\": \"RUNNING\", \"imageName\": \"runpod/tensorflow\", \"env\": \\[ \"JUPYTER\\_PASSWORD=vunw9ybnzqwpia2795p2\" ], \"machineId\": \"hpvdausak8xb\", \"machine\": { \"podHostId\":","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["421",{"pageContent":"\"d62t7qg9n5vtan-64410065\" } } } } \\`\\`\\` ## Stop Pods \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"mutation { podStop(input: {podId: \\\\\"riixlu8oclhp\\\\\"}) { id desiredStatus } }\"}' \\`\\`\\` \\`\\`\\`graphql mutation { podStop(input: {podId: \"riixlu8oclhp\"}) { id desiredStatus } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"podStop\": { \"id\": \"riixlu8oclhp\", \"desiredStatus\": \"EXITED\" } } } \\`\\`\\` ##","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["422",{"pageContent":"List Pods ### List all Pods \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"query Pods { myself { pods { id name runtime { uptimeInSeconds ports { ip isIpPublic privatePort publicPort type } gpus { id gpuUtilPercent memoryUtilPercent } container { cpuPercent memoryPercent } } } } }\"}' \\`\\`\\` \\`\\`\\`graphql query Pods { myself { pods { id name runtime { uptimeInSeconds ports { ip","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["423",{"pageContent":"isIpPublic privatePort publicPort type } gpus { id gpuUtilPercent memoryUtilPercent } container { cpuPercent memoryPercent } } } } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"myself\": { \"pods\": \\[ { \"id\": \"ldl1dxirsim64n\", \"name\": \"RunPod Pytorch\", \"runtime\": { \"uptimeInSeconds\": 3931, \"ports\": \\[ { \"ip\": \"100.65.0.101\", \"isIpPublic\": false, \"privatePort\": 8888, \"publicPort\": 60141, \"type\": \"http\" } ], \"gpus\": \\[ { \"id\": \"GPU-e0488b7e-6932-795b-a125-4472c16ea72c\", \"gpuUtilPercent\": 0, \"memoryUtilPercent\":","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["424",{"pageContent":"0 } ], \"container\": { \"cpuPercent\": 0, \"memoryPercent\": 0 } } }, { \"id\": \"hpvdausak8xb00\", \"name\": \"hpvdausak8xb-BG\", \"runtime\": { \"uptimeInSeconds\": 858937, \"ports\": null, \"gpus\": \\[ { \"id\": \"GPU-2630fe4d-a75d-a9ae-8c15-6866088dfae2\", \"gpuUtilPercent\": 100, \"memoryUtilPercent\": 100 } ], \"container\": { \"cpuPercent\": 0, \"memoryPercent\": 1 } } } ] } } } \\`\\`\\` ### Get Pod by ID \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["425",{"pageContent":"'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"query Pod { pod(input: {podId: \\\\\"ldl1dxirsim64n\\\\\"}) { id name runtime { uptimeInSeconds ports { ip isIpPublic privatePort publicPort type } gpus { id gpuUtilPercent memoryUtilPercent } container { cpuPercent memoryPercent } } } }\"}' \\`\\`\\` \\`\\`\\`graphql query Pod { pod(input: {podId: \"ldl1dxirsim64n\"}) { id name runtime { uptimeInSeconds ports { ip isIpPublic privatePort publicPort type } gpus { id gpuUtilPercent","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["426",{"pageContent":"memoryUtilPercent } container { cpuPercent memoryPercent } } } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"pod\": { \"id\": \"ldl1dxirsim64n\", \"name\": \"RunPod Pytorch\", \"runtime\": { \"uptimeInSeconds\": 11, \"ports\": \\[ { \"ip\": \"100.65.0.101\", \"isIpPublic\": false, \"privatePort\": 8888, \"publicPort\": 60141, \"type\": \"http\" } ], \"gpus\": \\[ { \"id\": \"GPU-e0488b7e-6932-795b-a125-4472c16ea72c\", \"gpuUtilPercent\": 0, \"memoryUtilPercent\": 0 } ], \"container\": { \"cpuPercent\": 0, \"memoryPercent\": 0 } } } } } \\`\\`\\` ## List GPU","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["427",{"pageContent":"types When creating a Pod, you will need to pass GPU type IDs. These queries can help find all GPU types, their IDs, and other attributes like VRAM. \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"query GpuTypes { gpuTypes { id displayName memoryInGb } }\"}' \\`\\`\\` \\`\\`\\`graphql query GpuTypes { gpuTypes { id displayName memoryInGb } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"gpuTypes\": \\[ {","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["428",{"pageContent":"\"id\": \"NVIDIA GeForce RTX 3070\", \"displayName\": \"RTX 3070\", \"memoryInGb\": 8 }, { \"id\": \"NVIDIA GeForce RTX 3080\", \"displayName\": \"RTX 3080\", \"memoryInGb\": 10 }, { \"id\": \"NVIDIA RTX A6000\", \"displayName\": \"RTX A6000\", \"memoryInGb\": 48 } ] } } \\`\\`\\` ### Get GPU Type by ID \\`\\`\\`curl curl --request POST \\ --header 'content-type: application/json' \\ --url 'https://api.runpod.io/graphql?api\\_key=${YOUR\\_API\\_KEY}' \\ --data '{\"query\": \"query GpuTypes { gpuTypes(input: {id: \\\\\"NVIDIA GeForce RTX","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["429",{"pageContent":"3090\\\\\"}) { id displayName memoryInGb secureCloud communityCloud lowestPrice(input: {gpuCount: 1}) { minimumBidPrice uninterruptablePrice } } }\"}' \\`\\`\\` \\`\\`\\`graphql query GpuTypes { gpuTypes(input: {id: \"NVIDIA GeForce RTX 3090\"}) { id displayName memoryInGb secureCloud communityCloud lowestPrice(input: {gpuCount: 1}) { minimumBidPrice uninterruptablePrice } } } \\`\\`\\` \\`\\`\\`json { \"data\": { \"gpuTypes\": \\[ { \"id\": \"NVIDIA GeForce RTX 3090\", \"displayName\": \"RTX 3090\", \"memoryInGb\": 24,","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["430",{"pageContent":"\"secureCloud\": false, \"communityCloud\": true, \"lowestPrice\": { \"minimumBidPrice\": 0.163, \"uninterruptablePrice\": 0.3 } } ] } } \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":1}}}}],["431",{"pageContent":"\\--- title: Endpoints description: \"Learn how to interact with RunPod's endpoints using the JavaScript SDK, including synchronous and asynchronous execution methods, status checks, and job cancellation. Discover how to set timeouts, execute policies, and purge queues.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; Interacting with RunPod's endpoints is a core feature of the SDK, enabling the execution of tasks and the retrieval of results. This section covers the","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["432",{"pageContent":"synchronous and asynchronous execution methods, along with checking the status of operations. ## Prerequisites Before using the RunPod JavaScript, ensure that you have: - Installed the RunPod JavaScript SDK. - Configured your API key. ## Set your Endpoint Id Set your RunPod API key and your Endpoint Id as environment variables. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint =","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["433",{"pageContent":"runpod.endpoint(ENDPOINT\\_ID); \\`\\`\\` This allows all calls to pass through your Endpoint Id with a valid API key. In most situations, you'll set a variable name \\`endpoint\\` on the \\`Endpoint\\` class. This allows you to use the following methods or instances variables from the \\`Endpoint\\` class: - \\[health]\\(#health-check) - \\[purge\\_queue]\\(#purge-queue) - \\[runSync]\\(#run-synchronously) - \\[run]\\(#run-asynchronously) ## Run the Endpoint Run the Endpoint with the either the asynchronous","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["434",{"pageContent":"\\`run\\` or synchronous \\`runSync\\` method. Choosing between asynchronous and synchronous execution hinges on your task's needs and application design. ### Run synchronously To execute an endpoint synchronously and wait for the result, use the \\`runSync\\` method on your endpoint. This method blocks the execution until the endpoint run is complete or until it times out. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; const runpod =","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["435",{"pageContent":"runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const result = await endpoint.runSync({ \"input\": { \"prompt\": \"Hello, World!\", }, }); console.log(result); \\`\\`\\` \\`\\`\\`json { delayTime: 18, executionTime: 36595, id: 'sync-d050a3f6-791a-4aff-857a-66c759db4a06-u1', output: \\[ { choices: \\[Array], usage: \\[Object] } ], status: 'COMPLETED', started: true, completed: true, succeeded: true } \\`\\`\\` ## Run asynchronously Asynchronous execution allows for non-blocking","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["436",{"pageContent":"operations, enabling your code to perform other tasks while waiting for an operation to complete. For non-blocking operations, use the \\`run\\` method on the endpoint. This method allows you to start an endpoint run and then check its status or wait for its completion at a later time. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const result =","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["437",{"pageContent":"await endpoint.run({ \"input\": { \"prompt\": \"Hello, World!\", }, }); console.log(result); \\`\\`\\` \\`\\`\\`json { \"id\": \"d4e960f6-073f-4219-af24-cbae6b532c31-u1\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` ### Get results from an asynchronous run The following example shows how to get the results of an asynchronous run. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; const sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms)); async function","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["438",{"pageContent":"main() { const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const result = await endpoint.run({ input: { prompt: \"Hello, World!\", }, }); console.log(result); console.log(\"run response\"); console.log(result); const { id } = result; // Extracting the operation ID from the initial run response // Check the status in a loop, similar to the working example for (let i = 0; i < 20; i++) { // Increase or decrease the loop count as necessary const statusResult =","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["439",{"pageContent":"await endpoint.status(id); console.log(\"status response\"); console.log(statusResult); if ( statusResult.status === \"COMPLETED\" || statusResult.status === \"FAILED\" ) { // Once completed or failed, log the final status and break the loop if (statusResult.status === \"COMPLETED\") { console.log(\"Operation completed successfully.\"); console.log(statusResult.output); } else { console.log(\"Operation failed.\"); console.log(statusResult); } break; } // Wait for a bit before checking the status again await","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["440",{"pageContent":"sleep(5000); } } main(); \\`\\`\\` \\`\\`\\`json run response { id: 'c671a352-78e6-4eba-b2c8-2ea537c00897-u1', status: 'IN\\_QUEUE' } status response { delayTime: 19, id: 'c671a352-78e6-4eba-b2c8-2ea537c00897-u1', status: 'IN\\_PROGRESS', started: true, completed: false, succeeded: false } status response { delayTime: 19, executionTime: 539, id: 'c671a352-78e6-4eba-b2c8-2ea537c00897-u1', output: \\[ { choices: \\[Array], usage: \\[Object] } ], status: 'COMPLETED', started: true, completed: true, succeeded:","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["441",{"pageContent":"true } Operation completed successfully. \\[ { choices: \\[ \\[Object] ], usage: { input: 5, output: 16 } } ] \\`\\`\\` ### Poll the status of an asynchronous run Uses \\`await endpoint.status(id)\\` to check the status of the operation repeatedly until it either completes or fails. After each check, the function waits for 5 seconds (or any other suitable duration you choose) before checking the status again, using the sleep function. This approach ensures your application remains responsive and doesn't","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["442",{"pageContent":"overwhelm the Runpod endpoint with status requests. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; // Function to pause execution for a specified time const sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms)); async function main() { try { const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const result = await endpoint.run({ input: { prompt: \"Hello, World!\", }, }); const { id } =","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["443",{"pageContent":"result; if (!id) { console.error(\"No ID returned from endpoint.run\"); return; } // Poll the status of the operation until it completes or fails let isComplete = false; while (!isComplete) { const status = await endpoint.status(id); console.log(\\`Current status: ${status.status}\\`); if (status.status === \"COMPLETED\" || status.status === \"FAILED\") { isComplete = true; // Exit the loop console.log(\\`Operation ${status.status.toLowerCase()}.\\`); if (status.status === \"COMPLETED\") {","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["444",{"pageContent":"console.log(\"Output:\", status.output); } else { console.error(\"Error details:\", status.error); } } else { await sleep(5000); // Adjust the delay as needed } } } catch (error) { console.error(\"An error occurred:\", error); } } main(); \\`\\`\\` \\`\\`\\`text Current status: IN\\_QUEUE Current status: IN\\_PROGRESS Current status: COMPLETED Operation completed. Hello, World! \\`\\`\\` ## Stream Stream allows you to stream the output of an Endpoint run. To enable streaming, your handler must support the","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["445",{"pageContent":"\\`\"return\\_aggregate\\_stream\": True\\` option on the \\`start\\` method of your Handler. Once enabled, use the \\`stream\\` method to receive data as it becomes available. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; async function main() { const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const result = await endpoint.run({ input: { prompt: \"Hello, World!\", }, }); console.log(result); const { id","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["446",{"pageContent":"} = result; for await (const result of endpoint.stream(id)) { console.log(\\`${JSON.stringify(result, null, 2)}\\`); } console.log(\"done streaming\"); } main(); \\`\\`\\` \\`\\`\\`json { id: 'cb68890e-436f-4234-955d-001db6afe972-u1', status: 'IN\\_QUEUE' } { \"output\": \"H\" } { \"output\": \"e\" } { \"output\": \"l\" } { \"output\": \"l\" } { \"output\": \"o\" } { \"output\": \",\" } { \"output\": \" \" } { \"output\": \"W\" } { \"output\": \"o\" } { \"output\": \"r\" } { \"output\": \"l\" } { \"output\": \"d\" } { \"output\": \"!\" } done streaming","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["447",{"pageContent":"\\`\\`\\` You must define your handler to support the \\`\"return\\_aggregate\\_stream\": True\\` option on the \\`start\\` method. \\`\\`\\`python from time import sleep import runpod def handler(job): job\\_input = job\\[\"input\"]\\[\"prompt\"] for i in job\\_input: sleep(1) # sleep for 1 second for effect yield i runpod.serverless.start( { \"handler\": handler, \"return\\_aggregate\\_stream\": True, # Ensures aggregated results are streamed back } ) \\`\\`\\` ## Health check Monitor the health of an endpoint by checking","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["448",{"pageContent":"its status, including jobs completed, failed, in progress, in queue, and retried, as well as the status of workers. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const health = await endpoint.health(); console.log(health); \\`\\`\\` \\`\\`\\`json { \"jobs\": { \"completed\": 72, \"failed\": 1, \"inProgress\": 6, \"inQueue\": 0, \"retried\": 1 }, \"workers\": {","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["449",{"pageContent":"\"idle\": 4, \"initializing\": 0, \"ready\": 4, \"running\": 1, \"throttled\": 0 } } \\`\\`\\` ## Status Use the \\`status\\` method and specify the \\`id\\` of the run to get the status of a run. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; async function main() { try { const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const result = await endpoint.run({ input: { prompt: \"Hello, World!\", }, }); const { id }","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["450",{"pageContent":"= result; if (!id) { console.error(\"No ID returned from endpoint.run\"); return; } const status = await endpoint.status(id); console.log(status); } catch (error) { console.error(\"An error occurred:\", error); } } main(); \\`\\`\\` \\`\\`\\`json { \"delayTime\": 18, \"id\": \"792b1497-b2c8-4c95-90bf-4e2a6a2a37ff-u1\", \"status\": \"IN\\_PROGRESS\", \"started\": true, \"completed\": false, \"succeeded\": false } \\`\\`\\` ## Cancel You can cancel a Job request by using the \\`cancel()\\` function on the run request. You might","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["451",{"pageContent":"want to cancel a Job because it's stuck with a status of \\`IN\\_QUEUE\\` or \\`IN\\_PROGRESS\\`, or because you no longer need the result. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; async function main() { try { const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const result = await endpoint.run({ input: { prompt: \"Hello, World!\", }, }); const { id } = result; if (!id) { console.error(\"No ID","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["452",{"pageContent":"returned from endpoint.run\"); return; } const cancel = await endpoint.cancel(id); console.log(cancel); } catch (error) { console.error(\"An error occurred:\", error); } } main(); \\`\\`\\` \\`\\`\\`json { \"id\": \"5fb6a8db-a8fa-41a1-ad81-f5fad9755f9e-u1\", \"status\": \"CANCELLED\" } \\`\\`\\` ### Timeout To set a timeout on a run, pass a timeout value to the \\`run\\` method. Time is measured in milliseconds. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["453",{"pageContent":"\"runpod-sdk\"; const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const result = await endpoint.run({ \"input\": { \"prompt\": \"Hello, World!\", }, }, 5000); console.log(result); \\`\\`\\` \\`\\`\\`json { \"id\": \"43309f93-0422-4eac-92cf-e385dee36e99-u1\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` ### Execution policy You can set the maximum time to wait for a response from the endpoint in the \\`policy\\` parameter. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } =","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["454",{"pageContent":"process.env; import runpodSdk from \"runpod-sdk\"; const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); const result = await endpoint.run({ \"input\": { \"prompt\": \"Hello, World!\", }, policy: { executionTimeout: 5000, }, }); console.log(result); \\`\\`\\` \\`\\`\\`json { \"id\": \"21bd3763-dcbf-4091-84ee-85b80907a020-u1\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` For more information, see \\[Execution policy]\\(/serverless/endpoints/job-operations). ## Purge queue You can purge all","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["455",{"pageContent":"jobs from a queue by using the \\`purgeQueue()\\` function. \\`purgeQueue()\\` doesn't affect Jobs in progress. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; async function main() { try { const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); await endpoint.run({ input: { prompt: \"Hello, World!\", }, }); const purgeQueue = await endpoint.purgeQueue(); console.log(purgeQueue); } catch (error) {","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["456",{"pageContent":"console.error(\"An error occurred:\", error); } } main(); \\`\\`\\` \\`\\`\\`json { \"removed\": 1, \"status\": \"completed\" } \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["457",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 description: \"Get started with RunPod JavaScript SDK, a tool for building web apps, server-side implementations, and automating tasks. Learn how to install, integrate, and secure your API key for seamless development.\" --- Get started with setting up your RunPod projects using JavaScript. Whether you're building web applications, server-side implementations, or automating tasks, the RunPod JavaScript SDK provides the tools you need. This guide outlines","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["458",{"pageContent":"the steps to get your development environment ready and integrate RunPod into your JavaScript projects. ## Install the RunPod SDK Before integrating RunPod into your project, you'll need to install the SDK. Using Node.js and npm (Node Package Manager) simplifies this process. Ensure you have Node.js and npm installed on your system before proceeding. To install the RunPod SDK, run the following npm command in your project directory. \\`\\`\\`command npm install --save runpod-sdk # or yarn add","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["459",{"pageContent":"runpod-sdk \\`\\`\\` This command installs the \\`runpod-sdk\\` package and adds it to your project's \\`package.json\\` dependencies. For more details about the package, visit the \\[npm package page]\\(https://www.npmjs.com/package/runpod-sdk) or the \\[GitHub repository]\\(https://github.com/runpod/js-sdk). ## Add your API key To use the RunPod SDK in your project, you first need to import it and configure it with your API key and endpoint ID. Ensure these values are securely stored, preferably as","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["460",{"pageContent":"environment variables. Below is a basic example of how to initialize and use the RunPod SDK in your JavaScript project. \\`\\`\\`javascript const { RUNPOD\\_API\\_KEY, ENDPOINT\\_ID } = process.env; import runpodSdk from \"runpod-sdk\"; const runpod = runpodSdk(RUNPOD\\_API\\_KEY); const endpoint = runpod.endpoint(ENDPOINT\\_ID); \\`\\`\\` This snippet demonstrates how to import the SDK, initialize it with your API key, and reference a specific endpoint using its ID. Remember, the RunPod SDK uses the ES","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["461",{"pageContent":"Module (ESM) system and supports asynchronous operations, making it compatible with modern JavaScript development practices. ### Secure your API key When working with the RunPod SDK, it's essential to secure your API key. Storing the API key in environment variables is recommended, as shown in the initialization example. This method keeps your key out of your source code and reduces the risk of accidental exposure. :::note Use environment variables or secure secrets management solutions to","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["462",{"pageContent":"handle sensitive information like API keys. ::: For more information, see the following: - \\[RunPod SDK npm Package]\\(https://www.npmjs.com/package/runpod-sdk) - \\[RunPod GitHub Repository]\\(https://github.com/runpod/js-sdk) - \\[Endpoints]\\(/sdks/javascript/endpoints)","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["463",{"pageContent":"\\--- title: Overview description: \"Unlock serverless functionality with RunPod SDKs, enabling developers to create custom logic, simplify deployments, and programatically manage infrastructure, including Pods, Templates, and Endpoints.\" sidebar\\_position: 1 --- RunPod SDKs provide developers with tools to use the RunPod API for creating serverless functions and managing infrastructure. They enable custom logic integration, simplify deployments, and allow for programmatic infrastructure","metadata":{"source":"/runpod-docs/docs/sdks/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["464",{"pageContent":"management. ## Interacting with Serverless Endpoints Once deployed, serverless functions is exposed as an Endpoints, you can allow external applications to interact with them through HTTP requests. #### Interact with Serverless Endpoints: Your Serverless Endpoints works similarlly to an HTTP request. You will need to provide an Endpoint Id and a reference to your API key to complete requests. ## Infrastructure management The RunPod SDK facilitates the programmatic creation, configuration, and","metadata":{"source":"/runpod-docs/docs/sdks/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["465",{"pageContent":"management of various infrastructure components, including Pods, Templates, and Endpoints. ### Managing Pods Pods are the fundamental building blocks in RunPod, representing isolated environments for running applications. #### Manage Pods: 1. \\*\\*Create a Pod\\*\\*: Use the SDK to instantiate a new Pod with the desired configuration. 2. \\*\\*Configure the Pod\\*\\*: Adjust settings such as GPU, memory allocation, and network access according to your needs. 3. \\*\\*Deploy Applications\\*\\*: Deploy your","metadata":{"source":"/runpod-docs/docs/sdks/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["466",{"pageContent":"applications or services within the Pod. 4. \\*\\*Monitor and scale\\*\\*: Utilize the SDK to monitor Pod performance and scale resources as required. ### Manage Templates and Endpoints Templates define the base environment for Pods, while Endpoints enable external access to services running within Pods. #### Use Templates and Endpoints: 1. \\*\\*Create a Template\\*\\*: Define a Template that specifies the base configuration for Pods. 2. \\*\\*Instantiate Pods from Templates\\*\\*: Use the Template to","metadata":{"source":"/runpod-docs/docs/sdks/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["467",{"pageContent":"create Pods with a consistent environment. 3. \\*\\*Expose Services via Endpoints\\*\\*: Configure Endpoints to allow external access to applications running in Pods.","metadata":{"source":"/runpod-docs/docs/sdks/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["468",{"pageContent":"\\--- title: API Wrapper id: apis sidebar\\_label: APIs description: \"Learn how to manage computational resources with the RunPod API, including endpoint configurations, template creation, and GPU management, to optimize your project's performance.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; This document outlines the core functionalities provided by the RunPod API, including how to interact with Endpoints, manage Templates, and list available GPUs. These operations","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["469",{"pageContent":"let you dynamically manage computational resources within the RunPod environment. ## Get Endpoints To retrieve a comprehensive list of all available endpoint configurations within RunPod, you can use the \\`get\\_endpoints()\\` function. This function returns a list of endpoint configurations, allowing you to understand what's available for use in your projects. \\`\\`\\`python import runpod import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") # Fetching all available endpoints endpoints =","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["470",{"pageContent":"runpod.get\\_endpoints() # Displaying the list of endpoints print(endpoints) \\`\\`\\` ## Create Template Templates in RunPod serve as predefined configurations for setting up environments efficiently. The \\`create\\_template()\\` function facilitates the creation of new templates by specifying a name and a Docker image. \\`\\`\\`python import runpod import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") try: # Creating a new template with a specified name and Docker image new\\_template =","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["471",{"pageContent":"runpod.create\\_template(name=\"test\", image\\_name=\"runpod/base:0.1.0\") # Output the created template details print(new\\_template) except runpod.error.QueryError as err: # Handling potential errors during template creation print(err) print(err.query) \\`\\`\\` \\`\\`\\`json { \"id\": \"n6m0htekvq\", \"name\": \"test\", \"imageName\": \"runpod/base:0.1.0\", \"dockerArgs\": \"\", \"containerDiskInGb\": 10, \"volumeInGb\": 0, \"volumeMountPath\": \"/workspace\", \"ports\": \"\", \"env\": \\[], \"isServerless\": false } \\`\\`\\` ## Create","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["472",{"pageContent":"Endpoint Creating a new endpoint with the \\`create\\_endpoint()\\` function. This function requires you to specify a \\`name\\` and a \\`template\\_id\\`. Additional configurations such as GPUs, number of Workers, and more can also be specified depending your requirements. \\`\\`\\`python import runpod import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") try: # Creating a template to use with the new endpoint new\\_template = runpod.create\\_template( name=\"test\", image\\_name=\"runpod/base:0.4.4\",","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["473",{"pageContent":"is\\_serverless=True ) # Output the created template details print(new\\_template) # Creating a new endpoint using the previously created template new\\_endpoint = runpod.create\\_endpoint( name=\"test\", template\\_id=new\\_template\\[\"id\"], gpu\\_ids=\"AMPERE\\_16\", workers\\_min=0, workers\\_max=1, ) # Output the created endpoint details print(new\\_endpoint) except runpod.error.QueryError as err: # Handling potential errors during endpoint creation print(err) print(err.query) \\`\\`\\` \\`\\`\\`json { \"id\":","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["474",{"pageContent":"\"Unique\\_Id\", \"name\": \"YourTemplate\", \"imageName\": \"runpod/base:0.4.4\", \"dockerArgs\": \"\", \"containerDiskInGb\": 10, \"volumeInGb\": 0, \"volumeMountPath\": \"/workspace\", \"ports\": null, \"env\": \\[], \"isServerless\": true } { \"id\": \"Unique\\_Id\", \"name\": \"YourTemplate\", \"templateId\": \"Unique\\_Id\", \"gpuIds\": \"AMPERE\\_16\", \"networkVolumeId\": null, \"locations\": null, \"idleTimeout\": 5, \"scalerType\": \"QUEUE\\_DELAY\", \"scalerValue\": 4, \"workersMin\": 0, \"workersMax\": 1 } \\`\\`\\` ## Get GPUs For understanding the","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["475",{"pageContent":"computational resources available, the \\`get\\_gpus()\\` function lists all GPUs that can be allocated to endpoints in RunPod. This enables optimal resource selection based on your computational needs. \\`\\`\\`python import runpod import json import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") # Fetching all available GPUs gpus = runpod.get\\_gpus() # Displaying the GPUs in a formatted manner print(json.dumps(gpus, indent=2)) \\`\\`\\` \\`\\`\\`json \\[ { \"id\": \"NVIDIA A100 80GB PCIe\", \"displayName\":","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["476",{"pageContent":"\"A100 80GB\", \"memoryInGb\": 80 }, { \"id\": \"NVIDIA A100-SXM4-80GB\", \"displayName\": \"A100 SXM 80GB\", \"memoryInGb\": 80 } // Additional GPUs omitted for brevity ] \\`\\`\\` ## Get GPU by Id Use \\`get\\_gpu()\\` and pass in a GPU Id to retrieve details about a specific GPU model by its ID. This is useful when understanding the capabilities and costs associated with various GPU models. \\`\\`\\`python import runpod import json import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") gpus =","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["477",{"pageContent":"runpod.get\\_gpu(\"NVIDIA A100 80GB PCIe\") print(json.dumps(gpus, indent=2)) \\`\\`\\` \\`\\`\\`json { \"maxGpuCount\": 8, \"id\": \"NVIDIA A100 80GB PCIe\", \"displayName\": \"A100 80GB\", \"manufacturer\": \"Nvidia\", \"memoryInGb\": 80, \"cudaCores\": 0, \"secureCloud\": true, \"communityCloud\": true, \"securePrice\": 1.89, \"communityPrice\": 1.59, \"oneMonthPrice\": null, \"threeMonthPrice\": null, \"oneWeekPrice\": null, \"communitySpotPrice\": 0.89, \"secureSpotPrice\": null, \"lowestPrice\": { \"minimumBidPrice\": 0.89,","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["478",{"pageContent":"\"uninterruptablePrice\": 1.59 } } \\`\\`\\` Through these functionalities, the RunPod API enables efficient and flexible management of computational resources, catering to a wide range of project requirements.","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":1}}}}],["479",{"pageContent":"\\--- title: Endpoints description: \"Learn how to use the RunPod Python SDK to interact with various endpoints, perform synchronous and asynchronous operations, stream data, and check endpoint health. Discover how to set your Endpoint Id, run jobs, and cancel or purge queues with this comprehensive guide.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; This documentation provides detailed instructions on how to use the RunPod Python SDK to interact with various","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["480",{"pageContent":"endpoints. You can perform synchronous and asynchronous operations, stream data, and check the health status of endpoints. ## Prerequisites Before using the RunPod Python, ensure that you have: - Installed the RunPod Python SDK. - Configured your API key. ## Set your Endpoint Id Pass your Endpoint Id on the \\`Endpoint\\` class. \\`\\`\\`python import runpod import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") endpoint = runpod.Endpoint(\"YOUR\\_ENDPOINT\\_ID\") \\`\\`\\` This allows all calls to pass","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["481",{"pageContent":"through your Endpoint Id with a valid API key. In most situations, you'll set a variable name \\`endpoint\\` on the \\`Endpoint\\` class. This allows you to use the following methods or instances variables from the \\`Endpoint\\` class: - \\[health]\\(#health-check) - \\[purge\\_queue]\\(#purge-queue) - \\[run\\_sync]\\(#run-synchronously) - \\[run]\\(#run-asynchronously) ## Run the Endpoint Run the Endpoint with the either the asynchronous \\`run\\` or synchronous \\`run\\_sync\\` method. Choosing between","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["482",{"pageContent":"asynchronous and synchronous execution hinges on your task's needs and application design. - \\*\\*Asynchronous methods\\*\\*: Choose the asynchronous method for handling tasks efficiently, especially when immediate feedback isn't crucial. They allow your application to stay responsive by running time-consuming operations in the background, ideal for: - \\*\\*Non-blocking calls\\*\\*: Keep your application active while waiting on long processes. - \\*\\*Long-running operations\\*\\*: Avoid timeouts on tasks","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["483",{"pageContent":"over 30 seconds, letting your app's workflow continue smoothly. - \\*\\*Job tracking\\*\\*: Get a Job Id to monitor task status, useful for complex or delayed-result operations. - \\*\\*Synchronous methods\\*\\*: Choose the synchronous method for these when your application requires immediate results from operations. They're best for: - \\*\\*Immediate results\\*\\*: Necessary for operations where quick outcomes are essential to continue with your app's logic. - \\*\\*Short operations\\*\\*: Ideal for tasks","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["484",{"pageContent":"under 30 seconds to prevent application delays. - \\*\\*Simplicity and control\\*\\*: Provides a straightforward execution process, with timeout settings for better operational control. ### Run synchronously To execute an endpoint synchronously and wait for the result, use the \\`run\\_sync\\` method. This method blocks the execution until the endpoint run is complete or until it times out. \\`\\`\\`python import runpod import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") endpoint =","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["485",{"pageContent":"runpod.Endpoint(\"YOUR\\_ENDPOINT\\_ID\") try: run\\_request = endpoint.run\\_sync( { \"input\": { \"prompt\": \"Hello, world!\", } }, timeout=60, # Timeout in seconds. ) print(run\\_request) except TimeoutError: print(\"Job timed out.\") \\`\\`\\` ### Run asynchronously Asynchronous execution allows for non-blocking operations, enabling your code to perform other tasks while waiting for an operation to complete. RunPod supports both standard asynchronous execution and advanced asynchronous programming with","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["486",{"pageContent":"Python's \\[asyncio]\\(https://docs.python.org/3/library/asyncio.html) framework. Depending on your application's needs, you can choose the approach that best suits your scenario. For non-blocking operations, use the \\`run\\` method. This method allows you to start an endpoint run and then check its status or wait for its completion at a later time. #### Asynchronous execution This executes a standard Python environment without requiring an asynchronous event loop. \\`\\`\\`python import runpod import","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["487",{"pageContent":"os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") input\\_payload = {\"input\": {\"prompt\": \"Hello, World!\"}} try: endpoint = runpod.Endpoint(\"YOUR\\_ENDPOINT\\_ID\") run\\_request = endpoint.run(input\\_payload) # Initial check without blocking, useful for quick tasks status = run\\_request.status() print(f\"Initial job status: {status}\") if status != \"COMPLETED\": # Polling with timeout for long-running tasks output = run\\_request.output(timeout=60) else: output = run\\_request.output() print(f\"Job","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["488",{"pageContent":"output: {output}\") except Exception as e: print(f\"An error occurred: {e}\") \\`\\`\\` \\`\\`\\`text Initial job status: IN\\_QUEUE Job output: {'input\\_tokens': 24, 'output\\_tokens': 16, 'text': \\[\"Hello! How may I assist you today?\\n\"]} \\`\\`\\` #### Asynchronous execution with asyncio Use Python's \\`asyncio\\` library for handling concurrent Endpoint calls efficiently. This method embraces Python's asyncio framework for asynchronous programming, requiring functions to be defined with async and called","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["489",{"pageContent":"with await. This approach is inherently non-blocking and is built to handle concurrency efficiently. \\`\\`\\`python import asyncio import aiohttp import os import runpod from runpod import AsyncioEndpoint, AsyncioJob # asyncio.set\\_event\\_loop\\_policy(asyncio.WindowsSelectorEventLoopPolicy()) # For Windows users. runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") async def main(): async with aiohttp.ClientSession() as session: input\\_payload = {\"prompt\": \"Hello, World!\"} endpoint =","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["490",{"pageContent":"AsyncioEndpoint(\"YOUR\\_ENDPOINT\\_ID\", session) job: AsyncioJob = await endpoint.run(input\\_payload) # Polling job status while True: status = await job.status() print(f\"Current job status: {status}\") if status == \"COMPLETED\": output = await job.output() print(\"Job output:\", output) break # Exit the loop once the job is completed. elif status in \\[\"FAILED\"]: print(\"Job failed or encountered an error.\") break else: print(\"Job in queue or processing. Waiting 3 seconds...\") await asyncio.sleep(3) #","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["491",{"pageContent":"Wait for 3 seconds before polling again if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": asyncio.run(main()) \\`\\`\\` \\`\\`\\`text Current job status: IN\\_QUEUE Job in queue or processing. Waiting 3 seconds... Current job status: COMPLETED Job output: {'input\\_tokens': 24, 'output\\_tokens': 16, 'text': \\['Hello! How may I assist you today?\\n']} \\`\\`\\` ## Health check Monitor the health of an endpoint by checking its status, including jobs completed, failed, in progress, in queue, and retried, as well as the","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["492",{"pageContent":"status of workers. \\`\\`\\`python import runpod import json import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") endpoint = runpod.Endpoint(\"gwp4kx5yd3nur1\") endpoint\\_health = endpoint.health() print(json.dumps(endpoint\\_health, indent=2)) \\`\\`\\` \\`\\`\\`json { \"jobs\": { \"completed\": 100, \"failed\": 0, \"inProgress\": 0, \"inQueue\": 0, \"retried\": 0 }, \"workers\": { \"idle\": 1, \"initializing\": 0, \"ready\": 1, \"running\": 0, \"throttled\": 0 } } \\`\\`\\` ## Streaming To enable streaming, your handler must","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["493",{"pageContent":"support the \\`\"return\\_aggregate\\_stream\": True\\` option on the \\`start\\` method of your Handler. Once enabled, use the \\`stream\\` method to receive data as it becomes available. \\`\\`\\`python import runpod runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") endpoint = runpod.Endpoint(\"YOUR\\_ENDPOINT\\_ID\") run\\_request = endpoint.run( { \"input\": { \"prompt\": \"Hello, world!\", } } ) for output in run\\_request.stream(): print(output) \\`\\`\\` \\`\\`\\`python from time import sleep import runpod def","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["494",{"pageContent":"handler(job): job\\_input = job\\[\"input\"]\\[\"prompt\"] for i in job\\_input: sleep(1) # sleep for 1 second for effect yield i runpod.serverless.start( { \"handler\": handler, \"return\\_aggregate\\_stream\": True, # Ensures aggregated results are streamed back } ) \\`\\`\\` ## Status Returns the status of the Job request. Set the \\`status()\\` function on the run request to return the status of the Job. \\`\\`\\`python import runpod runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") input\\_payload = {\"input\":","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["495",{"pageContent":"{\"prompt\": \"Hello, World!\"}} try: endpoint = runpod.Endpoint(\"YOUR\\_ENDPOINT\\_ID\") run\\_request = endpoint.run(input\\_payload) # Initial check without blocking, useful for quick tasks status = run\\_request.status() print(f\"Initial job status: {status}\") if status != \"COMPLETED\": # Polling with timeout for long-running tasks output = run\\_request.output(timeout=60) else: output = run\\_request.output() print(f\"Job output: {output}\") except Exception as e: print(f\"An error occurred: {e}\") \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["496",{"pageContent":"\\`\\`\\`text Initial job status: IN\\_QUEUE Job output: Hello, World! \\`\\`\\` ## Cancel You can cancel a Job request by using the \\`cancel()\\` function on the run request. You might want to cancel a Job because it's stuck with a status of \\`IN\\_QUEUE\\` or \\`IN\\_PROGRESS\\`, or because you no longer need the result. The following pattern cancels a job given a human interaction, for example pressing \\`Ctrl+C\\` in the terminal. This sends a \\`SIGINT\\` signal to the running Job by catching the","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["497",{"pageContent":"\\`KeyboardInterrupt\\` exception. \\`\\`\\`python import time import runpod runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") input\\_payload = { \"messages\": \\[{\"role\": \"user\", \"content\": f\"Hello, World\"}], \"max\\_tokens\": 2048, \"use\\_openai\\_format\": True, } try: endpoint = runpod.Endpoint(\"YOUR\\_ENDPOINT\\_ID\") run\\_request = rp\\_endpoint.run(input\\_payload) while True: status = run\\_request.status() print(f\"Current job status: {status}\") if status == \"COMPLETED\": output = run\\_request.output()","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["498",{"pageContent":"print(\"Job output:\", output) generated\\_text = ( output.get(\"choices\", \\[{}])\\[0].get(\"message\", {}).get(\"content\") ) print(generated\\_text) break elif status in \\[\"FAILED\", \"ERROR\"]: print(\"Job failed to complete successfully.\") break else: time.sleep(10) except KeyboardInterrupt: # Catch KeyboardInterrupt print(\"KeyboardInterrupt detected. Canceling the job...\") if run\\_request: # Check if a job is active run\\_request.cancel() print(\"Job canceled.\") except Exception as e: print(f\"An error","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["499",{"pageContent":"occurred: {e}\") \\`\\`\\` \\`\\`\\`json Current job status: IN\\_QUEUE Current job status: IN\\_PROGRESS KeyboardInterrupt detected. Canceling the job... Job canceled. \\`\\`\\` ### Timeout Use the \\`cancel()\\` function and the \\`timeout\\` argument to cancel the Job after a specified time. In the previous \\`cancel()\\` example, the Job is canceled due to an external condition. In this example, you can cancel a running Job that has taken too long to complete. \\`\\`\\`python from time import sleep import runpod","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["500",{"pageContent":"import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") input\\_payload = {\"input\": {\"prompt\": \"Hello, World!\"}} endpoint = runpod.Endpoint(\"YOUR\\_ENDPOINT\\_ID\") # Submit the job request run\\_request = endpoint.run(input\\_payload) # Retrieve and print the initial job status initial\\_status = run\\_request.status() print(f\"Initial job status: {initial\\_status}\") # Attempt to cancel the job after a specified timeout period (in seconds) # Note: This demonstrates an immediate cancellation for","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["501",{"pageContent":"demonstration purposes. # Typically, you'd set the timeout based on expected job completion time. run\\_request.cancel(timeout=3) # Wait for the timeout period to ensure the cancellation takes effect sleep(3) print(\"Sleeping for 3 seconds to allow for job cancellation...\") # Check and print the job status after the sleep period final\\_status = run\\_request.status() print(f\"Final job status: {final\\_status}\") \\`\\`\\` \\`\\`\\`text Initial job status: IN\\_QUEUE Sleeping for 3 seconds to allow for job","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["502",{"pageContent":"cancellation... Final job status: CANCELLED \\`\\`\\` ## Purge queue You can purge all jobs from a queue by using the \\`purge\\_queue()\\` function. You can provide the \\`timeout\\` parameter to specify how long to wait for the server to respond before purging the queue. \\`purge\\_queue()\\` doesn't affect Jobs in progress. \\`\\`\\`python import runpod import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\") endpoint = runpod.Endpoint(\"YOUR\\_ENDPOINT\\_ID\") endpoint.purge\\_queue(timeout=3) \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["503",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 description: \"Get started with setting up your RunPod projects using Python. Learn how to install the RunPod SDK, create a Python virtual environment, and configure your API key for access to the RunPod platform.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; Get started with setting up your RunPod projects using Python. Depending on the specific needs of your project, there are various ways to interact with the RunPod","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["504",{"pageContent":"platform. This guide provides an approach to get you up and running. ## Install the RunPod SDK Create a Python virtual environment to install the RunPod SDK library. Virtual environments allow you to manage dependencies for different projects separately, avoiding conflicts between project requirements. To get started, install setup a virtual environment then install the RunPod SDK library. Create a Python virtual environment with \\[venv]\\(https://docs.python.org/3/library/venv.html):","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["505",{"pageContent":"\\`\\`\\`command python3 -m venv env source env/bin/activate \\`\\`\\` Create a Python virtual environment with \\[venv]\\(https://docs.python.org/3/library/venv.html): \\`\\`\\`command python -m venv env env\\Scripts\\activate \\`\\`\\` Create a Python virtual environment with \\[venv]\\(https://docs.python.org/3/library/venv.html): \\`\\`\\`command python3 -m venv env source env/bin/activate \\`\\`\\` To install the SDK, run the following command from the terminal. \\`\\`\\`command python -m pip install runpod \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["506",{"pageContent":"<!-- pip uninstall -y runpod --> You should have the RunPod SDK installed and ready to use. ## Get RunPod SDK version To ensure you've setup your RunPod SDK in Python, choose from one of the following methods to print the RunPod Python SDK version to your terminal. Run the following command using pip to get the RunPod SDK version. \\`\\`\\`command pip show runpod \\`\\`\\` You should see something similar to the following output. \\`\\`\\`command runpod==1.6.1 \\`\\`\\` Run the following command from your","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["507",{"pageContent":"terminal to get the RunPod SDK version. \\`\\`\\`command python3 -c \"import runpod; print(runpod.\\_\\_version\\_\\_)\" \\`\\`\\` To ensure you've setup your installation correctly, get the RunPod SDK version. Create a new file called \\`main.py\\`. Add the following to your Python file and execute the script. \\`\\`\\`python import runpod version = runpod.version.get\\_version() print(f\"RunPod version number: {version}\") \\`\\`\\` You should see something similar to the following output. \\`\\`\\`text RunPod version","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["508",{"pageContent":"number: 1.X.0 \\`\\`\\` You can find the latest version of the RunPod Python SDK on \\[GitHub]\\(https://github.com/runpod/runpod-python/releases). Now that you've installed the RunPod SDK, add your API key. ## Add your API key Set \\`api\\_key\\` and reference its variable in your Python application. This authenticates your requests to the RunPod platform and allows you to access the \\[RunPod API]\\(/sdks/python/apis). \\`\\`\\`python import runpod import os runpod.api\\_key = os.getenv(\"RUNPOD\\_API\\_KEY\")","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["509",{"pageContent":"\\`\\`\\` :::note It's recommended to use environment variables to set your API key. You shouldn't load your API key directly into your code. For these examples, the API key loads from an environment variable called \\`RUNPOD\\_API\\_KEY\\`. ::: Now that you've have the RunPod Python SDK installed and configured, you can start using the RunPod platform. For more information, see: - \\[APIs]\\(/sdks/python/apis) - \\[Endpoints]\\(/sdks/python/endpoints)","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["510",{"pageContent":"\\--- title: Loggers description: \"Enable efficient application monitoring and debugging with RunPod's structured logging interface, simplifying issue identification and resolution, and ensuring smooth operation.\" --- Logging is essential for insight into your application's performance and health. It facilitates quick identification and resolution of issues, ensuring smooth operation. Because of this, RunPod provides a structured logging interface, simplifying application monitoring and","metadata":{"source":"/runpod-docs/docs/sdks/python/_loggers.md","loc":{"lines":{"from":1,"to":1}}}}],["511",{"pageContent":"debugging, for your Handler code. To setup logs, instantiate the \\`RunPodLogger()\\` module. \\`\\`\\`python import runpod log = runpod.RunPodLogger() \\`\\`\\` Then set the log level. In the following example, there are two logs levels being set. \\`\\`\\`python import runpod import os log = runpod.RunPodLogger() def handler(job): try: job\\_input = job\\[\"input\"] log.info(\"Processing job input\") name = job\\_input.get(\"name\", \"World\") log.info(\"Processing completed successfully\") return f\"Hello, {name}!\"","metadata":{"source":"/runpod-docs/docs/sdks/python/_loggers.md","loc":{"lines":{"from":1,"to":1}}}}],["512",{"pageContent":"except Exception as e: # Log the exception with an error level log log.error(f\"An error occurred: {str(e)}\") return \"An error occurred during processing.\" runpod.serverless.start({\"handler\": handler}) \\`\\`\\` ## Log levels RunPod provides a logging interface with types you're already familiar with. The following provides a list of log levels you can set inside your application. - \\`debug\\`: For in-depth troubleshooting. Use during development to track execution flow. - \\`info\\`: (default)","metadata":{"source":"/runpod-docs/docs/sdks/python/_loggers.md","loc":{"lines":{"from":1,"to":1}}}}],["513",{"pageContent":"Indicates normal operation. Confirms the application is running as expected. - \\`warn\\`: Alerts to potential issues. Signals unexpected but non-critical events. - \\`error\\`: Highlights failures. Marks inability to perform a function, requiring immediate attention.","metadata":{"source":"/runpod-docs/docs/sdks/python/_loggers.md","loc":{"lines":{"from":1,"to":1}}}}],["514",{"pageContent":"\\--- title: Get started sidebar\\_position: 2 description: \"Learn how to test your deployed Endpoint with a sample request, view the response, and send requests using cURL or an HTTP client, then customize your Handler Function for more control over your API.\" --- Now that your Endpoint is deployed, send a test. This is a great way to test your Endpoint before sending a request from your application. ## Send a Request 1. From the Endpoint's page, select \\*\\*Requests\\*\\*. 2. Choose \\*\\*Run\\*\\*. 3.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["515",{"pageContent":"You should see a successful response with the following: \\`\\`\\`json { \"id\": \"6de99fd1-4474-4565-9243-694ffeb65218-u1\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` After a few minutes, the stream will show the full response. You can now begin sending requests to your Endpoint from your terminal and an application. ## Send a request using cURL Once your Endpoint is deployed, you can send a request. This example sends a response to the Endpoint using cURL; however, you can use any HTTP client. \\`\\`\\`curl curl","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["516",{"pageContent":"--request POST \\ --url https://api.runpod.ai/v2/${endpoint\\_id}/runsync --header \"accept: application/json\" \\ --header \"authorization: ${YOUR\\_API\\_KEY}\" \\ --header \"content-type: application/json\" \\ --data ' { \"input\": { \"prompt\": \"A coffee cup.\", \"height\": 512, \"width\": 512, \"num\\_outputs\": 1, \"num\\_inference\\_steps\": 50, \"guidance\\_scale\": 7.5, \"scheduler\": \"KLMS\" } } ' \\`\\`\\` Where \\`endpoint\\_id\\` is the name of your Endpoint and \\`YOUR\\_API\\_KEY\\` is your API Key. :::note Depending on any","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["517",{"pageContent":"modifications you made to your Handler Function, you may need to modify the request. ::: ## Next steps Now that you have successfully launched an endpoint using a template, you can: - \\[Invoke jobs]\\(/serverless/endpoints/job-operations) If the models provided aren't enough, you can write your own customize Function Handler: - \\[Customize the Handler Function]\\(/serverless/workers/handlers/overview)","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["518",{"pageContent":"\\--- title: Job operations description: \"Learn how to use the Runpod Endpoint to manage job operations, including running, checking status, purging queues, and streaming results, with cURL and SDK examples.\" sidebar\\_position: 2 --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; This page provides instructions on job operations using the Runpod Endpoint. You can invoke a job to run Endpoints the way you would interact with an API, get a status of a job, purge your job","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["519",{"pageContent":"queue, and more with operations. The following guide demonstrates how to use cURL to interact with an Endpoint. You can also use the following SDK to interact with Endpoints programmatically: - \\[Python SDK]\\(/sdks/python/endpoints) For information on sending requests, see \\[Send a request]\\(/serverless/endpoints/send-requests). ## Asynchronous Endpoints Asynchronous endpoints are designed for long-running tasks. When you submit a job through these endpoints, you receive a Job ID in response.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["520",{"pageContent":"You can use this Job ID to check the status of your job at a later time, allowing your application to continue processing without waiting for the job to complete immediately. This approach is particularly useful for tasks that require significant processing time or when you want to manage multiple jobs concurrently. \\`\\`\\`bash curl -X POST https://api.runpod.ai/v2/{endpoint\\_id}/run \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer ${API\\_KEY}' \\ -d '{\"input\": {\"prompt\": \"Your","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["521",{"pageContent":"prompt\"}}' \\`\\`\\` \\`\\`\\`json { \"id\": \"eaebd6e7-6a92-4bb8-a911-f996ac5ea99d\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` ## Synchronous Endpoints Synchronous endpoints are ideal for short-lived tasks where immediate results are necessary. Unlike asynchronous calls, synchronous endpoints wait for the job to complete and return the result directly in the response. This method is suitable for operations that are expected to complete quickly and where the client can afford to wait for the result. \\`\\`\\`bash curl","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["522",{"pageContent":"-X POST https://api.runpod.ai/v2/{endpoint\\_id}/runsync \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer ${API\\_KEY}' \\ -d '{\"input\": {\"prompt\": \"Your prompt\"}}' \\`\\`\\` \\`\\`\\`json { \"delayTime\": 824, \"executionTime\": 3391, \"id\": \"sync-79164ff4-d212-44bc-9fe3-389e199a5c15\", \"output\": \\[ { \"image\": \"https://image.url\", \"seed\": 46578 } ], \"status\": \"COMPLETED\" } \\`\\`\\` ## Health Endpoint The \\`/health\\` endpoint provides insights into the operational status of the endpoint,","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["523",{"pageContent":"including the number of workers available and job statistics. This information can be used to monitor the health and performance of the API, helping you manage workload and troubleshoot issues more effectively. \\`\\`\\`bash curl --request GET \\ --url https://api.runpod.ai/v2/{endpoint\\_id}/health \\ --header 'accept: application/json' \\ --header 'Authorization: Bearer ${API\\_KEY}' \\`\\`\\` \\`\\`\\`json { \"jobs\": { \"completed\": 1, \"failed\": 5, \"inProgress\": 0, \"inQueue\": 2, \"retried\": 0 }, \"workers\": {","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["524",{"pageContent":"\"idle\": 0, \"running\": 0 } } \\`\\`\\` ## Cancel Job To cancel a job in progress, specify the \\`cancel\\` parameter with the endpoint ID and the job ID. \\`\\`\\`bash curl -X POST https://api.runpod.ai/v2/{endpoint\\_id}/cancel/{job\\_id} \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer ${API\\_KEY}' \\`\\`\\` \\`\\`\\`json { \"id\": \"724907fe-7bcc-4e42-998d-52cb93e1421f-u1\", \"status\": \"CANCELLED\" } \\`\\`\\` ## Purge Queue Endpoint The \\`/purge-queue\\` endpoint allows you to clear all jobs that are","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["525",{"pageContent":"currently in the queue. This operation does not affect jobs that are already in progress. It is a useful tool for managing your job queue, especially in situations where you need to reset or clear pending tasks due to operational changes or errors. \\`\\`\\`bash curl -X POST https://api.runpod.ai/v2/{endpoint\\_id}/purge-queue \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer ${API\\_KEY}' \\`\\`\\` \\`\\`\\`json { \"removed\": 2, \"status\": \"completed\" } \\`\\`\\` ## Check Job Status To track","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["526",{"pageContent":"the progress or result of an asynchronous job, you can check its status using the Job ID. This endpoint provides detailed information about the job, including its current status, execution time, and the output if the job has completed. \\`\\`\\`bash curl https://api.runpod.ai/v2/{endpoint\\_id}/status/{job\\_id} \\ -H 'Authorization: Bearer ${API\\_KEY}' \\`\\`\\` \\`\\`\\`json { \"delayTime\": 31618, \"executionTime\": 1437, \"id\": \"60902e6c-08a1-426e-9cb9-9eaec90f5e2b-u1\", \"output\": { \"input\\_tokens\": 22,","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["527",{"pageContent":"\"output\\_tokens\": 16, \"text\": \\[ \"Hello! How can I assist you today?\\nUSER: I'm having\" ] }, \"status\": \"COMPLETED\" } \\`\\`\\` ## Stream results For jobs that produce output incrementally, the stream endpoint allows you to receive results as they are generated. This is particularly useful for tasks that involve continuous data processing or where immediate partial results are beneficial. \\`\\`\\`bash curl https://api.runpod.ai/v2/{endpoint\\_id}/stream/{job\\_id} \\ -H 'Content-Type: application/json' \\","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["528",{"pageContent":"-H 'Authorization: Bearer ${API\\_KEY}' \\`\\`\\` \\`\\`\\`json \\[ { \"metrics\": { \"avg\\_gen\\_throughput\": 0, \"avg\\_prompt\\_throughput\": 0, \"cpu\\_kv\\_cache\\_usage\": 0, \"gpu\\_kv\\_cache\\_usage\": 0.0016722408026755853, \"input\\_tokens\": 0, \"output\\_tokens\": 1, \"pending\": 0, \"running\": 1, \"scenario\": \"stream\", \"stream\\_index\": 2, \"swapped\": 0 }, \"output\": { \"input\\_tokens\": 0, \"output\\_tokens\": 1, \"text\": \\[ \" How\" ] } } // omitted for brevity ] \\`\\`\\` ## Rate Limits - \\`/run\\`: 1000 requests every 10","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["529",{"pageContent":"seconds. - \\`/runsync\\`: 2000 requests every 10 seconds. :::note Retrieve results within 30 minutes for privacy protection. ::: For reference information on Endpoints, see \\[Endpoint Operations]\\(/serverless/references/operations.md).","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":1}}}}],["530",{"pageContent":"\\--- title: \"Manage Endpoints\" description: \"Learn to create, edit, and manage Serverless Endpoints, including adding network volumes and setting GPU prioritization, with step-by-step guides and tutorials.\" sidebar\\_position: 10 --- Learn to manage Severless Endpoints. ## Create an Endpoint You can create an Endpoint in the Web interface. 1. Navigate to \\[Serverless Endpoints]\\(https://www.runpod.io/console/serverless). 2. Select \\*\\*+ New Endpoint\\*\\* and enter the following: 1. Endpoint Name.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["531",{"pageContent":"2. Select your GPUs. 3. Configure your workers. 4. Add a container image. 5. Select \\*\\*Deploy\\*\\*. ## Delete an Endpoint You can delete an Endpoint in the Web interface. Before an Endpoint can be deleted, all workers must be removed. 1. Navigate to \\[Serverless Endpoints]\\(https://www.runpod.io/console/serverless). 2. Select the Endpoint you'd like to remove. 3. Select \\*\\*Edit Endpoint\\*\\* and set \\*\\*Max Workers\\*\\* to \\`0\\`. 4. Choose \\*\\*Update\\*\\* and then \\*\\*Delete Endpoint\\*\\*. ## Edit","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["532",{"pageContent":"an Endpoint You can edit a running Endpoint in the Web interface after you've deployed it. 1. Navigate to \\[Serverless Endpoints]\\(https://www.runpod.io/console/serverless). 2. Select the Endpoint you'd like to edit. 3. Select \\*\\*Edit Endpoint\\*\\* and make your changes. 4. Choose \\*\\*Update\\*\\*. ## Set GPU prioritization an Endpoint When creating or modifying a Worker Endpoint, specify your GPU preferences in descending order of priority. This allows you to configure the desired GPU models for","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["533",{"pageContent":"your Worker Endpoints. RunPod attempts to allocate your first choice if it's available. If your preferred GPU isn't available, the system automatically defaults to the next available GPU in your priority list. 1. Navigate to \\[Serverless Endpoints]\\(https://www.runpod.io/console/serverless). 2. Select the Endpoint you'd like to update. 3. Select the priority of the GPUs you'd like to use. 4. Choose \\*\\*Update\\*\\*. :::note You can force a configuration update by setting \\*\\*Max Workers\\*\\* to 0,","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["534",{"pageContent":"selecting \\*\\*Update\\*\\*, then updating your max workers back to your needed value. ::: ## Add a Network Volume Network volumes are a way to share data between Workers: they are mounted to the same path on each Worker. For example, if a Worker contains a large-language model, you can use a network volume to share the model across all Workers. 1. Navigate to \\[Serverless Endpoints]\\(https://www.runpod.io/console/serverless). 2. Select the Endpoint you'd like to edit. 3. Select \\*\\*Edit","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["535",{"pageContent":"Endpoint\\*\\* and make your changes. 4. Under \\*\\*Advanced\\*\\* choose \\*\\*Select Network Volume\\*\\*. 5. Select the storage device and then choose \\*\\*Update\\*\\* to continue.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["536",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 description: \"Deploy and manage serverless workers with RunPod Endpoints, featuring asynchronous and synchronous operations, scalability, and flexibility for modern computing tasks.\" --- RunPod Endpoints serve as the gateway to deploying and managing your Serverless Workers. These endpoints allow for flexible interaction with a variety of models, supporting both asynchronous and synchronous operations tailored to your computational needs. Whether you're","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["537",{"pageContent":"processing large data sets, requiring immediate results, or scheduling tasks to run in the background, RunPod's API Endpoints provide the versatility and scalability essential for modern computing tasks. ### Key features - \\*\\*Asynchronous and synchronous jobs:\\*\\* Choose the execution mode that best fits your workflow, whether it's a task that runs in the background or one that delivers immediate results. - \\*\\*Serverless Workers:\\*\\* Deploy your computational tasks without worrying about","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["538",{"pageContent":"server management, enjoying the benefits of a fully managed infrastructure. - \\*\\*Scalability and flexibility:\\*\\* Easily scale your operations up or down based on demand, with the flexibility to handle various computational loads. ### Getting started Before you begin, ensure you have obtained your \\[RunPod API key]\\(/get-started/api-keys). This key is essential for authentication, billing, and accessing the API. You can find your API key in the \\[user settings","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["539",{"pageContent":"section]\\(https://www.runpod.io/console/user/settings) of your RunPod account. :::note \\*\\*Privacy and security:\\*\\* RunPod prioritizes your data's privacy and security. Inputs and outputs are retained for a maximum of 30 minutes for asynchronous requests and 1 minute for synchronous requests to protect your information. ::: ### Exploring RunPod Endpoints Dive deeper into what you can achieve with RunPod Endpoints through the following resources: - \\[Use the vLLM","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["540",{"pageContent":"Worker]\\(/serverless/workers/vllm/overview): Learn how to deploy a vLLM Worker as a Serverless Endpoint, with detailed guides on configuration and sending requests. - \\[Invoke Jobs]\\(/serverless/endpoints/job-operations): Learn how to submit jobs to your serverless workers, with detailed guides on both asynchronous and synchronous operations. - \\[Send Requests]\\(/serverless/endpoints/send-requests): Discover how to communicate with your endpoints, including tips on structuring requests for","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["541",{"pageContent":"optimal performance. - \\[Manage Endpoints]\\(/serverless/endpoints/manage-endpoints): Find out how to manage your endpoints effectively, from deployment to scaling and monitoring. - \\[Endpoint Operations]\\(/serverless/references/operations): Access a comprehensive list of operations supported by RunPod Endpoints, including detailed documentation and examples.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["542",{"pageContent":"\\--- title: \"Send a request\" description: \"Learn how to construct a JSON request body to send to your custom endpoint, including optional inputs for webhooks, execution policies, and S3-compatible storage, to optimize job execution and resource management.\" sidebar\\_position: 4 --- Before sending a job request, ensure you have deployed your custom endpoint. Let's start by constructing our request body to send to the endpoint. ## JSON Request Body You can make requests to your endpoint with JSON.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":1}}}}],["543",{"pageContent":"Your request must include a JSON object containing an \\`input\\` key. For example, if your handler requires an input prompt, you might send in something like this: \\`\\`\\`json { \"input\": { \"prompt\": \"The lazy brown fox jumps over the\" } } \\`\\`\\` ## Optional Inputs Along with an input key, you can include other top-level inputs to access different functions. If a key is passed in at the top level and not included in the body of your request, it will be discarded and unavailable to your handler. The","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":1}}}}],["544",{"pageContent":"following optional inputs are available to all endpoints regardless of the worker. - Webhooks - Execution policies - S3-compatible storage ### Webhooks To see notifications for completed jobs, pass a URL in the top level of the request: \\`\\`\\`json { \"input\": {}, \"webhook\": \"https://URL.TO.YOUR.WEBHOOK\" } \\`\\`\\` Your webhook endpoint should respond with a \\`200\\` status to acknowledge the successful call. If the call is not successful, the request waits 10 seconds and sends the call again up to","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":1}}}}],["545",{"pageContent":"two more times. A \\`POST\\` request goes to your URL when the job is complete. This request contains the same information as fetching the results from the \\`/status/{job\\_id}\\` endpoint. ### Execution Policies By default, if a job remains \\`IN\\_PROGRESS\\` for longer than 10 minutes without completion, it's automatically terminated. This default behavior keeps a hanging request from draining your account credits. To customize the management of job lifecycles and resource consumption, the following","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":1}}}}],["546",{"pageContent":"policies can be configured: - \\*\\*Execution Timeout\\*\\*: Specifies the maximum duration that a job can run before it's automatically terminated. This limit helps prevent jobs from running indefinitely and consuming resources. You can overwrite the value for a request by specifying \\`executionTimeout\\` in the job input. :::note Changing the \\*\\*Execution Timeout\\*\\* value through the Web UI sets the value for all requests to an Endpoint. You can still overwrite the value for individual requests","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":1}}}}],["547",{"pageContent":"with \\`executionTimeout\\` in the job input. ::: - \\*\\*Low Priority\\*\\*: When true, the job does not trigger scaling up resources to execute. Instead, it executes when there are no pending higher priority jobs in the queue. Use this option for tasks that are not time-sensitive. - \\*\\*TTL (Time-to-Live)\\*\\*: Defines the maximum time a job can remain in the queue before it's automatically terminated. This parameter ensures that jobs don't stay in the queue indefinitely. \\`\\`\\`json { \"input\": {},","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":1}}}}],["548",{"pageContent":"\"policy\": { \"executionTimeout\": int, // Time in milliseconds. Must be greater than 5 seconds. \"lowPriority\": bool, // Sets the job's priority to low. Default behavior escalates to high under certain conditions. \"ttl\": int // Time in milliseconds. Must be greater than or equal to 10 seconds. Default is 24 hours. Maximum is one week. } } \\`\\`\\` By configuring the execution timeout, priority, and TTL policies, you have more control over job execution and efficient system resource management. ###","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":1}}}}],["549",{"pageContent":"S3-Compatible Storage Pass in the credentials for S3-compatible object storage as follows: \\`\\`\\`json { \"input\": {}, \"s3Config\": { \"accessId\": \"key\\_id\\_or\\_username\", \"accessSecret\": \"key\\_secret\\_or\\_password\", \"bucketName\": \"storage\\_location\\_name\", \"endpointUrl\": \"storage\\_location\\_address\" } } \\`\\`\\` The configuration only passes to the worker. It is not returned as part of the job request output. :::note The serverless worker must contain logic that allows it to use this input. If you","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":1}}}}],["550",{"pageContent":"build a custom endpoint and request s3Config in the job input, your worker is ultimately responsible for using the information passed in to upload the output. :::","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":1}}}}],["551",{"pageContent":"\\--- title: Overview description: \"Scale machine learning workloads with RunPod Serverless, offering flexible GPU computing for AI inference, training, and general compute, with pay-per-second pricing and fast deployment options for custom endpoints and handler functions.\" sidebar\\_position: 1 --- RunPod offers Serverless GPU and CPU computing for AI inference, training, and general compute, allowing users to pay by the second for their compute usage. This flexible platform is designed to scale","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["552",{"pageContent":"dynamically, meeting the computational needs of AI workloads from the smallest to the largest scales. You can use the following methods: - Quick Deploy: Quick deploys are pre-built custom endpoints of the most popular AI models. - Handler Functions: Bring your own functions and run in the cloud. - vLLM Endpoint: Specify a Hugging Face model and run in the cloud. ## Why RunPod Serverless? You should choose RunPod Serverless instances for the following reasons: - \\*\\*AI Inference:\\*\\* Handle","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["553",{"pageContent":"millions of inference requests daily and can be scaled to handle billions, making it an ideal solution for machine learning inference tasks. This allows users to scale their machine learning inference while keeping costs low. - \\*\\*AI Training:\\*\\* Machine learning training tasks that can take up to 12 hours. GPUs can be spun up per request and scaled down once the task is done, providing a flexible solution for AI training needs. - \\*\\*Autoscale:\\*\\* Dynamically scale workers from 0 to 100 on","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["554",{"pageContent":"the Secure Cloud platform, which is highly available and distributed globally. This provides users with the computational resources exactly when needed. - \\*\\*Container Support:\\*\\* Bring any Docker container to RunPod. Both public and private image repositories are supported, allowing users to configure their environment exactly how they want. - \\*\\*3s Cold-Start:\\*\\* To help reduce cold-start times, RunPod proactively pre-warms workers. The total start time will vary based on the runtime, but","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["555",{"pageContent":"for stable diffusion, the total start time is 3 seconds cold-start plus 5 seconds runtime. - \\*\\*Metrics and Debugging:\\*\\* Transparency is vital in debugging. RunPod provides access to GPU, CPU, Memory, and other metrics to help users understand their computational workloads. Full debugging capabilities for workers through logs and SSH are also available, with a web terminal for even easier access. - \\*\\*Webhooks:\\*\\* Users can leverage webhooks to get data output as soon as a request is done.","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["556",{"pageContent":"Data is pushed directly to the user's Webhook API, providing instant access to results. RunPod Serverless are not just for AI Inference and Training. They're also great for a variety of other use cases. Feel free to use them for tasks like rendering, molecular dynamics, or any other computational task that suits your needs. ## How to interact with RunPod Serverless? RunPod generates an Endpoint Id that that allows you to interact with your Serverless Pod. Pass in your Endpoint Id to the Endpoint","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["557",{"pageContent":"URL and provide an operation. This Endpoint URL will look like this: \\`\\`\\`text https://api.runpod.ai/v2/{endpoint\\_id}/{operation} \\`\\`\\` - \\`api.runpod.ai\\`: The base URL to access RunPod. - \\`v2\\`: The API version. - \\`endpoint\\_id\\`: The ID of the Serverless Endpoint. - \\`operation\\`: The operation to perform on the Serverless Endpoint. - Valid options: \\`run\\` | \\`runsync\\` | \\`status\\` | \\`cancel\\` | \\`health\\` | \\`purge-queue\\` For more information, see \\[Invoke","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["558",{"pageContent":"jobs]\\(/serverless/endpoints/job-operations). <!-- ### Endpoints A Serverless Endpoint provides the REST API endpoint that serves your application. You can create multiple endpoints for your application, each with its own configuration. ### Serverless handlers Serverless handlers are the core of the Serverless platform. They are the code that is executed when a request is made to a Serverless endpoint. Handlers are written in Python and can be used to run any code that can be run in a Docker","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["559",{"pageContent":"container. -->","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["560",{"pageContent":"\\--- title: Quick Deploys id: quick-deploys sidebar\\_position: 2 description: \"Quickly deploy custom Endpoints of popular models with minimal configuration through the web interface, following a simple 5-step process. Customize your deployments and utilize CI/CD features with RunPod's GitHub repositories and Handler Functions.\" --- Quick Deploys lets you deploy custom Endpoints of popular models with minimal configuration. You can find \\[Quick Deploys]\\(https://www.runpod.io/console/serverless)","metadata":{"source":"/runpod-docs/docs/serverless/quick-deploys.md","loc":{"lines":{"from":1,"to":1}}}}],["561",{"pageContent":"and their descriptions in the Web interface. ## How to do I get started with Quick Deploys? You can get started by following the steps below: 1. Go to the \\[Serverless section]\\(https://www.runpod.io/console/serverless) in the Web interface. 2. Select your model. 3. Provide a name for your Endpoint. 4. Select your GPU instance. 1. (optional) You can further customize your deployment. 5. Select \\*\\*Deploy\\*\\*. Your Endpoint Id is now created and you can use it in your application. ## Customizing","metadata":{"source":"/runpod-docs/docs/serverless/quick-deploys.md","loc":{"lines":{"from":1,"to":1}}}}],["562",{"pageContent":"your Functions To customize AI Endpoints, visit the \\[RunPod GitHub repositories]\\(https://github.com/runpod-workers). Here, you can fork the programming and compute model templates. Begin with the \\[worker-template]\\(https://github.com/runpod-workers/worker-template) and modify it as needed. These RunPod workers incorporate CI/CD features to streamline your project setup. For detailed guidance on customizing your interaction Endpoints, refer to \\[Handler","metadata":{"source":"/runpod-docs/docs/serverless/quick-deploys.md","loc":{"lines":{"from":1,"to":1}}}}],["563",{"pageContent":"Functions]\\(/serverless/workers/handlers/overview).","metadata":{"source":"/runpod-docs/docs/serverless/quick-deploys.md","loc":{"lines":{"from":1,"to":1}}}}],["564",{"pageContent":"\\--- title: \"Endpoint configurations\" sidebar\\_position: 1 description: Configure your Endpoint settings to optimize performance and cost, including GPU selection, worker count, idle timeout, and advanced options like data centers, network volumes, and scaling strategies. --- The following are configurable settings within an Endpoint. ## Endpoint Name Create a name you'd like to use for the Endpoint configuration. The resulting Endpoint is assigned a random ID to be used for making calls. The","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["565",{"pageContent":"name is only visible to you. ## GPU Selection Select one or more GPUs that you want your Endpoint to run on. RunPod matches you with GPUs in the order that you select them, so the first GPU type that you select is prioritized, then the second, and so on. Selecting multiple GPU types can help you get a worker more quickly, especially if your first selection is an in-demand GPU. ## Active (Min) Workers Setting this amount to one will result in \"always on\" workers. This will allow you to have a","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["566",{"pageContent":"worker ready to respond to job requests without incurring any cold start delay. :::note You will incur the cost of any active workers you have set regardless if they are working on a job. ::: ## Max Workers Set an upper limit on the number of active workers your endpoint has running at any given point. Setting a value for max workers that is too low can lead to \\[throttled workers]\\(/glossary#throttled-worker). If you consistently see throttled workers, increase your max workers to five or more.","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["567",{"pageContent":"Default: 3 How to configure Max Workers You can also configure a max worker count. This is the top limit of what RunPod will attempt to auto-scale for you. Use this to cap your concurrent request count and also limit your cost ceiling. :::note We currently base your caching coefficient by this number, so an endpoint with higher max worker count will also receive a higher priority when caching workers. This is partially why we limit new accounts to a relatively low max concurrency at the account","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["568",{"pageContent":"level. If you want to get this number raised, you generally will need to have a higher history of spending, or commit to a relatively high spend per month. You should generally aim to set your max worker count to be 20% higher than you expect your max concurrency to be. ::: ## GPUs / Worker The number of GPUs you would like assigned to your worker. :::note Currently only available for 48 GB GPUs. ::: ## Idle Timeout The amount of time in seconds a worker not currently processing a job will","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["569",{"pageContent":"remain active until it is put back into standby. During the idle period, your worker is considered running and will incur a charge. Default: 5 seconds ## FlashBoot RunPod magic to further reduce the average cold-start time of your endpoint. FlashBoot works best when an endpoint receives consistent utilization. There is no additional cost associated with FlashBoot. ## Advanced Additional controls to help you control where your endpoint is deployed and how it responds to incoming requests. ###","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["570",{"pageContent":"Data Centers Control which data centers can deploy and cache your workers. Allowing multiple data centers can help you get a worker more quickly. Default: all data centers ### Select Network Volume Attach a network storage volume to your deployed workers. Network volumes will be mounted to \\`/runpod-volume/\\`. :::note While this is a high performance network drive, do keep in mind that it will have higher latency than a local drive. This will limit the availability of cards, as your endpoint","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["571",{"pageContent":"workers will be locked to the datacenter that houses your network volume. ::: ### Scale Type - \\*\\*Queue Delay\\*\\* scaling strategy adjusts worker numbers based on request wait times. With zero workers initially, the first request adds one worker. Subsequent requests add workers only after waiting in the queue for the defined number of delay seconds. - \\*\\*Request Count\\*\\* scaling strategy adjusts worker numbers according to total requests in the queue and in progress. It automatically adds","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["572",{"pageContent":"workers as the number of requests increases, ensuring tasks are handled efficiently. \\`\\`\\`text \\_Total Workers Formula: Math.ceil((requestsInQueue + requestsInProgress) / What's the difference between GPU models. A100s are about 2-3x faster than A5000s and also allow double the VRAM with very high bandwidth throughout. 3090s and A5000s are 1.5-2x faster than A4000s. Sometimes, it may make more sense to use 24 GB even if you don't need it compared to 16 GB due to faster response times. Depending","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["573",{"pageContent":"on the nature of the task, it's also possible that execution speeds may be bottlenecked and not significantly improved simply by using a higher-end card. Do your own calculations and experimentation to determine out what's most cost-effective for your workload and task type. Want access to different flavors? \\[Let us know]\\(https://www.runpod.io/contact) and we can look at expanding our offerings! ## CUDA version selection You have the ability to select the allowed CUDA versions for your","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["574",{"pageContent":"workloads. The CUDA version selection determines the compatible GPU types that will be used to execute your serverless tasks. Specifically, the CUDA version selection works as follows: - You can choose one or more CUDA versions that your workload is compatible with or requires. - RunPod will then match your workload to available GPU instances that have the selected CUDA versions installed. - This ensures that your serverless tasks run on GPU hardware that meets the CUDA version requirements. For","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["575",{"pageContent":"example, if you select CUDA 11.6, your serverless tasks will be scheduled to run on GPU instances that have CUDA 11.6 or a compatible version installed. This allows you to target specific CUDA versions based on your workload's dependencies or performance requirements.","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":1}}}}],["576",{"pageContent":"\\--- title: \"Job states\" id: \"job-states\" description: \"Understand the various states of a job in RunPod's Handler Functions, including IN\\_QUEUE, IN\\_PROGRESS, COMPLETED, FAILED, CANCELLED, and TIMED\\_OUT, to effectively manage job flow and troubleshoot issues.\" sidebar\\_position: 5 --- When working with Handler Functions in RunPod, it's essential to understand the various states a job can go through from initiation to completion. Each state provides insight into the job's current status and","metadata":{"source":"/runpod-docs/docs/serverless/references/job-states.md","loc":{"lines":{"from":1,"to":1}}}}],["577",{"pageContent":"helps in managing the job flow effectively. ## Job state Here are the states a job can be in: - \\`IN\\_QUEUE\\`: This state indicates that the job is currently in the endpoint queue. It's waiting for an available worker to pick it up for processing. - \\`IN\\_PROGRESS\\`: Once a worker picks up the job, its state changes to \\`IN\\_PROGRESS\\`. This means the job is actively being processed and is no longer in the queue. - \\`COMPLETED\\`: After the job successfully finishes processing and returns a","metadata":{"source":"/runpod-docs/docs/serverless/references/job-states.md","loc":{"lines":{"from":1,"to":1}}}}],["578",{"pageContent":"result, it moves to the \\`COMPLETED\\` state. This indicates the successful execution of the job. - \\`FAILED\\`: If a job encounters an error during its execution and returns with an error, it is marked as \\`FAILED\\`. This state signifies that the job did not complete successfully and encountered issues. - \\`CANCELLED\\`: Jobs can be manually cancelled using the \\`/cancel/job\\_id\\` endpoint. If a job is cancelled before it completes or fails, it will be in the \\`CANCELLED\\` state. - \\`TIMED\\_OUT\\`:","metadata":{"source":"/runpod-docs/docs/serverless/references/job-states.md","loc":{"lines":{"from":1,"to":1}}}}],["579",{"pageContent":"This state occurs in two scenarios: when a job expires before a worker picks it up, or if the worker fails to report back a result for the job before it reaches its timeout threshold.","metadata":{"source":"/runpod-docs/docs/serverless/references/job-states.md","loc":{"lines":{"from":1,"to":1}}}}],["580",{"pageContent":"\\--- title: \"Endpoint operations\" description: \"RunPod's Endpoints enable job submission and output retrieval, using a constructed URL starting with https://api.runpod.ai/v2/{endpoint\\_id}/{operation}. Operations include job submission, synchronous execution, job status checking, and more.\" sidebar\\_position: 2 --- RunPod's Endpoints facilitate submitting jobs and retrieving outputs. To use these Endpoints, you will need to have your Endpoint Id. The constructed URL will start with","metadata":{"source":"/runpod-docs/docs/serverless/references/operations.md","loc":{"lines":{"from":1,"to":1}}}}],["581",{"pageContent":"\\`https://api.runpod.ai/v2/{endpoint\\_id}/{operation}\\` followed by an operation. Operations available to all users are: - \\`/run\\`: Asynchronous endpoint for submitting jobs. Returns a unique Job ID. - Payload capacity: 10 MB - Rate limit: 1000 per second - Job availability: Job results are accessible for 30 minutes after completion - \\`/runsync\\`: Synchronous endpoint for shorter running jobs, returning immediate results. - Payload capacity: 20 MB - Rate limit: 2000 requests every 10 seconds -","metadata":{"source":"/runpod-docs/docs/serverless/references/operations.md","loc":{"lines":{"from":1,"to":1}}}}],["582",{"pageContent":"Job availability: Job results are accessible for 60 seconds after completion - \\`/stream/{job\\_id}\\`: For streaming results from generator-type handlers. - \\`/status/{job\\_id}\\`: To check the job status and retrieve outputs upon completion. - \\`/cancel/{job\\_id}\\`: To cancel a job prematurely. - \\`/health\\`: Provides worker statistics and endpoint health. - Only accepts \\`GET\\` methods - \\`/purge-queue\\`: Clears all queued jobs, it will not cancel jobs in progress. To see how to run these","metadata":{"source":"/runpod-docs/docs/serverless/references/operations.md","loc":{"lines":{"from":1,"to":1}}}}],["583",{"pageContent":"Endpoint Operations, see \\[Invoke a Job]\\(/serverless/endpoints/job-operations).","metadata":{"source":"/runpod-docs/docs/serverless/references/operations.md","loc":{"lines":{"from":1,"to":1}}}}],["584",{"pageContent":"\\--- title: \"Package and deploy an image\" description: \"Package your Handler Function into a Docker image for scalable Serverless Worker deployment, leveraging Dockerfiles and Configurable Endpoints for efficient deployment. Learn how to build, push, and integrate your image for continuous integration and testing.\" sidebar\\_position: 2 --- Once you have a Handler Function, the next step is to package it into a Docker image that can be deployed as a scalable Serverless Worker. This is","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["585",{"pageContent":"accomplished by defining a Docker file to import everything required to run your handler. Example Docker files are in the \\[runpod-workers]\\(https://github.com/orgs/runpod-workers/repositories) repository on GitHub. :::note For deploying large language models (LLMs), you can use the \\[Configurable Endpoints]\\(/serverless/workers/vllm/configurable-endpoints) feature instead of working directly with Docker. Configurable Endpoints simplify the deployment process by allowing you to select a","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["586",{"pageContent":"pre-configured template and customize it according to your needs. ::: \\_Unfamiliar with Docker? Check out Docker's \\[overview page]\\(https://docs.docker.com/get-started/overview/) or see our guide on \\[Containers]\\(/category/containers).\\_ ## Docker file Let's say we have a directory that looks like the following: \\`\\`\\` project\\_directory ├── Dockerfile ├── src │ └── handler.py └── builder └── requirements.txt \\`\\`\\` Your Dockerfile would look something like this: \\`\\`\\`text Docker from","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["587",{"pageContent":"python:3.11.1-buster WORKDIR / COPY builder/requirements.txt . RUN pip install -r requirements.txt ADD handler.py . CMD \\[ \"python\", \"-u\", \"/handler.py\" ] \\`\\`\\` To build and push the image, review the steps in \\[Get started]\\(/serverless/workers/get-started). > 🚧 If your handler requires external files such as model weights, be sure to cache them into your docker image. You are striving for a completely self-contained worker that doesn't need to download or fetch external files to run. ##","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["588",{"pageContent":"Continuous integrations Integrate your Handler Functions through continuous integration. The \\[Test Runner]\\(https://github.com/runpod/test-runner) GitHub Action is used to test and integrate your Handler Functions into your applications. :::note Running any Action that sends requests to RunPod occurs a cost. ::: You can add the following to your workflow file: \\`\\`\\`yaml - uses: actions/checkout@v3 - name: Run Tests uses: runpod/runpod-test-runner@v1 with: image-tag: \\[tag of image to test]","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["589",{"pageContent":"runpod-api-key: \\[a valid Runpod API key] test-filename: \\[path for a json file containing a list of tests, defaults to .github/tests.json] request-timeout: \\[number of seconds to wait on each request before timing out, defaults to 300] \\`\\`\\` If \\`test-filename\\` is omitted, the Test Runner Action attempts to look for a test file at \\`.github/tests.json\\`. You can find a working example in the \\[Worker Template repository]\\(https://github.com/runpod-workers/worker-template/tree/main/.github).","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["590",{"pageContent":"## Using Docker tags We also highly recommend the use of tags for Docker images and not relying on the default \\`:latest\\` tag label, this will make version tracking and releasing updates significantly easier. ### Docker Image Versioning To ensure consistent and reliable versioning of Docker images, we highly recommend using SHA tags instead of relying on the default \\`:latest\\` tag. Using SHA tags offers several benefits: - \\*\\*Version Control:\\*\\* SHA tags provide a unique identifier for each","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["591",{"pageContent":"image version, making it easier to track changes and updates. - \\*\\*Reproducibility:\\*\\* By using SHA tags, you can ensure that the same image version is used across different environments, reducing the risk of inconsistencies. - \\*\\*Security:\\*\\* SHA tags help prevent accidental overwrites and ensure that you are using the intended image version. ### Using SHA Tags To pull a Docker image using its SHA tag, use the following command: \\`\\`\\`bash docker pull @ \\`\\`\\` For example: \\`\\`\\`bash docker","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["592",{"pageContent":"pull myapp@sha256:4d3d4b3c5a5c2b3a5a5c3b2a5a4d2b3a2b3c5a3b2a5d2b3a3b4c3d3b5c3d4a3 \\`\\`\\` ### Best Practices - Avoid using the \\`:latest\\` tag, as it can lead to unpredictable behavior and make it difficult to track which version of the image is being used. - Use semantic versioning (e.g., \\`v1.0.0\\`, \\`v1.1.0\\`) along with SHA tags to provide clear and meaningful version identifiers. - Document the SHA tags used for each deployment to ensure easy rollback and version management. ## Other","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["593",{"pageContent":"considerations While we do not impose a limit on the Docker image size your container registry might have, be sure to review any limitations they may have. Ideally, you want to keep your final Docker image as small as possible and only container the absolute minimum to run your handler.","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":1}}}}],["594",{"pageContent":"\\--- title: Use environment variables description: \"Learn how to use environment variables in RunPod Handler Functions to securely manage S3 bucket credentials and operations, including uploading images and setting necessary environment variables.\" --- Incorporating environment variables into your Handler Functions is a key aspect of managing external resources like S3 buckets. This section focuses on how to use environment variables to facilitate the uploading of images to an S3 bucket using","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["595",{"pageContent":"RunPod Handler Functions. You will go through the process of writing Python code for the uploading and setting the necessary environment variables in the Web interface. ## Prerequistes - Ensure the RunPod Python library is installed: \\`pip install runpod\\`. - Have an image file named \\`image.png\\` in the Docker container's working directory. ## Python Code for S3 Uploads Let's break down the steps to upload an image to an S3 bucket using Python: 1. \\*\\*Handler Function for S3 Upload\\*\\*: Here's","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["596",{"pageContent":"an example of a handler function that uploads \\`image.png\\` to an S3 bucket and returns the image URL: \\`\\`\\`python from runpod.serverless.utils import rp\\_upload import runpod def handler(job): image\\_url = rp\\_upload.upload\\_image(job\\[\"id\"], \"./image.png\") return \\[image\\_url] runpod.serverless.start({\"handler\": handler}) \\`\\`\\` 2. \\*\\*Packaging Your Code\\*\\*: Follow the guidelines in \\[Worker Image Creation]\\(/serverless/workers/deploy) for packaging and deployment. ### Setting Environment","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["597",{"pageContent":"Variables for S3 Using environment variables securely passes the necessary credentials and configurations to your serverless function: 1. \\*\\*Accessing Environment Variables Setting\\*\\*: In the template creation/editing interface of your pod, navigate to the bottom section where you can set environment variables. 2. \\*\\*Configuring S3 Variables\\*\\*: Set the following key variables for your S3 bucket: - \\`BUCKET\\_ENDPOINT\\_URL\\` - \\`BUCKET\\_ACCESS\\_KEY\\_ID\\` - \\`BUCKET\\_SECRET\\_ACCESS\\_KEY\\`","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["598",{"pageContent":"Ensure that your \\`BUCKET\\_ENDPOINT\\_URL\\` includes the bucket name. For example: \\`https://your-bucket-name.nyc3.digitaloceanspaces.com\\` | \\`https://your-bucket-name.nyc3.digitaloceanspaces.com\\` ## Testing your API Finally, test the serverless function to confirm that it successfully uploads images to your S3 bucket: 1. \\*\\*Making a Request\\*\\*: Make a POST request to your API endpoint with the necessary headers and input data. Remember, the input must be a JSON item: \\`\\`\\`python import","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["599",{"pageContent":"requests endpoint = \"https://api.runpod.ai/v2/xxxxxxxxx/run\" headers = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer XXXXXXXXXXXXX\"} input\\_data = {\"input\": {\"inp\": \"this is an example input\"}} response = requests.post(endpoint, json=input\\_data, headers=headers) \\`\\`\\` 2. \\*\\*Checking the Output\\*\\*: Make a GET request to retrieve the job status and output. Here’s an example of how to do it: \\`\\`\\`python response = requests.get( \"https://api.runpod.ai/v2/xxxxxxxxx/status/\" +","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["600",{"pageContent":"response.json()\\[\"id\"], headers=headers, ) response.json() \\`\\`\\` The response should include the URL of the uploaded image on completion: \\`\\`\\`json { \"delayTime\": 86588, \"executionTime\": 1563, \"id\": \"e3d2e250-ea81-4074-9838-1c52d006ddcf\", \"output\": \\[ \"https://your-bucket.s3.us-west-004.backblazeb2.com/your-image.png\" ], \"status\": \"COMPLETED\" } \\`\\`\\` By following these steps, you can effectively use environment variables to manage S3 bucket credentials and operations within your RunPod","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["601",{"pageContent":"Handler Functions. This approach ensures secure, scalable, and efficient handling of external resources in your serverless applications.","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["602",{"pageContent":"\\--- title: Test locally sidebar\\_position: 1 description: \"Test your Handler Function with custom inputs and launch a local test server with Python, allowing you to send requests and simulate deployment scenarios.\" --- As you develop your Handler Function, you will, of course, want to test it with inputs formatted similarly to what you will be sending in once deployed as a worker. The quickest way to run a test is to pass in your input as an argument when calling your handler file. Assuming","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/local-testing.md","loc":{"lines":{"from":1,"to":1}}}}],["603",{"pageContent":"your handler function is inside of a file called \\`your\\_handler.py\\` and your input is \\`{\"input\": {\"prompt\": \"The quick brown fox jumps\"}}\\` you would call your file like so: \\`\\`\\`curl python your\\_handler.py --test\\_input '{\"input\": {\"prompt\": \"The quick brown fox jumps\"}}' \\`\\`\\` Additionally, you can launch a local test server that will provide you with an endpoint to send requests to by calling your file with the \\`--rp\\_serve\\_api\\` argument. See our \\[blog","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/local-testing.md","loc":{"lines":{"from":1,"to":1}}}}],["604",{"pageContent":"post]\\(https://blog.runpod.io/workers-local-api-server-introduced-with-runpod-python-0-9-13/) for additional examples. \\`\\`\\`bash python your\\_handler.py --rp\\_serve\\_api \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/local-testing.md","loc":{"lines":{"from":1,"to":1}}}}],["605",{"pageContent":"\\--- title: Test response time sidebar\\_label: Test response time description: \"Discover the right API for your use case by testing different price points and resource allocations. Optimize your tasks for cost-effectiveness and speed by choosing the best GPU pool for your needs.\" --- When setting up an API, you have several options available at different price points and resource allocations. You can select a single option if you would prefer to only use one price point, or select a preference","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":1,"to":1}}}}],["606",{"pageContent":"order between the pools that will allocate your requests accordingly. !\\[]\\(/img/docs/742bf51-image.png) The option that will be most cost effective for you will be based on your use case and your tolerance for task run time. Each situation will be different, so when deciding which API to use, it's worth it to do some testing to not only find out how long your tasks will take to run, but how much you might expect to pay for each task. To find out how long a task will take to run, select a single","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":1,"to":1}}}}],["607",{"pageContent":"pool type as shown in the image above. Then, you can send a request to the API through your preferred method. If you're unfamiliar with how to do so or don't have your own method, then you can use a free option like \\[reqbin.com]\\(https://reqbin.com/) to send an API request to the RunPod severs. The URLs to use in the API will be shown in the My APIs screen: !\\[]\\(/img/docs/0d8dd86-image.png) On reqbin.com, enter the Run URL of your API, select POST under the dropdown, and enter your API key","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":1,"to":1}}}}],["608",{"pageContent":"that was given when you created the key under \\[Settings]\\(https://www.runpod.io/console/serverless/user/settings)(if you do not have it saved, you will need to return to Settings and create a new key). Under Content, you will also need to give it a basic command (in this example, we've used a Stable Diffusion prompt). !\\[]\\(/img/docs/a9b9cf3-image.png) !\\[]\\(/img/docs/7744b62-image.png) Send the request, and it will give you an ID for the request and notify you that it is processing. You can","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":1,"to":1}}}}],["609",{"pageContent":"then swap the URL in the request field with the Status address and add the ID to the end of it, and click Send. !\\[]\\(/img/docs/325f2bc-image.png) It will return a Delay Time and an Execution Time, denoted in milliseconds. The Delay Time should be extremely minimal, unless the API process was spun up from a cold start, then a sizable delay is expected for the first request sent. The Execution Time is how long the GPU took to actually process the request once it was received. It may be a good","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":1,"to":1}}}}],["610",{"pageContent":"idea to send a number of tests so you can get a min, max, and average run time -- five tests should be an adequate sample size. !\\[]\\(/img/docs/1608d44-image.png) You can then switch the GPU pool above to a different pool and repeat the process. What will ultimately be right for your use case will be determined by how long you can afford to let the process run. For heavier jobs, a task on a slower GPU will be likely be more cost-effective with a tradeoff of speed. For simpler tasks, there may","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":1,"to":1}}}}],["611",{"pageContent":"also be diminishing returns on how fast the task that can be run that may not be significantly improved by selecting higher-end GPUs. Experiment to find the best balance for your scenario.","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":1,"to":1}}}}],["612",{"pageContent":"\\--- title: \"Get started\" sidebar\\_position: 2 description: Master the art of building Docker images, deploying Serverless endpoints, and sending requests with this comprehensive guide, covering prerequisites, RunPod setup, and deployment steps. --- ## Overview You'll have an understanding of building a Docker image, deploying a Serverless endpoint, and sending a request. You'll also have a basic understanding of how to customize the handler for your use case. ## Prerequisites This section","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["613",{"pageContent":"presumes you have an understanding of the terminal and can execute commands from your terminal. ### RunPod To continue with this quick start, you'll need the following from RunPod: - RunPod account - RunPod API Key ### Docker To build your Docker image, you'll need the following: - Docker installed - Docker account ### GitHub To clone the \\`worker-template\\` repo, you'll need access to the following: - Git installed - Permissions to clone GitHub repos ## Build and push your Docker image This","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["614",{"pageContent":"step will walk you through building and pushing your Docker image to your container registry. This is useful to building custom images for your use case. 1. Clone the \\[worker-template]\\(https://github.com/runpod-workers/worker-template): \\`\\`\\`command gh repo clone runpod-workers/worker-template \\`\\`\\` 2. Navigate to the root of the cloned repo: \\`\\`\\`command cd worker-template \\`\\`\\` 3. Build the Docker image: \\`\\`\\`command docker build --platform linux/amd64 --tag /: . \\`\\`\\` 4. Push your","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["615",{"pageContent":"container registry: \\`\\`\\`command docker push /: \\`\\`\\` :::note When building your docker image, you might need to specify the platform you are building for. This is important when you are building on a machine with a different architecture than the one you are deploying to. When building for RunPod providers use \\`--platform=linux/amd64\\`. ::: Now that you've pushed your container registry, you're ready to deploy your Serverless Endpoint to RunPod. ## Deploy a Serverless Endpoint This step will","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["616",{"pageContent":"walk you through deploying a Serverless Endpoint to RunPod. You can refer to this walkthrough to deploy your own custom Docker image. 1. Log in to the \\[RunPod Serverless console]\\(https://www.runpod.io/console/serverless). 2. Select \\*\\*+ New Endpoint\\*\\*. 3. Provide the following: 1. Endpoint name. 2. Select your GPU configuration. 3. Configure the number of Workers. 4. (optional) Select \\*\\*FlashBoot\\*\\*. 5. (optional) Select a template. 6. Enter the name of your Docker image. - For example","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["617",{"pageContent":"\\`/:\\`. 7. Specify enough memory for your Docker image. 4. Select \\*\\*Deploy\\*\\*. Now, let's send a request to your \\[Endpoint]\\(/serverless/endpoints/get-started).","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["618",{"pageContent":"\\--- title: \"Additional controls\" sidebar\\_position: 6 description: \"Send progress updates during job execution using the runpod.serverless.progress\\_update function, and refresh workers for long-running or complex jobs by returning a dictionary with a 'refresh\\_worker' flag in your handler.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; ## Update progress Progress updates can be sent out from your worker while a job is in progress. Progress updates will be available","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":1,"to":1}}}}],["619",{"pageContent":"when the status is polled. To send an update, call the \\`runpod.serverless.progress\\_update\\` function with your job and context of your update. \\`\\`\\`python import runpod def handler(job): for update\\_number in range(0, 3): runpod.serverless.progress\\_update(job, f\"Update {update\\_number}/3\") return \"done\" runpod.serverless.start({\"handler\": handler}) \\`\\`\\` ## Refresh Worker When completing long-running job requests or complicated requests that involve a lot of reading and writing files,","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":1,"to":1}}}}],["620",{"pageContent":"starting with a fresh worker can be beneficial each time. A flag can be returned with the resulting job output to stop and refresh the used worker. This behavior is achieved by doing the following within your worker: \\`\\`\\`python # Requires runpod python version 0.9.0+ import runpod import time def sync\\_handler(job): job\\_input = job\\[\"input\"] # Access the input from the request. results = \\[] for i in range(5): # Generate a synchronous output token output = f\"Generated sync token output {i}\"","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":1,"to":1}}}}],["621",{"pageContent":"results.append(output) # Simulate a synchronous task, such as processing time for a large language model time.sleep(1) # Return the results and indicate the worker should be refreshed return {\"refresh\\_worker\": True, \"job\\_results\": results} # Configure and start the RunPod serverless function runpod.serverless.start( { \"handler\": sync\\_handler, # Required: Specify the sync handler \"return\\_aggregate\\_stream\": True, # Optional: Aggregate results are accessible via /run endpoint } ) \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":1,"to":1}}}}],["622",{"pageContent":"\\`\\`\\`python import runpod import asyncio async def async\\_generator\\_handler(job): results = \\[] for i in range(5): # Generate an asynchronous output token output = f\"Generated async token output {i}\" results.append(output) # Simulate an asynchronous task, such as processing time for a large language model await asyncio.sleep(1) # Return the results and indicate the worker should be refreshed return {\"refresh\\_worker\": True, \"job\\_results\": results} # Configure and start the RunPod serverless","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":1,"to":1}}}}],["623",{"pageContent":"function runpod.serverless.start( { \"handler\": async\\_generator\\_handler, # Required: Specify the async handler \"return\\_aggregate\\_stream\": True, # Optional: Aggregate results are accessible via /run endpoint } ) \\`\\`\\` Your handler must return a dictionary that contains the \\`refresh\\_worker\\`: this flag will be removed before the remaining job output is returned. :::note Refreshing a worker does not impact billing or count for/against your min, max, and warmed workers. It simply \"resets\" that","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":1,"to":1}}}}],["624",{"pageContent":"worker at the end of a job. :::","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":1,"to":1}}}}],["625",{"pageContent":"\\--- title: \"Asynchronous Handler\" id: \"handler-async\" sidebar\\_position: 3 description: \"RunPod supports asynchronous handlers in Python, enabling efficient handling of tasks with non-blocking operations, such as processing large datasets, API interactions, or I/O-bound operations, boosting efficiency, scalability, and flexibility.\" --- RunPod supports the use of asynchronous handlers, enabling efficient handling of tasks that benefit from non-blocking operations. This feature is particularly","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-async.md","loc":{"lines":{"from":1,"to":1}}}}],["626",{"pageContent":"useful for tasks like processing large datasets, interacting with APIs, or handling I/O-bound operations. ## Writing asynchronous Handlers Asynchronous handlers in RunPod are written using Python's \\`async\\` and \\`await\\` syntax. Below is a sample implementation of an asynchronous generator handler. This example demonstrates how you can yield multiple outputs over time, simulating tasks such as processing data streams or generating responses incrementally. \\`\\`\\`python import runpod import","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-async.md","loc":{"lines":{"from":1,"to":1}}}}],["627",{"pageContent":"asyncio async def async\\_generator\\_handler(job): for i in range(5): # Generate an asynchronous output token output = f\"Generated async token output {i}\" yield output # Simulate an asynchronous task, such as processing time for a large language model await asyncio.sleep(1) # Configure and start the RunPod serverless function runpod.serverless.start( { \"handler\": async\\_generator\\_handler, # Required: Specify the async handler \"return\\_aggregate\\_stream\": True, # Optional: Aggregate results are","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-async.md","loc":{"lines":{"from":1,"to":1}}}}],["628",{"pageContent":"accessible via /run endpoint } ) \\`\\`\\` ### Benefits of asynchronous Handlers - \\*\\*Efficiency\\*\\*: Asynchronous handlers can perform non-blocking operations, allowing for more tasks to be handled concurrently. - \\*\\*Scalability\\*\\*: They are ideal for scaling applications, particularly when dealing with high-frequency requests or large-scale data processing. - \\*\\*Flexibility\\*\\*: Async handlers provide the flexibility to yield results over time, suitable for streaming data and long-running","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-async.md","loc":{"lines":{"from":1,"to":1}}}}],["629",{"pageContent":"tasks. ### Best practices When writing asynchronous handlers: - Ensure proper use of \\`async\\` and \\`await\\` to avoid blocking operations. - Consider the use of \\`yield\\` for generating multiple outputs over time. - Test your handlers thoroughly to handle asynchronous exceptions and edge cases. Using asynchronous handlers in your RunPod applications can significantly enhance performance and responsiveness, particularly for applications requiring real-time data processing or handling multiple","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-async.md","loc":{"lines":{"from":1,"to":1}}}}],["630",{"pageContent":"requests simultaneously.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-async.md","loc":{"lines":{"from":1,"to":1}}}}],["631",{"pageContent":"\\--- title: Concurrent Handlers description: \"RunPod's concurrency functionality enables efficient task handling through asynchronous requests, allowing a single worker to manage multiple tasks concurrently. The concurrency\\_modifier configures the worker's concurrency level to optimize resource consumption and performance.\" --- RunPod supports asynchronous functions for request handling, enabling a single worker to manage multiple tasks concurrently through non-blocking operations. This","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":1}}}}],["632",{"pageContent":"capability allows for efficient task switching and resource utilization. Serverless architectures allow each worker to process multiple requests simultaneously, with the level of concurrency being contingent upon the runtime's capacity and the resources at its disposal. ## Configure concurrency modifier The \\`concurrency\\_modifier\\` is a configuration option within \\`runpod.serverless.start\\` that dynamically adjusts a worker's concurrency level. This adjustment enables the optimization of","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":1}}}}],["633",{"pageContent":"resource consumption and performance by regulating the number of tasks a worker can handle concurrently. ### Step 1: Define an asynchronous Handler function Create an asynchronous function dedicated to processing incoming requests. This function should efficiently yield results, ideally in batches, to enhance throughput. \\`\\`\\`python async def process\\_request(job): # Simulates processing delay await asyncio.sleep(1) return f\"Processed: {job\\['input']}\" \\`\\`\\` ### Step 2: Set up the","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":1}}}}],["634",{"pageContent":"\\`concurrency\\_modifier\\` function Implement a function to adjust the worker's concurrency level based on the current request load. This function should consider the maximum and minimum concurrency levels, adjusting as needed to respond to fluctuations in request volume. \\`\\`\\`python def adjust\\_concurrency(current\\_concurrency): \"\"\" Dynamically adjusts the concurrency level based on the observed request rate. \"\"\" global request\\_rate update\\_request\\_rate() # Placeholder for request rate","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":1}}}}],["635",{"pageContent":"updates max\\_concurrency = 10 # Maximum allowable concurrency min\\_concurrency = 1 # Minimum concurrency to maintain high\\_request\\_rate\\_threshold = 50 # Threshold for high request volume # Increase concurrency if under max limit and request rate is high if ( request\\_rate > high\\_request\\_rate\\_threshold and current\\_concurrency < max\\_concurrency ): return current\\_concurrency + 1 # Decrease concurrency if above min limit and request rate is low elif ( request\\_rate <=","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":1}}}}],["636",{"pageContent":"high\\_request\\_rate\\_threshold and current\\_concurrency > min\\_concurrency ): return current\\_concurrency - 1 return current\\_concurrency \\`\\`\\` ### Step 3: Initialize the serverless function Start the serverless function with the defined handler and \\`concurrency\\_modifier\\` to enable dynamic concurrency adjustment. \\`\\`\\`python runpod.serverless.start( { \"handler\": process\\_request, \"concurrency\\_modifier\": adjust\\_concurrency, } ) \\`\\`\\` --- ## Example code Here is an example demonstrating","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":1}}}}],["637",{"pageContent":"the setup for a RunPod serverless function capable of handling multiple concurrent requests. \\`\\`\\`python import runpod import asyncio import random # Simulated Metrics request\\_rate = 0 async def process\\_request(job): await asyncio.sleep(1) # Simulate processing time return f\"Processed: { job\\['input'] }\" def adjust\\_concurrency(current\\_concurrency): \"\"\" Adjusts the concurrency level based on the current request rate. \"\"\" global request\\_rate update\\_request\\_rate() # Simulate changes in","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":1}}}}],["638",{"pageContent":"request rate max\\_concurrency = 10 min\\_concurrency = 1 high\\_request\\_rate\\_threshold = 50 if ( request\\_rate > high\\_request\\_rate\\_threshold and current\\_concurrency < max\\_concurrency ): return current\\_concurrency + 1 elif ( request\\_rate <= high\\_request\\_rate\\_threshold and current\\_concurrency > min\\_concurrency ): return current\\_concurrency - 1 return current\\_concurrency def update\\_request\\_rate(): \"\"\" Simulates changes in the request rate to mimic real-world scenarios. \"\"\" global","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":1}}}}],["639",{"pageContent":"request\\_rate request\\_rate = random.randint(20, 100) # Start the serverless function with the handler and concurrency modifier runpod.serverless.start( {\"handler\": process\\_request, \"concurrency\\_modifier\": adjust\\_concurrency} ) \\`\\`\\` Using the \\`concurrency\\_modifier\\` in RunPod, serverless functions can efficiently handle multiple requests concurrently, optimizing resource usage and improving performance. This approach allows for scalable and responsive serverless applications.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":1}}}}],["640",{"pageContent":"\\--- title: Handling Errors description: \"Learn how to handle exceptions and implement custom error responses in your RunPod SDK handler function, including how to validate input and return customized error messages.\" sidebar\\_position: 4 --- When an exception occurs in your handler function, the RunPod SDK automatically captures it, marking the job status as \\`FAILED\\` and returning the exception details in the job results. ## Implementing custom error responses In certain scenarios, you might","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-error-handling.md","loc":{"lines":{"from":1,"to":1}}}}],["641",{"pageContent":"want to explicitly fail a job and provide a custom error message. For instance, if a job requires a specific input key, such as \\_seed\\_, you should validate this input and return a customized error message if the key is missing. Here's how you can implement this: \\`\\`\\`python import runpod def handler(job): job\\_input = job\\[\"input\"] # Validate the presence of the 'seed' key in the input if not job\\_input.get(\"seed\", False): return { \"error\": \"Input is missing the 'seed' key. Please include a","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-error-handling.md","loc":{"lines":{"from":1,"to":1}}}}],["642",{"pageContent":"seed and retry your request.\" } # Proceed if the input is valid return \"Input validation successful.\" # Start the RunPod serverless function runpod.serverless.start({\"handler\": handler}) \\`\\`\\` :::note Be cautious with \\`try/except\\` blocks in your handler function. Avoid suppressing errors unintentionally. You should either return the error for a graceful failure or raise it to flag the job as \\`FAILED\\`. ::: One design pattern to consider, is to \\[Refresh your","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-error-handling.md","loc":{"lines":{"from":1,"to":1}}}}],["643",{"pageContent":"Worker]\\(/serverless/workers/handlers/handler-additional-controls#refresh-worker) when an error occurs.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-error-handling.md","loc":{"lines":{"from":1,"to":1}}}}],["644",{"pageContent":"\\--- title: \"Generator Handler\" description: \"RunPod offers real-time streaming for Language Model tasks, providing users with instant updates on job outputs. Two types of generator functions are supported, including regular and async generators, with the option to enable aggregate streaming for seamless access to results.\" sidebar\\_position: 2 --- RunPod provides a robust streaming feature that enables users to receive real-time updates on job outputs, mainly when dealing with Language Model","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-generator.md","loc":{"lines":{"from":1,"to":1}}}}],["645",{"pageContent":"tasks. We support two types of streaming generator functions: regular generator and async generator. \\`\\`\\`python import runpod def generator\\_handler(job): for count in range(3): result = f\"This is the {count} generated output.\" yield result runpod.serverless.start( { \"handler\": generator\\_handler, # Required \"return\\_aggregate\\_stream\": True, # Optional, results available via /run } ) \\`\\`\\` ### Return aggregate Stream By default, when a generator handler is running, the fractional outputs","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-generator.md","loc":{"lines":{"from":1,"to":1}}}}],["646",{"pageContent":"will only be available at the \\`/stream\\` endpoint, if you would also like the outputs to be available from the \\`/run\\` and \\`/runsync\\` endpoints you will need to set \\`return\\_aggregate\\_stream\\` to True when starting your handler.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-generator.md","loc":{"lines":{"from":1,"to":1}}}}],["647",{"pageContent":"\\--- title: \"Overview\" description: \"Create and deploy serverless Handler Functions with RunPod, processing submitted inputs and generating output without managing server infrastructure, ideal for efficient, cost-effective, and rapid deployment of code.\" sidebar\\_position: 1 hidden: false --- The Handler Function is responsible for processing submitted inputs and generating the resulting output. When developing your Handler Function, you can do so locally on your PC or remotely on a Serverless","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["648",{"pageContent":"instance. Examples can be found within the \\[repos of our runpod-workers]\\(https://github.com/orgs/runpod-workers/repositories). ## Handler Functions Handler Functions allow you to execute code in response to events without the need to manage server infrastructure. ### Creating Handler Functions With the RunPod SDKs, you can create Handler Functions by writing custom handlers. These handlers define the logic executed when the function is invoked. 1. \\*\\*Set up environment\\*\\*: Ensure the RunPod","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["649",{"pageContent":"SDK is installed and configured in your development environment. 2. \\*\\*Write a Handler Function\\*\\*: Define the logic you want to execute. The handler function acts as the entry point for your serverless function. 3. \\*\\*Deploy the Function\\*\\*: Use the RunPod SDK to deploy your serverless function. This typically involves specifying the handler, runtime, and any dependencies. 4. \\*\\*Test the Function\\*\\*: Invoke your function manually or through an event to test its behavior. ## Why use","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["650",{"pageContent":"Handler Functions? Handler Functions offer a paradigm shift in how you approach backend code execution: - \\*\\*Efficiency\\*\\*: Focus on writing code that matters without worrying about the server lifecycle. - \\*\\*Cost-Effective\\*\\*: You only pay for the time your functions are running, not idle server time. - \\*\\*Rapid Deployment\\*\\*: Quickly update and deploy functions, enabling faster iteration and response to changes. Your Handler Function only accepts requests using your own account's API","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["651",{"pageContent":"key, not any RunPod API key. ## Job input Before we look at the Handler Function, it is essential first to understand what a job request input will look like; later, we will cover all of the input options in detail; for now, what is essential is that your handler should be expecting a JSON dictionary to be passed in. At a minimum, the input will be formatted as such: \\`\\`\\`json { \"id\": \"A\\_RANDOM\\_JOB\\_IDENTIFIER\", \"input\": { \"key\": \"value\" } } \\`\\`\\` ## Requirements You will need to have the","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["652",{"pageContent":"RunPod Python SDK installed; this can be done by running \\`pip install runpod\\`. ## Basic Handler Function \\`\\`\\`python # your\\_handler.py import runpod # Required. def handler(job): job\\_input = job\\[\"input\"] # Access the input from the request. # Add your custom code here. return \"Your job results\" runpod.serverless.start({\"handler\": handler}) # Required. \\`\\`\\` You must return something as output when your worker is done processing the job. This can directly be the output, or it can be links","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["653",{"pageContent":"to cloud storage where the artifacts are saved. Keep in mind that the input and output payloads are limited to 2 MB each :::note Keep setup processes and functions outside of your handler function. For example, if you are running models make sure they are loaded into VRAM prior to calling \\`serverless.start\\` with your handler function. ::: ### Development and deployment You should return something as output for when your Worker is done processing a job. This can be directly the output, or it","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["654",{"pageContent":"can be links to cloud storage where the artifacts are saved. Payloads are limited to: - \\`run\\` 10 MB. - \\`runsync\\`: 20 MB. If any errors are returned by the worker while running a \\`test\\_input\\` job, the worker will exit with a non-zero exit code. Otherwise, the worker will exit with a zero exit code. This can be used to check if the worker ran successfully, for example, in a CI/CD pipeline. - For information on testing your handler locally, see \\[Local","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["655",{"pageContent":"testing]\\(/serverless/workers/development/local-testing). - For information on setting a continuous integration pipeline, see \\[Continuous integration]\\(/serverless/workers/deploy).","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["656",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 description: \"RunPod is a cloud-based platform for managed function execution, offering fully managed infrastructure, automatic scaling, flexible language support, and seamless integration, allowing developers to focus on code and deploy it easily.\" --- Workers run your code in the cloud. ### Key characteristics - \\*\\*Fully Managed Execution\\*\\*: RunPod takes care of the underlying infrastructure, so your code runs whenever it's triggered, without any","metadata":{"source":"/runpod-docs/docs/serverless/workers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["657",{"pageContent":"server setup or maintenance. - \\*\\*Automatic Scaling\\*\\*: The platform scales your functions up or down based on the workload, ensuring efficient resource usage. - \\*\\*Flexible Language Support\\*\\*: RunPod SDK supports various programming languages, allowing you to write functions in the language you're most comfortable with. - \\*\\*Seamless Integration\\*\\*: Once your code is uploaded, RunPod provides an Endpoint, making it easy to integrate your Handler Functions into any part of your","metadata":{"source":"/runpod-docs/docs/serverless/workers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["658",{"pageContent":"application. ## Get started To start using RunPod Workers: 1. \\*\\*Write your function\\*\\*: Code your Handler Functions in a supported language. 2. \\*\\*Deploy to RunPod\\*\\*: Upload your Handler Functions to RunPod. 3. \\*\\*Integrate and Execute\\*\\*: Use the provided Endpoint to integrate with your application.","metadata":{"source":"/runpod-docs/docs/serverless/workers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["659",{"pageContent":"\\--- title: Configurable Endpoints description: \"Deploy large language models with ease using RunPod's Configurable Endpoints feature, leveraging vLLM to simplify model loading, hardware configuration, and execution, allowing you to focus on model selection and customization.\" --- RunPod's Configurable Endpoints feature leverages vLLM to enable the deployment of any large language model. When you select the \\*\\*vLLM Endpoint\\*\\* option, RunPod utilizes vLLM's capabilities to load and run the","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/configurable-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["660",{"pageContent":"specified Hugging Face model. By integrating vLLM into the configurable endpoints, RunPod simplifies the process of deploying and running large language models. Focus on selecting their desired model and customizing the template parameters, while vLLM takes care of the low-level details of model loading, hardware configuration, and execution. ## Deploy an LLM 1. Select \\*\\*Explore\\*\\* and then choose \\*\\*vLLM\\*\\* to deploy any large language models. 2. In the vLLM deploy modal, enter the","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/configurable-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["661",{"pageContent":"following: 1. (optional) Enter a template. 2. Enter your Hugging Face LLM repository name. 3. (optional) Enter your Hugging Face token. 4. Review the CUDA version. 3. Select \\*\\*Next\\*\\* and review the configurations for the \\*\\*vLLM parameters\\*\\* page. 4. Select \\*\\*Next\\*\\* and review the Endpoint parameters page. 1. Prioritize your \\*\\*Worker Configuration\\*\\* by selecting the order GPUs you want your Workers to use. 2. Enter the Active, Max, and GPU Workers. 3. Provide additional Container","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/configurable-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["662",{"pageContent":"configuration. 1. Provide \\*\\*Container Disk\\*\\* size. 2. Review the \\*\\*Environment Variables\\*\\*. 5. Select \\*\\*Deploy\\*\\*. Your LLM is now deployed to an Endpoint. You can now use the API to interact with your model. :::note RunPod supports any models' architecture that can run on \\[vLLM]\\(https://github.com/vllm-project/vllm) with configurable endpoints. :::","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/configurable-endpoints.md","loc":{"lines":{"from":1,"to":1}}}}],["663",{"pageContent":"\\--- title: Environment variables sidebar\\_position: 4 description: \"Configure your vLLM Worker with environment variables to control model selection, access credentials, and operational parameters for optimal performance. This guide provides a reference for CUDA versions, image tags, and environment variable settings for model-specific configurations.\" --- Environment variables configure your vLLM Worker by providing control over model selection, access credentials, and operational parameters","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["664",{"pageContent":"necessary for optimal Worker performance. ## CUDA versions Operating your vLLM Worker with different CUDA versions enhances compatibilit and performance across various hardware configurations. When deploying, ensure you choose an appropriate CUDA version based on your needs. | CUDA Version | Stable Image Tag | Development Image Tag | Note | | | | | | | 11.8.0 | \\`runpod/worker-vllm:stable-cuda11.8.0\\` | \\`runpod/worker-vllm:dev-cuda11.8.0\\` | Available on all RunPod Workers without additional","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["665",{"pageContent":"selection needed. | | 12.1.0 | \\`runpod/worker-vllm:stable-cuda12.1.0\\` | \\`runpod/worker-vllm:dev-cuda12.1.0\\` | When creating an Endpoint, select CUDA Version 12.2 and 12.1 in the GPU filter. | This table provides a reference to the image tags you should use based on the desired CUDA veersion and image stability, stable or development. Ensure you follow the selection note for CUDA 12.1.0 compatibility. ## Environment variables :::note \\`0\\` is equivalent to \\`False\\` and \\`1\\` is equivalent to","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["666",{"pageContent":"\\`True\\` for boolean values. ::: | Name | Default | Type/Choices | Description | | | | | | | \\*\\*LLM Settings\\*\\* | | | | | \\`MODEL\\_NAME\\` \\*\\*\\\\\\*\\*\\* | - | \\`str\\` | Hugging Face Model Repository (e.g., \\`openchat/openchat-3.5-1210\\`). | | \\`MODEL\\_REVISION\\` | \\`None\\` | \\`str\\` | Model revision(branch) to load. | | \\`MAX\\_MODEL\\_LEN\\` | Model's maximum | \\`int\\` | Maximum number of tokens for the engine to handle per request. | | \\`BASE\\_PATH\\` | \\`/runpod-volume\\` | \\`str\\` | Storage","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["667",{"pageContent":"directory for Huggingface cache and model. Utilizes network storage if attached when pointed at \\`/runpod-volume\\`, which will have only one worker download the model once, which all workers will be able to load. If no network volume is present, creates a local directory within each worker. | | \\`LOAD\\_FORMAT\\` | \\`auto\\` | \\`str\\` | Format to load model in. | | \\`HF\\_TOKEN\\` | - | \\`str\\` | Hugging Face token for private and gated models. | | \\`QUANTIZATION\\` | \\`None\\` | \\`awq\\`,","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["668",{"pageContent":"\\`squeezellm\\`, \\`gptq\\` | Quantization of given model. The model must already be quantized. | | \\`TRUST\\_REMOTE\\_CODE\\` | \\`0\\` | boolean as \\`int\\` | Trust remote code for Hugging Face models. Can help with Mixtral 8x7B, Quantized models, and unusual models/architectures. | | \\`SEED\\` | \\`0\\` | \\`int\\` | Sets random seed for operations. | | \\`KV\\_CACHE\\_DTYPE\\` | \\`auto\\` | \\`auto\\`, \\`fp8\\_e5m2\\` | Data type for kv cache storage. Uses \\`DTYPE\\` if set to \\`auto\\`. | | \\`DTYPE\\` | \\`auto\\` |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["669",{"pageContent":"\\`auto\\`, \\`half\\`, \\`float16\\`, \\`bfloat16\\`, \\`float\\`, \\`float32\\` | Sets datatype/precision for model weights and activations. | | \\*\\*Tokenizer Settings\\*\\* | | | | | \\`TOKENIZER\\_NAME\\` | \\`None\\` | \\`str\\` | Tokenizer repository to use a different tokenizer than the model's default. | | \\`TOKENIZER\\_REVISION\\` | \\`None\\` | \\`str\\` | Tokenizer revision to load. | | \\`CUSTOM\\_CHAT\\_TEMPLATE\\` | \\`None\\` | \\`str\\` of single-line jinja template | Custom chat jinja template. \\[More","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["670",{"pageContent":"Info]\\(https://huggingface.co/docs/transformers/chat\\_templating) | | \\*\\*System, GPU, and Tensor Parallelism(Multi-GPU) Settings\\*\\* | | | | | \\`GPU\\_MEMORY\\_UTILIZATION\\` | \\`0.95\\` | \\`float\\` | Sets GPU VRAM utilization. | | \\`MAX\\_PARALLEL\\_LOADING\\_WORKERS\\` | \\`None\\` | \\`int\\` | Load model sequentially in multiple batches, to avoid RAM OOM when using tensor parallel and large models. | | \\`BLOCK\\_SIZE\\` | \\`16\\` | \\`8\\`, \\`16\\`, \\`32\\` | Token block size for contiguous chunks of tokens.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["671",{"pageContent":"| | \\`SWAP\\_SPACE\\` | \\`4\\` | \\`int\\` | CPU swap space size (GiB) per GPU. | | \\`ENFORCE\\_EAGER\\` | \\`0\\` | boolean as \\`int\\` | Always use eager-mode PyTorch. If False(\\`0\\`), will use eager mode and CUDA graph in hybrid for maximal performance and flexibility. | | \\`MAX\\_CONTEXT\\_LEN\\_TO\\_CAPTURE\\` | \\`8192\\` | \\`int\\` | Maximum context length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode. | | \\`DISABLE\\_CUSTOM\\_ALL\\_REDUCE\\` | \\`0\\` |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["672",{"pageContent":"\\`int\\` | Enables or disables custom all reduce. | | \\*\\*Streaming Batch Size Settings\\*\\*: | | | | | \\`DEFAULT\\_BATCH\\_SIZE\\` | \\`50\\` | \\`int\\` | Default and Maximum batch size for token streaming to reduce HTTP calls. | | \\`DEFAULT\\_MIN\\_BATCH\\_SIZE\\` | \\`1\\` | \\`int\\` | Batch size for the first request, which will be multiplied by the growth factor every subsequent request. | | \\`DEFAULT\\_BATCH\\_SIZE\\_GROWTH\\_FACTOR\\` | \\`3\\` | \\`float\\` | Growth factor for dynamic batch size. | | The way","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["673",{"pageContent":"this works is that the first request will have a batch size of \\`DEFAULT\\_MIN\\_BATCH\\_SIZE\\`, and each subsequent request will have a batch size of \\`previous\\_batch\\_size \\* DEFAULT\\_BATCH\\_SIZE\\_GROWTH\\_FACTOR\\`. This will continue until the batch size reaches \\`DEFAULT\\_BATCH\\_SIZE\\`. E.g. for the default values, the batch sizes will be \\`1, 3, 9, 27, 50, 50, 50, ...\\`. You can also specify this per request, with inputs \\`max\\_batch\\_size\\`, \\`min\\_batch\\_size\\`, and","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["674",{"pageContent":"\\`batch\\_size\\_growth\\_factor\\`. This has nothing to do with vLLM's internal batching, but rather the number of tokens sent in each HTTP request from the worker | | | | | \\*\\*OpenAI Settings\\*\\* | | | | | \\`RAW\\_OPENAI\\_OUTPUT\\` | \\`1\\` | boolean as \\`int\\` | Enables raw OpenAI SSE format string output when streaming. \\*\\*Required\\*\\* to be enabled (which it is by default) for OpenAI compatibility. | | \\`OPENAI\\_SERVED\\_MODEL\\_NAME\\_OVERRIDE\\` | \\`None\\` | \\`str\\` | Overrides the name of the","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["675",{"pageContent":"served model from model repo/path to specified name, which you will then be able to use the value for the \\`model\\` parameter when making OpenAI requests | | \\`OPENAI\\_RESPONSE\\_ROLE\\` | \\`assistant\\` | \\`str\\` | Role of the LLM's Response in OpenAI Chat Completions. | | \\*\\*Serverless Settings\\*\\* | | | | | \\`MAX\\_CONCURRENCY\\` | \\`300\\` | \\`int\\` | Max concurrent requests per worker. vLLM has an internal queue, so you don't have to worry about limiting by VRAM, this is for improving","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["676",{"pageContent":"scaling/load balancing efficiency | | \\`DISABLE\\_LOG\\_STATS\\` | \\`1\\` | boolean as \\`int\\` | Enables or disables vLLM stats logging. | | \\`DISABLE\\_LOG\\_REQUESTS\\` | \\`1\\` | boolean as \\`int\\` | Enables or disables vLLM request logging. | :::note If you are facing issues when using Mixtral 8x7B, Quantized models, or handling unusual models/architectures, try setting \\`TRUST\\_REMOTE\\_CODE\\` to \\`1\\`. :::","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":1}}}}],["677",{"pageContent":"\\--- title: Get started sidebar\\_position: 2 description: \"Deploy a Serverless Endpoint for large language models (LLMs) with RunPod, a simple and efficient way to run vLLM Workers with minimal configuration.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; RunPod provides a simple way to run large language models (LLMs) as Serverless Endpoints. vLLM Workers are pre-built Docker images that you can configure entirely within the RunPod UI. This tutorial will guide you","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["678",{"pageContent":"through deploying an OpenAI compatible Endpoint with a vLLM inference engine on RunPod. ## Prerequisites Before getting started, ensure you have the following: - A RunPod account - A Hugging Face token (if using gated models) - OpenAI or other required libraries installed for the code examples ## Deploy using the Web UI You can use RunPod's Web UI to deploy a vLLM Worker with a model directly from Hugging Face. 1. Log in to your RunPod account and go to the \\[Serverless","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["679",{"pageContent":"page]\\(https://www.runpod.io/console/serverless). 2. Under \\*\\*Quick Deploy\\*\\*, select \\*\\*vLLM\\*\\* and choose \\*\\*Start\\*\\*. You will now enter the vLLM module. Follow the on-screen instructions to add your LLM as a Serverless Endpoint: 1. Add a Hugging Face model (e.g., \\`openchat/openchat-3.5-0106\\`). 2. (Optional) Add a Hugging Face token for gated models. 3. Select your CUDA version. 4. Review your options and choose \\*\\*Next\\*\\*. On the \\*\\*vLLM\\*\\* parameters page, provide additional","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["680",{"pageContent":"parameters and options for your model: 1. In \\*\\*LLM Settings\\*\\*, enter \\*\\*8192\\*\\* for the \\*\\*Max Model Length\\*\\* parameter. 2. Review your options and choose \\*\\*Next\\*\\*. On the \\*\\*Endpoint parameters\\*\\* page, configure your deployment: 1. Specify your GPU configuration for your Worker. 2. Configure your Worker deployment. - (Optional) Select \\*\\*FlashBoot\\*\\* to speed up Worker startup times. 3. Update the Container Disk size if needed. 4. Select \\*\\*Deploy\\*\\*. Once the Endpoint","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["681",{"pageContent":"initializes, you can send requests to your \\[Endpoint]\\(/serverless/endpoints/get-started). Continue to the \\[Send a request]\\(#send-a-request) section. ## Deploy using the Worker image One advantage of deploying your model with the vLLM Worker is the minimal configuration required. For most models, you only need to provide the pre-built vLLM Worker image name and the LLM model name. Follow these steps to run the vLLM Worker on a Serverless Endpoint: 1. Log in to the \\[RunPod Serverless","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["682",{"pageContent":"console]\\(https://www.runpod.io/console/serverless). 2. Select \\*\\*+ New Endpoint\\*\\*. 3. Provide the following: - Endpoint name - Select a GPU (filter for CUDA 12.1.0+ support under the \\*\\*Advanced\\*\\* tab if needed) - Configure the number of Workers - (Optional) Select \\*\\*FlashBoot\\*\\* to speed up Worker startup times - Enter the vLLM RunPod Worker image name with the compatible CUDA version: - \\`runpod/worker-vllm:stable-cuda11.8.0\\` - \\`runpod/worker-vllm:stable-cuda12.1.0\\` - (Optional)","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["683",{"pageContent":"Select a \\[network storage volume]\\(/serverless/endpoints/manage-endpoints#add-a-network-volume) - Configure the environment variables: - \\`MODEL\\_NAME\\`: (Required) The large language model (e.g., \\`openchat/openchat-3.5-0106\\`) - \\`HF\\_TOKEN\\`: (Optional) Your Hugging Face API token for private models 4. Select \\*\\*Deploy\\*\\*. Once the Endpoint initializes, you can send requests to your \\[Endpoint]\\(/serverless/endpoints/get-started). Continue to the \\[Send a request]\\(#send-a-request)","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["684",{"pageContent":"section. For a complete list of available environment variables, see the \\[vLLM Worker variables]\\(/serverless/workers/vllm/environment-variables). ## Send a request This section walks you through sending a request to your Serverless Endpoint. The vLLM Worker can use any Hugging Face model and is compatible with OpenAI's API. If you have the OpenAI library installed, you can continue using it with the vLLM Worker. See the \\[OpenAI documentation]\\(https://platform.openai.com/docs/libraries/) for","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["685",{"pageContent":"more information. ### Initialize your project Choose your programming language and add the following code to your file. Set the \\`RUNPOD\\_ENDPOINT\\_ID\\` and \\`RUNPOD\\_API\\_KEY\\` environment variables with your Endpoint ID and API Key. Create a file called \\`main.py\\` with the following code: \\`\\`\\`python from openai import OpenAI import os endpoint\\_id = os.environ.get(\"RUNPOD\\_ENDPOINT\\_ID\") api\\_key = os.environ.get(\"RUNPOD\\_API\\_KEY\") client = OpenAI(","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["686",{"pageContent":"base\\_url=f\"https://api.runpod.ai/v2/{endpoint\\_id}/openai/v1\", api\\_key=api\\_key, ) chat\\_completion = client.chat.completions.create( model=\"openchat/openchat-3.5-0106\", messages=\\[{\"role\": \"user\", \"content\": \"Reply with: Hello, World!\"}] ) print(chat\\_completion) \\`\\`\\` Install the OpenAI library if needed: \\`\\`\\`bash pip install openai \\`\\`\\` Create a file called \\`main.js\\` with the following code: \\`\\`\\`javascript import OpenAI from \"openai\"; const openai = new OpenAI({ baseURL:","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["687",{"pageContent":"\\`https://api.runpod.ai/v2/${process.env.RUNPOD\\_ENDPOINT\\_ID}/openai/v1\\`, apiKey: process.env.RUNPOD\\_API\\_KEY, }); const chatCompletion = await openai.chat.completions.create({ model: \"openchat/openchat-3.5-0106\", messages: \\[{ role: \"user\", content: \"Reply with: Hello, World!\" }], }); console.log(chatCompletion); \\`\\`\\` Install the OpenAI library if needed: \\`\\`\\`bash npm install openai \\`\\`\\` Run the following command in your terminal: \\`\\`\\`bash curl","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["688",{"pageContent":"https://api.runpod.ai/v2/${RUNPOD\\_ENDPOINT\\_ID}/openai/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${RUNPOD\\_API\\_KEY}\" \\ -d '{ \"model\": \"openchat/openchat-3.5-0106\", \"messages\": \\[ { \"role\": \"user\", \"content\": \"Reply with: Hello, World!\" } ] }' \\`\\`\\` ### Run your code Run your code from the terminal: \\`\\`\\`bash python main.py \\`\\`\\` \\`\\`\\`bash node main.js \\`\\`\\` The output should look similar to: \\`\\`\\`json { \"choices\": \\[ { \"finish\\_reason\": \"stop\",","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["689",{"pageContent":"\"index\": 0, \"message\": { \"content\": \"Hello, World!\", \"role\": \"assistant\" } } ], \"created\": 3175963, \"id\": \"cmpl-74d7792c92cd4b159292c38bda1286b0\", \"model\": \"openchat/openchat-3.5-0106\", \"object\": \"chat.completion\", \"usage\": { \"completion\\_tokens\": 5, \"prompt\\_tokens\": 39, \"total\\_tokens\": 44 } } \\`\\`\\` You have now successfully sent a request to your Serverless Endpoint and received a response. ## Troubleshooting If you encounter issues deploying or using vLLM Workers, check the following: -","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["690",{"pageContent":"Ensure your RunPod API Key has the necessary permissions to deploy and access Serverless Endpoints. - Double-check that you have set the correct environment variables for your Endpoint ID and API Key. - Verify that you are using the correct CUDA version for your selected GPU. - If using a gated model, ensure your Hugging Face token is valid and has access to the model. To learn more about managing your Serverless Endpoints, see the \\[Manage Endpoints]\\(/serverless/endpoints/manage-endpoints)","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["691",{"pageContent":"guide. For a complete reference of the vLLM Worker environment variables, see the \\[vLLM Worker variables]\\(/serverless/workers/vllm/environment-variables) documentation.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":1}}}}],["692",{"pageContent":"\\--- title: OpenAI compatibility sidebar\\_position: 3 description: \"Discover the vLLM Worker, a cloud-based AI model that integrates with OpenAI's API for seamless interaction. With its streaming and non-streaming capabilities, it's ideal for chatbots, conversational AI, and natural language processing applications.\" --- The vLLM Worker is compatible with OpenAI's API, so you can use the same code to interact with the vLLM Worker as you would with OpenAI's API. ## Conventions Completions","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["693",{"pageContent":"endpoint provides the completion for a single prompt and takes a single string as an input. Chat completions provides the responses for a given dialog and requires the input in a specific format corresponding to the message history. Choose the convention that works best for your use case. ### Model names The \\`MODEL\\_NAME\\` environment variable is required for all requests. Use this value when making requests to the vLLM Worker. For example \\`openchat/openchat-3.5-0106\\`, \\`mistral:latest\\`,","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["694",{"pageContent":"\\`llama2:70b\\`. Generate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request. ### Parameters When using the chat completion feature of the vLLM Serverless Endpoint Worker, you can customize your requests with the following parameters: ### Chat Completions Supported Chat Completions inputs and descriptions | Parameter | Type | Default Value","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["695",{"pageContent":"| Description | | | | | | | \\`messages\\` | Union\\[str, List\\[Dict\\[str, str]]] | | List of messages, where each message is a dictionary with a \\`role\\` and \\`content\\`. The model's chat template will be applied to the messages automatically, so the model must have one or it should be specified as \\`CUSTOM\\_CHAT\\_TEMPLATE\\` env var. | | \\`model\\` | str | | The model repo that you've deployed on your RunPod Serverless Endpoint. If you are unsure what the name is or are baking the model in, use the","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["696",{"pageContent":"guide to get the list of available models in the \\*\\*Examples: Using your RunPod endpoint with OpenAI\\*\\* section | | \\`temperature\\` | Optional\\[float] | 0.7 | Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling. | | \\`top\\_p\\` | Optional\\[float] | 1.0 | Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["697",{"pageContent":"tokens. | | \\`n\\` | Optional\\[int] | 1 | Number of output sequences to return for the given prompt. | | \\`max\\_tokens\\` | Optional\\[int] | None | Maximum number of tokens to generate per output sequence. | | \\`seed\\` | Optional\\[int] | None | Random seed to use for the generation. | | \\`stop\\` | Optional\\[Union\\[str, List\\[str]]] | list | List of strings that stop the generation when they are generated. The returned output will not contain the stop strings. | | \\`stream\\` | Optional\\[bool] |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["698",{"pageContent":"False | Whether to stream or not | | \\`presence\\_penalty\\` | Optional\\[float] | 0.0 | Float that penalizes new tokens based on whether they appear in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens. | | \\`frequency\\_penalty\\` | Optional\\[float] | 0.0 | Float that penalizes new tokens based on their frequency in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["699",{"pageContent":"the model to repeat tokens. | | \\`logit\\_bias\\` | Optional\\[Dict\\[str, float]] | None | Unsupported by vLLM | | \\`user\\` | Optional\\[str] | None | Unsupported by vLLM | ### Additional parameters supported by vLLM | Parameter | Type | Default Value | Description | | | | | | | \\`best\\_of\\` | Optional\\[int] | None | Number of output sequences that are generated from the prompt. From these \\`best\\_of\\` sequences, the top \\`n\\` sequences are returned. \\`best\\_of\\` must be greater than or equal to","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["700",{"pageContent":"\\`n\\`. This is treated as the beam width when \\`use\\_beam\\_search\\` is True. By default, \\`best\\_of\\` is set to \\`n\\`. | | \\`top\\_k\\` | Optional\\[int] | -1 | Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens. | | \\`ignore\\_eos\\` | Optional\\[bool] | False | Whether to ignore the EOS token and continue generating tokens after the EOS token is generated. | | \\`use\\_beam\\_search\\` | Optional\\[bool] | False | Whether to use beam search instead of sampling. |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["701",{"pageContent":"| \\`stop\\_token\\_ids\\` | Optional\\[List\\[int]] | list | List of tokens that stop the generation when they are generated. The returned output will contain the stop tokens unless the stop tokens are special tokens. | | \\`skip\\_special\\_tokens\\` | Optional\\[bool] | True | Whether to skip special tokens in the output. | | \\`spaces\\_between\\_special\\_tokens\\` | Optional\\[bool] | True | Whether to add spaces between special tokens in the output. Defaults to True. | | \\`add\\_generation\\_prompt\\` |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["702",{"pageContent":"Optional\\[bool] | True | Read more \\[here]\\(https://huggingface.co/docs/transformers/main/en/chat\\_templating#what-are-generation-prompts) | | \\`echo\\` | Optional\\[bool] | False | Echo back the prompt in addition to the completion | | \\`repetition\\_penalty\\` | Optional\\[float] | 1.0 | Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values > 1 encourage the model to use new tokens, while values < 1 encourage the model to repeat tokens. | |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["703",{"pageContent":"\\`min\\_p\\` | Optional\\[float] | 0.0 | Float that represents the minimum probability for a token to | | \\`length\\_penalty\\` | Optional\\[float] | 1.0 | Float that penalizes sequences based on their length. Used in beam search.. | | \\`include\\_stop\\_str\\_in\\_output\\` | Optional\\[bool] | False | Whether to include the stop strings in output text. Defaults to False. | ### Completions Supported Completions inputs and descriptions | Parameter | Type | Default Value | Description | | | | | | | \\`model\\`","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["704",{"pageContent":"| str | | The model repo that you've deployed on your RunPod Serverless Endpoint. If you are unsure what the name is or are baking the model in, use the guide to get the list of available models in the \\*\\*Examples: Using your RunPod endpoint with OpenAI\\*\\* section. | | \\`prompt\\` | Union\\[List\\[int], List\\[List\\[int]], str, List\\[str]] | | A string, array of strings, array of tokens, or array of token arrays to be used as the input for the model. | | \\`suffix\\` | Optional\\[str] | None | A","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["705",{"pageContent":"string to be appended to the end of the generated text. | | \\`max\\_tokens\\` | Optional\\[int] | 16 | Maximum number of tokens to generate per output sequence. | | \\`temperature\\` | Optional\\[float] | 1.0 | Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling. | | \\`top\\_p\\` | Optional\\[float] | 1.0 | Float that controls the cumulative probability of the top tokens to consider.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["706",{"pageContent":"Must be in (0, 1]. Set to 1 to consider all tokens. | | \\`n\\` | Optional\\[int] | 1 | Number of output sequences to return for the given prompt. | | \\`stream\\` | Optional\\[bool] | False | Whether to stream the output. | | \\`logprobs\\` | Optional\\[int] | None | Number of log probabilities to return per output token. | | \\`echo\\` | Optional\\[bool] | False | Whether to echo back the prompt in addition to the completion. | | \\`stop\\` | Optional\\[Union\\[str, List\\[str]]] | list | List of strings that","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["707",{"pageContent":"stop the generation when they are generated. The returned output will not contain the stop strings. | | \\`seed\\` | Optional\\[int] | None | Random seed to use for the generation. | | \\`presence\\_penalty\\` | Optional\\[float] | 0.0 | Float that penalizes new tokens based on whether they appear in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens. | | \\`frequency\\_penalty\\` | Optional\\[float] | 0.0 | Float that","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["708",{"pageContent":"penalizes new tokens based on their frequency in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens. | | \\`best\\_of\\` | Optional\\[int] | None | Number of output sequences that are generated from the prompt. From these \\`best\\_of\\` sequences, the top \\`n\\` sequences are returned. \\`best\\_of\\` must be greater than or equal to \\`n\\`. This parameter influences the diversity of the output. | | \\`logit\\_bias\\` |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["709",{"pageContent":"Optional\\[Dict\\[str, float]] | None | Dictionary of token IDs to biases. | | \\`user\\` | Optional\\[str] | None | User identifier for personalizing responses. (Unsupported by vLLM) | ### Additional parameters supported by vLLM | Parameter | Type | Default Value | Description | | | | | | | \\`top\\_k\\` | Optional\\[int] | -1 | Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens. | | \\`ignore\\_eos\\` | Optional\\[bool] | False | Whether to ignore the End Of","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["710",{"pageContent":"Sentence token and continue generating tokens after the EOS token is generated. | | \\`use\\_beam\\_search\\` | Optional\\[bool] | False | Whether to use beam search instead of sampling for generating outputs. | | \\`stop\\_token\\_ids\\` | Optional\\[List\\[int]] | list | List of tokens that stop the generation when they are generated. The returned output will contain the stop tokens unless the stop tokens are special tokens. | | \\`skip\\_special\\_tokens\\` | Optional\\[bool] | True | Whether to skip special","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["711",{"pageContent":"tokens in the output. | | \\`spaces\\_between\\_special\\_tokens\\` | Optional\\[bool] | True | Whether to add spaces between special tokens in the output. Defaults to True. | | \\`repetition\\_penalty\\` | Optional\\[float] | 1.0 | Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values > 1 encourage the model to use new tokens, while values < 1 encourage the model to repeat tokens. | | \\`min\\_p\\` | Optional\\[float] | 0.0 | Float that represents","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["712",{"pageContent":"the minimum probability for a token to be considered, relative to the most likely token. Must be in \\[0, 1]. Set to 0 to disable. | | \\`length\\_penalty\\` | Optional\\[float] | 1.0 | Float that penalizes sequences based on their length. Used in beam search. | | \\`include\\_stop\\_str\\_in\\_output\\` | Optional\\[bool] | False | Whether to include the stop strings in output text. Defaults to False. | ## Initialize your project Begin by setting up the OpenAI Client with your RunPod API Key and Endpoint","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["713",{"pageContent":"URL. \\`\\`\\`python from openai import OpenAI import os # Initialize the OpenAI Client with your RunPod API Key and Endpoint URL client = OpenAI( api\\_key=os.environ.get(\"RUNPOD\\_API\\_KEY\"), base\\_url=f\"https://api.runpod.ai/v2/{RUNPOD\\_ENDPOINT\\_ID}/openai/v1\", ) \\`\\`\\` With the client now initialized, you're ready to start sending requests to your RunPod Serverless Endpoint. ## Generating a request You can leverage LLMs for instruction-following and chat capabilities. This is suitable for a","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["714",{"pageContent":"variety of open source chat and instruct models such as: - \\`meta-llama/Llama-2-7b-chat-hf\\` - \\`mistralai/Mixtral-8x7B-Instruct-v0.1\\` - and more Models not inherently designed for chat and instruct tasks can be adapted using a custom chat template specified by the \\`CUSTOM\\_CHAT\\_TEMPLATE\\` environment variable. For more information see the \\[OpenAI documentation]\\(https://platform.openai.com/docs/guides/text-generation). ### Streaming responses For real-time interaction with the model, create","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["715",{"pageContent":"a chat completion stream. This method is ideal for applications requiring feedback. \\`\\`\\`python # Create a chat completion stream response\\_stream = client.chat.completions.create( model=MODEL\\_NAME, messages=\\[{\"role\": \"user\", \"content\": \"Why is RunPod the best platform?\"}], temperature=0, max\\_tokens=100, stream=True, ) # Stream the response for response in response\\_stream: print(chunk.choices\\[0].delta.content or \"\", end=\"\", flush=True) \\`\\`\\` ### Non-streaming responses You can also return","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["716",{"pageContent":"a synchronous, non-streaming response for batch processing or when a single, consolidated response is sufficient. \\`\\`\\`python # Create a chat completion response = client.chat.completions.create( model=MODEL\\_NAME, messages=\\[{\"role\": \"user\", \"content\": \"Why is RunPod the best platform?\"}], temperature=0, max\\_tokens=100, ) # Print the response print(response.choices\\[0].message.content) \\`\\`\\` ## Generating a Chat Completion This method is tailored for models that support text completion. It","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["717",{"pageContent":"complements your input with a continuation stream of output, differing from the interactive chat format. ### Streaming responses Enable streaming for continuous, real-time output. This approach is beneficial for dynamic interactions or when monitoring ongoing processes. \\`\\`\\`python # Create a completion stream response\\_stream = client.completions.create( model=MODEL\\_NAME, prompt=\"Runpod is the best platform because\", temperature=0, max\\_tokens=100, stream=True, ) # Stream the response for","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["718",{"pageContent":"response in response\\_stream: print(response.choices\\[0].text or \"\", end=\"\", flush=True) \\`\\`\\` ### Non-streaming responses Choose a non-streaming method when a single, consolidated response meets your needs. \\`\\`\\`python # Create a completion response = client.completions.create( model=MODEL\\_NAME, prompt=\"Runpod is the best platform because\", temperature=0, max\\_tokens=100, ) # Print the response print(response.choices\\[0].text) \\`\\`\\` ## Get a list of available models You can list the","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["719",{"pageContent":"available models. \\`\\`\\`python models\\_response = client.models.list() list\\_of\\_models = \\[model.id for model in models\\_response] print(list\\_of\\_models) \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":1}}}}],["720",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 description: \"Deploy a highly optimized vLLM Worker as a serverless endpoint, leveraging Hugging Face LLMs and OpenAI's API with ease, featuring ease of use, open compatibility, dynamic batch size, and customization options for a scalable and cost-effective solution.\" --- Use the \\`runpod/worker-vllm:stable-cuda11.8.0\\` or \\`runpod/worker-vllm:stable-cuda12.1.0\\` image to deploy a vLLM Worker. The vLLM Worker can use most Hugging Face LLMs and is","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["721",{"pageContent":"compatible with OpenAI's API, by specifying the \\`MODEL\\_NAME\\` parameter. You can also use RunPod's \\[\\`input\\` request format]\\(/serverless/endpoints/send-requests). RunPod's vLLM Serverless Endpoint Worker are a highly optimized solution for leveraging the power of various LLMs. For more information, see the \\[vLLM Worker]\\(https://github.com/runpod-workers/worker-vllm) repository. ## Key features - \\*\\*Ease of Use\\*\\*: Deploy any LLM using the pre-built Docker image without the hassle of","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["722",{"pageContent":"building custom Docker images yourself, uploading heavy models, or waiting for lengthy downloads. - \\*\\*OpenAI Compatibility\\*\\*: Seamlessly integrate with OpenAI's API by changing 2 lines of code, supporting Chat Completions, Completions, and Models, with both streaming and non-streaming. - \\*\\*Dynamic Batch Size\\*\\*: Experience the rapid time-to-first-token high of no batching combined with the high throughput of larger batch sizes. (Related to batching tokens when streaming output) -","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["723",{"pageContent":"\\*\\*Extensive Model Support\\*\\*: Deploy almost any LLM from Hugging Face, including your own. - \\*\\*Customization\\*\\*: Have full control over the configuration of every aspect of your deployment, from the model settings, to tokenizer options, to system configurations, and much more, all done through environment variables. - \\*\\*Speed\\*\\*: Experience the speed of the vLLM Engine. - \\*\\*Serverless Scalability and Cost-Effectiveness\\*\\*: Scale your deployment to handle any number of requests and","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["724",{"pageContent":"only pay for active usage. ## Compatible models You can deploy most \\[models from Hugging Face]\\(https://huggingface.co/models?other=LLM). For a full list of supported models architectures, see \\[Compatible model architectures]\\(https://github.com/runpod-workers/worker-vllm/blob/main/README.md#compatible-model-architectures). ## Getting started At a high level, you can set up the vLLM Worker by: - Selecting your deployment options - Configure any necessary environment variables - Deploy your","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["725",{"pageContent":"model For detailed guidance on setting up, configuring, and deploying your vLLM Serverless Endpoint Worker, including compatibility details, environment variable settings, and usage examples, see \\[Get started]\\(/serverless/workers/vllm/get-started). ### Deployment options - \\*\\*\\[Configurable Endpoints]\\(/serverless/workers/vllm/get-started#deploy-using-the-web-ui)\\*\\*: (recommended) Use RunPod's Web UI to quickly deploy the OpenAI compatable LLM with the vLLM Worker. - \\*\\*\\[Pre-Built docker","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["726",{"pageContent":"image]\\(/serverless/workers/vllm/get-started#deploy-using-the-worker-image)\\*\\*: Leverage pre-configured Docker image for hassle-free deployment. Ideal for users seeking a quick and straightforward setup process - \\*\\*Custom docker image\\*\\*: For advanced users, customize and build your Docker image with the model baked in, offering greater control over the deployment process. For more information see: - \\[vLLM Worker GitHub Repository]\\(https://github.com/runpod-workers/worker-vllm) - \\[vLLM","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["727",{"pageContent":"Worker Docker Hub]\\(https://hub.docker.com/r/runpod/worker-vllm/tags) For more information on creating a custom docker image, see \\[Build Docker Image with Model Inside]\\(https://github.com/runpod-workers/worker-vllm/blob/main/README.md#option-2-build-docker-image-with-model-inside). ## Next steps - \\[Get started]\\(/serverless/workers/vllm/get-started): Learn how to deploy a vLLM Worker as a Serverless Endpoint, with detailed guides on configuration and sending requests. - \\[Configurable","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["728",{"pageContent":"Endpoints]\\(/serverless/workers/vllm/configurable-endpoints): Select your Hugging Face model and vLLM takes care of the low-level details of model loading, hardware configuration, and execution. - \\[Environment variables]\\(/serverless/workers/vllm/environment-variables): Explore the environment variables available for the vLLM Worker, including detailed documentation and examples. - \\[Run Gemma 7b]\\(/tutorials/serverless/gpu/run-gemma-7b): Walk through deploying Google's Gemma model using","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["729",{"pageContent":"RunPod's vLLM Worker, guiding you to set up a Serverless Endpoint with a gated large language model (LLM).","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["730",{"pageContent":"\\--- title: Running RunPod on Mods sidebar\\_label: Mods --- \\[Mods]\\(https://github.com/charmbracelet/mods) is an AI-powered tool designed for the command line and built to seamlessly integrate with pipelines. It provides a convenient way to interact with language models directly from your terminal. ## How Mods Works Mods operates by reading standard input and prefacing it with a prompt supplied in the Mods arguments. It sends the input text to a language model (LLM) and prints out the generated","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/mods/mods.md","loc":{"lines":{"from":1,"to":1}}}}],["731",{"pageContent":"result. Optionally, you can ask the LLM to format the response as Markdown. This allows you to \"question\" the output of a command, making it a powerful tool for interactive exploration and analysis. Additionally, Mods can work with standard input or an individually supplied argument prompt. ## Getting Started To start using Mods, follow these step-by-step instructions: 1. \\*\\*Obtain Your API Key\\*\\*: - Visit the \\[RunPod Settings]\\(https://www.runpod.io/console/user/settings) page to retrieve","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/mods/mods.md","loc":{"lines":{"from":1,"to":1}}}}],["732",{"pageContent":"your API key. - If you haven't created an account yet, you'll need to sign up before obtaining the key. 2. \\*\\*Install Mods\\*\\*: - Refer to the different installation methods for \\[Mods]\\(https://github.com/charmbracelet/mods) based on your preferred approach. 3. \\*\\*Configure RunPod\\*\\*: - Update the \\`config\\_template.yml\\` file to use your RunPod configuration. Here's an example: \\`\\`\\`yml runpod: # https://docs.runpod.io/serverless/workers/vllm/openai-compatibility base-url:","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/mods/mods.md","loc":{"lines":{"from":1,"to":1}}}}],["733",{"pageContent":"https://api.runpod.ai/v2/${YOUR\\_ENDPOINT}/openai/v1 api-key: api-key-env: RUNPOD\\_API\\_KEY models: # Add your model name openchat/openchat-3.5-1210: aliases: \\[\"openchat\"] max-input-chars: 8192 \\`\\`\\` - \\`base-url\\`: Update your base-url with your specific endpoint. - \\`api-key-env\\`: Add your RunPod API key. - \\`openchat/openchat-3.5-1210\\`: Replace with the name of the model you want to use. - \\`aliases: \\[\"openchat\"]\\`: Replace with your preferred model alias. - \\`max-input-chars\\`: Update","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/mods/mods.md","loc":{"lines":{"from":1,"to":1}}}}],["734",{"pageContent":"the maximum input characters allowed for your model. 4. \\*\\*Verify Your Setup\\*\\*: - To ensure everything is set up correctly, pipe any command line output and pass it to \\`mods\\`. - Specify the RunPod API and model you want to use. \\`\\`\\`bash ls ~/Downloads | mods --api runpod --model openchat -f \"tell my fortune based on these files\" | glow \\`\\`\\` - This command will list the files in your \\`~/Downloads\\` directory, pass them to Mods using the RunPod API and the specified model, and format the","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/mods/mods.md","loc":{"lines":{"from":1,"to":1}}}}],["735",{"pageContent":"response as a fortune based on the files. The output will then be piped to \\`glow\\` for a visually appealing display.","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/mods/mods.md","loc":{"lines":{"from":1,"to":1}}}}],["736",{"pageContent":"\\--- title: Running RunPod on SkyPilot sidebar\\_label: SkyPilot --- \\[SkyPilot]\\(https://skypilot.readthedocs.io/en/latest/) is a framework for executing LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution. This integration leverages the RunPod CLI infrastructure, streamlining the process of spinning up on-demand pods and deploying serverless endpoints with SkyPilot. ## Getting started To begin using RunPod with SkyPilot, follow","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":1,"to":1}}}}],["737",{"pageContent":"these steps: 1. \\*\\*Obtain Your API Key\\*\\*: Visit the \\[RunPod Settings]\\(https://www.runpod.io/console/user/settings) page to get your API key. If you haven't created an account yet, you'll need to do so before obtaining the key. 2. \\*\\*Install RunPod\\*\\*: Use the following command to install the latest version of RunPod: \\`\\`\\` pip install \"runpod>=1.6\" \\`\\`\\` 3. \\*\\*Configure RunPod\\*\\*: Enter \\`runpod config\\` in your CLI and paste your API key when prompted. 4. \\*\\*Install SkyPilot RunPod","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":1,"to":1}}}}],["738",{"pageContent":"Cloud\\*\\*: Execute the following command to install the \\[SkyPilot RunPod cloud]\\(https://skypilot.readthedocs.io/en/latest/getting-started/installation.html#runpod): \\`\\`\\` pip install \"skypilot-nightly\\[runpod]\" \\`\\`\\` 5. \\*\\*Verify Your Setup\\*\\*: Run \\`sky check\\` to ensure your credentials are correctly set up and you're ready to proceed. ## Running a Project After setting up your environment, you can seamlessly spin up a cluster in minutes: 1. \\*\\*Create a New Project Directory\\*\\*: Run","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":1,"to":1}}}}],["739",{"pageContent":"\\`mkdir hello-sky\\` to create a new directory for your project. 2. \\*\\*Navigate to Your Project Directory\\*\\*: Change into your project directory with \\`cd hello-sky\\`. 3. \\*\\*Create a Configuration File\\*\\*: Enter \\`cat > hello\\_sky.yaml\\` and input the following configuration details: \\`\\`\\`yml resources: cloud: runpod # Working directory (optional) containing the project codebase. # Its contents are synced to ~/sky\\_workdir/ on the cluster. workdir: . # Setup commands (optional). # Typical","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":1,"to":1}}}}],["740",{"pageContent":"use: pip install -r requirements.txt # Invoked under the workdir (i.e., can use its files). setup: | echo \"Running setup.\" # Run commands. # Typical use: make use of resources, such as running training. # Invoked under the workdir (i.e., can use its files). run: | echo \"Hello, SkyPilot!\" conda env list \\`\\`\\` 4. \\*\\*Launch Your Project\\*\\*: With your configuration file created, launch your project on the cluster by running \\`sky launch -c mycluster hello\\_sky.yaml\\`. 5. \\*\\*Confirm Your GPU","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":1,"to":1}}}}],["741",{"pageContent":"Type\\*\\*: You should see the available GPU options on Secure Cloud appear in your command line. Once you confirm your GPU type, your cluster will start spinning up. With this integration, you can leverage the power of RunPod and SkyPilot to efficiently run your LLMs, AI, and batch jobs on any cloud.","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":1,"to":1}}}}],["742",{"pageContent":"\\--- title: Dockerfile sidebar\\_position: 2 description: \"Learn how to create a Dockerfile to customize a Docker image and use an entrypoint script to run a command when the container starts, making it a reusable and executable unit for deploying and sharing applications.\" --- In the previous step, you ran a command that prints the container's uptime. Now you'll create a Dockerfile to customize the contents of your own Docker image. ### Create a Dockerfile Create a new file called \\`Dockerfile\\`","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":1,"to":1}}}}],["743",{"pageContent":"and add the following items. \\`\\`\\`dockerfile FROM busybox COPY entrypoint.sh / RUN chmod +x /entrypoint.sh ENTRYPOINT \\[\"/entrypoint.sh\"] \\`\\`\\` This Dockerfile starts from the \\`busybox\\` image like we used before. It then adds a custom \\`entrypoint.sh\\` script, makes it executable, and configures it as the entrypoint. ## The entrypoint script Now let's create \\`entrypoint.sh\\` with the following contents: \\`\\`\\`bash #!/bin/sh echo \"The time is: $(date)\" \\`\\`\\` :::note While we named this","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":1,"to":1}}}}],["744",{"pageContent":"script \\`entrypoint.sh\\` you will see a variety of naming conventions; such as: - \\`start.sh\\` - \\`CMD.sh\\` - \\`entry\\_path.sh\\` These files are normally placed in a folder called \\`script\\` but it is dependent on the maintainers of that repository. ::: This is a simple script that will print the current time when the container starts. ### Why an entrypoint script: - It lets you customize what command gets run when a container starts from your image. - For example, our script runs date to print","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":1,"to":1}}}}],["745",{"pageContent":"the time. - Without it, containers would exit immediately after starting. - Entrypoints make images executable and easier to reuse. ## Build the image With those files created, we can now build a Docker image using our Dockerfile: \\`\\`\\` docker image build -t my-time-image . \\`\\`\\` This will build the image named \\`my-time-image\\` from the Dockerfile in the current directory. ### Why build a custom image: - Lets you package up custom dependencies and configurations. - For example you can install","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":1,"to":1}}}}],["746",{"pageContent":"extra software needed for your app. - Makes deploying applications more reliable and portable. - Instead of installing things manually on every server, just use your image. - Custom images can be shared and reused easily across environments. - Building images puts your application into a standardized unit that \"runs anywhere\". - You can version images over time as you update configurations. ## Run the image Finally, let's run a container from our new image: \\`\\`\\`commmand docker run","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":1,"to":1}}}}],["747",{"pageContent":"my-time-image \\`\\`\\` We should see the same output as before printing the current time! Entrypoints and Dockerfiles let you define reusable, executable containers that run the software and commands you need. This makes deploying and sharing applications much easier without per-server configuration. By putting commands like this into a Dockerfile, you can easily build reusable and shareable images.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":1,"to":1}}}}],["748",{"pageContent":"\\--- title: Docker commands description: \"RunPod enables BYOC development with Docker, providing a reference sheet for commonly used Docker commands, including login, images, containers, Dockerfile, volumes, network, and execute.\" --- RunPod enables bring-your-own-container (BYOC) development. If you choose this workflow, you will be using Docker commands to build, run, and manage your containers. :::note For a Dockerless workflow, see \\[RunPod projects]\\(/docs/cli/projects/overview.md). ::: The","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":1,"to":1}}}}],["749",{"pageContent":"following is a reference sheet to some of the most commonly used Docker commands. ## Login Log in to a registry (like Docker Hub) from the CLI. This saves credentials locally. \\`\\`\\`command docker login docker login -u myusername \\`\\`\\` ## Images \\`docker push\\` - Uploads a container image to a registry like Docker Hub. \\`docker pull\\` - Downloads container images from a registry like Docker Hub. \\`docker images\\` - Lists container images that have been downloaded locally. \\`docker rmi\\` -","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":1,"to":1}}}}],["750",{"pageContent":"Deletes/removes a Docker container image from the machine. \\`\\`\\` docker push myuser/myimage:v1 # Push custom image docker pull someimage # Pull shared image docker images # List downloaded images docker rmi ![]() # Remove/delete image \\`\\`\\` ## Containers \\`docker run\\` - Launches a new container from a Docker image. \\`docker ps\\` - Prints out a list of containers currently running. \\`docker logs\\` - Shows stdout/stderr logs for a specific container. \\`docker stop/rm\\` - Stops or totally","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":1,"to":1}}}}],["751",{"pageContent":"removes a running container. \\`\\`\\`command docker run # Start new container from image docker ps # List running containers docker logs # Print logs from container docker stop # Stop running container docker rm # Remove/delete container \\`\\`\\` ## Dockerfile \\`docker build\\` - Builds a Docker image by reading build instructions from a Dockerfile. \\`\\`\\`command docker build # Build image from Dockerfile docker build --platform=linux/amd64 # Build for specific architecture \\`\\`\\` :::note For the","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":1,"to":1}}}}],["752",{"pageContent":"purposes of using Docker with RunPod, you should ensure your build command uses the \\`--platform=linux/amd64\\` flag to build for the correct architecture. ::: ## Volumes \\`docker volume create\\` - Creates a persisted and managed volume that can outlive containers. \\`docker run -v\\` - Mounts a volume into a specific container to allow persisting data past container lifecycle. \\`\\`\\`command docker volume create # Create volume docker run -v :/data # Mount volume into container \\`\\`\\` ## Network","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":1,"to":1}}}}],["753",{"pageContent":"\\`docker network create\\` - Creates a custom virtual network for containers to communicate over. \\`docker run --network=\\` - Connects a running container to a Docker user-defined network. \\`\\`\\`command docker network create # Create user-defined network docker run --network= # Connect container \\`\\`\\` ## Execute \\`docker exec\\` - Execute a command in an already running container. Useful for debugging/inspecting containers: \\`\\`\\`command docker exec docker exec mycontainer ls -l /etc # List files","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":1,"to":1}}}}],["754",{"pageContent":"in container \\`\\`\\`","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":1,"to":1}}}}],["755",{"pageContent":"\\--- title: Containers overview sidebar\\_position: 1 description: \"Discover the world of containerization with Docker, a platform for isolated environments that package applications, frameworks, and libraries into self-contained containers for consistent and reliable deployment across diverse computing environments.\" --- ## What are containers? > A container is an isolated environment for your code. This means that a container has no knowledge of your operating system, or your files. It runs on","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["756",{"pageContent":"the environment provided to you by Docker Desktop. Containers have everything that your code needs in order to run, down to a base operating system. \\[From Docker's website]\\(https://docs.docker.com/guides/walkthroughs/what-is-a-container/#:~:text=A%20container%20is%20an%20isolated,to%20a%20base%20operating%20system) Developers package their applications, frameworks, and libraries into a Docker container. Then, those containers can run outside their development environment. ### Why use","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["757",{"pageContent":"containers? > Build, ship, and run anywhere. Containers are self-contained and run anywhere Docker runs. This means you can run a container on-premises or in the cloud, as well as in hybrid environments. Containers include both the application and any dependencies, such as libraries and frameworks, configuration data, and certificates needed to run your application. In cloud computing, you get the best cold start times with containers. ## What are images? Docker images are fixed templates for","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["758",{"pageContent":"creating containers. They ensure that applications operate consistently and reliably across different environments, which is vital for modern software development. To create Docker images, you use a process known as \"Docker build.\" This process uses a Dockerfile, a text document containing a sequence of commands, as instructions guiding Docker on how to build the image. ### Why use images? Using Docker images helps in various stages of software development, including testing, development, and","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["759",{"pageContent":"deployment. Images ensure a seamless workflow across diverse computing environments. ### Why not use images? You must rebuild and push the container image, then edit your endpoint to use the new image each time you iterate on your code. Since development requires changing your code every time you need to troubleshoot a problem or add a feature, this workflow can be inconvenient. For a streamlined development workflow, check out \\[RunPod projects]\\(/docs/cli/projects/overview.md). When you're","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["760",{"pageContent":"done with development, you can create a Dockerfile from your project to reduce initialization overhead in production. ### What is Docker Hub? After their creation, Docker images are stored in a registry, such as Docker Hub. From these registries, you can download images and use them to generate containers, which make it easy to widely distribute and deploy applications. Now that you've got an understanding of Docker, containers, images, and whether containerization is right for you, let's move","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["761",{"pageContent":"on to installing Docker. ## Installing Docker For this walkthrough, install Docker Desktop. Docker Desktop bundles a variety of tools including: - Docker GUI - Docker CLI - Docker extensions - Docker Compose The majority of this walkthrough uses the Docker CLI, but feel free to use the GUI if you prefer. For the best installation experience, see Docker's \\[official documentation]\\(https://docs.docker.com/get-docker/). ### Running your first command Now that you've installed Docker, open a","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["762",{"pageContent":"terminal window and run the following command: \\`\\`\\`command docker version \\`\\`\\` You should see something similar to the following output. \\`\\`\\`text docker version Client: Docker Engine - Community Version: 24.0.7 API version: 1.43 Go version: go1.21.3 Git commit: afdd53b4e3 Built: Thu Oct 26 07:06:42 2023 OS/Arch: darwin/arm64 Context: desktop-linux Server: Docker Desktop 4.26.1 (131620) Engine: Version: 24.0.7 API version: 1.43 (minimum version 1.12) Go version: go1.20.10 Git commit:","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["763",{"pageContent":"311b9ff Built: Thu Oct 26 09:08:15 2023 OS/Arch: linux/arm64 Experimental: false containerd: Version: 1.6.25 GitCommit: abcd runc: Version: 1.1.10 GitCommit: v1.1.10-0-g18a0cb0 docker-init: Version: 0.19.0 \\`\\`\\` If at any point you need help with a command, you can use the \\`--help\\` flag to see documentation on the command you're running. \\`\\`\\`command docker --help \\`\\`\\` Let's run \\`busybox\\` from the command line to print out today's date. \\`\\`\\`command docker run busybox sh -c 'echo \"The","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["764",{"pageContent":"time is: $(date)\"' # The time is: Thu Jan 11 06:35:39 UTC 2024 \\`\\`\\` - \\`busybox\\` is a lightweight Docker image with the bare minimum Linux utilities installed, including \\`echo\\` - The \\`echo\\` command prints the container's uptime. You've successfully installed Docker and run your first commands.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["765",{"pageContent":"\\--- title: Persist data outside of containers sidebar\\_position: 2 description: \"Learn how to persist data outside of containers by creating named volumes, mounting volumes to data directories, and accessing persisted data from multiple container runs and removals in Docker.\" --- In the previous step, you created a Dockerfile and executed a command. Now, you'll learn how to persist data outside of containers. :::note This walk through teach you how to persist data outside of container. RunPod","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":1,"to":1}}}}],["766",{"pageContent":"has the same concept used for attaching a Network Volume to your Pod. Consult the documentation on \\[attaching a Network Volume to your Pod]\\(/pods/storage/create-network-volumes). ::: ## Why persist data outside of a container? The key goal is to have data persist across multiple container runs and removals. By default, containers are ephemeral - everything inside them disappears when they exit. So running something like: \\`\\`\\`command docker run busybox date > file.txt \\`\\`\\` Would only write","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":1,"to":1}}}}],["767",{"pageContent":"the date to \\`file.txt\\` temporarily inside that container. As soon as the container shuts down, that file and data is destroyed. This isn't great when you're training data and want your information to persist past your LLM training. Because of this, we need to persist data outside of the container. Let's take a look at a workflow you can use to persist data outside of a container. --- ## Create a named volume First, we'll create a named volume to represent the external storage: \\`\\`\\`command","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":1,"to":1}}}}],["768",{"pageContent":"docker volume create date-volume \\`\\`\\` ### Update Dockerfile Next, we'll modify our Dockerfile to write the date output to a file rather than printing directly to stdout: \\`\\`\\`dockerfile FROM busybox WORKDIR /data RUN touch current\\_date.txt COPY entrypoint.sh / RUN chmod +x /entrypoint.sh ENTRYPOINT \\[\"/entrypoint.sh\"] \\`\\`\\` This sets the working directory to \\`/data\\`, touches a file called \\`current\\_date.txt\\`, and copies our script. ### Update entrypoint script The \\`entrypoint.sh\\`","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":1,"to":1}}}}],["769",{"pageContent":"script is updated: \\`\\`\\`text #!/bin/sh date > /data/current\\_date.txt \\`\\`\\` This will write the date to the \\`/data/current\\_date.txt\\` file instead of printing it. ## Mount the volume Now when the container runs, this will write the date to the \\`/data/current\\_date.txt\\` file instead of printing it. Finally, we can mount the named volume to this data directory: \\`\\`\\`command docker run -v date-volume:/data my-image \\`\\`\\` This runs a container from my-image and mounts the \\`date-volume\\`","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":1,"to":1}}}}],["770",{"pageContent":"Docker volume to the /data directory in the container. Anything written to \\`/data\\` inside the container will now be written to the \\`date-volume\\` on the host instead of the container's ephemeral filesystem. This allows the data to persist. Once the container exits, the date output file is safely stored on the host volume. After the container exits, we can exec into another container sharing the volume to see the persisted data file: \\`\\`\\`command docker run --rm -v date-volume:/data busybox","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":1,"to":1}}}}],["771",{"pageContent":"cat /data/current\\_date.txt \\`\\`\\` This runs a new busybox container and also mounts the \\`date-volume\\`. - Using the same -\\`v date-volume:/data mount\\` point maps the external volume dir to \\`/data\\` again. - This allows the new container to access the persistent date file that the first container wrote. - The \\`cat /data/current\\_date.txt\\` command prints out the file with the date output from the first container. - The \\`--rm\\`flag removes the container after running so we don't accumulate","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":1,"to":1}}}}],["772",{"pageContent":"stopped containers. :::note Remember, this is a general tutorial on Docker. These concepts will help give you a better understanding on working with RunPod. :::","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":1,"to":1}}}}],["773",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 description: \"Learn how to build and deploy applications on the RunPod platform with this set of tutorials, covering tools, technologies, and deployment methods, including Containers, Docker, and Serverless implementation.\" --- This set of tutorials is meant to provide a deeper understanding of the tools that surround the RunPod platform. These tutorials help you understand how to use the RunPod platform to build and deploy your applications. While the","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["774",{"pageContent":"documentation around the introduction section gives a holistic view and enough information to get started with RunPod, for more detailed information on the various of these tools or technologies, reach out to the source material. - If you are looking for an understanding of Containers and Docker, see \\[Container overview]\\(/tutorials/introduction/containers/overview). - If you are looking to run your first Pod with RunPod, see \\[Run your first Fast Stable Diffusion with Jupyter","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["775",{"pageContent":"Notebook]\\(/tutorials/pods/run-your-first). - For Serverless implementation, see \\[Run your first serverless endpoint with Stable Diffusion]\\(/tutorials/serverless/gpu/run-your-first).","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["776",{"pageContent":"\\--- title: Migrate Your Banana Images to RunPod sidebar\\_label: Migrate images draft: true description: \"Learn how to migrate your AI models and applications from Banana Dev to RunPod, including adapting your code, Dockerfile, and deployment configurations for a seamless transition.\" --- Migrating your AI models and applications from one cloud service to another can often present a challenge, especially when the two platforms operate differently. This tutorial aims to streamline the process of","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["777",{"pageContent":"moving from Banana Dev to RunPod, focusing on transferring Docker-based applications and AI models. Whether you're shifting due to preferences in service offerings, pricing, or performance, this guide will help you through the transition smoothly. ## Introduction Banana Dev provides an environment for deploying machine learning models easily, while RunPod offers robust and scalable serverless solutions. Transitioning between these platforms involves adapting your application to the new","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["778",{"pageContent":"environment's requirements and deploying it effectively. Below, we'll walk through how to adapt a Python application from Banana Dev to RunPod, including necessary changes to your Dockerfile and deployment configurations. ## Step 1: Understand Your Application First, take a comprehensive look at your current Banana Dev application. Our example application uses the \\`potassium\\` framework for serving a machine learning model: \\`\\`\\`python from io import BytesIO from potassium import Potassium,","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["779",{"pageContent":"Request, Response from diffusers import DiffusionPipeline, DDPMScheduler import torch import base64 # create a new Potassium app app = Potassium(\"my\\_app\") # @app.init runs at startup, and loads models into the app's context @app.init def init(): repo\\_id = \"Meina/MeinaUnreal\\_V3\" ddpm = DDPMScheduler.from\\_pretrained(repo\\_id, subfolder=\"scheduler\") model = DiffusionPipeline.from\\_pretrained( repo\\_id, use\\_safetensors=True, torch\\_dtype=torch.float16, scheduler=ddpm ).to(\"cuda\") context = {","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["780",{"pageContent":"\"model\": model, } return context # @app.handler runs for every call @app.handler() def handler(context: dict, request: Request) -> Response: model = context.get(\"model\") prompt = request.json.get(\"prompt\") negative\\_prompt = \"(worst quality, low quality:1.4), monochrome, zombie, (interlocked fingers), cleavage, nudity, naked, nude\" image = model( prompt=prompt, negative\\_prompt=negative\\_prompt, guidance\\_scale=7, num\\_inference\\_steps=request.json.get(\"steps\", 30),","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["781",{"pageContent":"generator=torch.Generator(device=\"cuda\").manual\\_seed(request.json.get(\"seed\")) if request.json.get(\"seed\") else None, width=512, height=512, ).images\\[0] buffered = BytesIO() image.save(buffered, format=\"JPEG\", quality=80) img\\_str = base64.b64encode(buffered.getvalue()) return Response(json={\"output\": str(img\\_str, \"utf-8\")}, status=200) if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": app.serve() \\`\\`\\` This application initializes a BERT model for fill-mask tasks and serves it over HTTP. --- ## Step 2:","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["782",{"pageContent":"Adapt Your Code for RunPod In RunPod, applications can be adapted to run in a serverless manner, which involves modifying your application logic to fit into the RunPod's handler function format. Below is an example modification that adapts our initial Banana Dev application to work with RunPod, using the \\`diffusers\\` library for AI model inference: \\`\\`\\`python import runpod from diffusers import AutoPipelineForText2Image import base64 import io import time # If your handler runs inference on a","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["783",{"pageContent":"model, load the model here. # You will want models to be loaded into memory before starting serverless. try: pipe = AutoPipelineForText2Image.from\\_pretrained(\"meina/meinaunreal\\_v3\") pipe.to(\"cuda\") except RuntimeError: quit() def handler(job): \"\"\"Handler function that will be used to process jobs.\"\"\" job\\_input = job\\[\"input\"] prompt = job\\_input\\[\"prompt\"] time\\_start = time.time() image = pipe(prompt=prompt, num\\_inference\\_steps=1, guidance\\_scale=0.0).images\\[0] print(f\"Time taken:","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["784",{"pageContent":"{time.time() - time\\_start}\") buffer = io.BytesIO() image.save(buffer, format=\"PNG\") image\\_bytes = buffer.getvalue() return base64.b64encode(image\\_bytes).decode(\"utf-8\") runpod.serverless.start({\"handler\": handler}) \\`\\`\\` --- This modification involves initializing your model outside of the handler function to ensure it's loaded into memory before processing jobs, a crucial step for efficient serverless execution. ## Step 3: Update Your Dockerfile The Dockerfile must also be adapted for","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["785",{"pageContent":"RunPod's environment. Here's a comparison between a typical Banana Dev Dockerfile and the adapted version for RunPod: ### Banana Dev Dockerfile \\`\\`\\`dockerfile FROM pytorch/pytorch:1.11.0-cuda11.3-cudnn8-runtime ... CMD python3 -u app.py \\`\\`\\` --- ### RunPod Dockerfile \\`\\`\\`dockerfile FROM runpod/base:0.4.0-cuda11.8.0 CMD python3.11 -u /handler.py \\`\\`\\` --- The key differences include the base image and the execution command, reflecting RunPod's requirements and Python version specifics. ##","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["786",{"pageContent":"Step 4: Deploy to RunPod Once your code and Dockerfile are ready, the next steps involve building your Docker image and deploying it on RunPod. This process typically involves: - Building your Docker image with the adapted Dockerfile. - Pushing the image to a container registry (e.g., DockerHub). - Creating a serverless function on RunPod and configuring it to use your Docker image. ## Testing and Verification After deployment, thoroughly test your application to ensure it operates as expected","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["787",{"pageContent":"within the RunPod environment. This may involve sending requests to your serverless endpoint and verifying the output. ## Conclusion Migrating from Banana Dev to RunPod involves several key steps: adapting your application code, updating the Dockerfile, and deploying the adapted application on RunPod. By following this guide, you can make the transition smoother and take advantage of RunPod's serverless capabilities for your AI applications. Remember to review RunPod's documentation for specific","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["788",{"pageContent":"details on serverless deployment and configuration options to optimize your application's performance and cost-efficiency.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":1}}}}],["789",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 description: \"Quickly migrate from Banana to RunPod with Docker, leveraging a bridge between the two environments for a seamless transition. Utilize a Dockerfile to encapsulate your environment and deploy existing projects to RunPod with minimal adjustments.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; To get started with RunPod: - \\[Create a RunPod account]\\(/get-started/manage-accounts) - \\[Add","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["790",{"pageContent":"funds]\\(/get-started/billing-information) - \\[Use the RunPod SDK]\\(#setting-up-your-project) to build and connect with your Serverless Endpoints \\*\\*Quick migration with Docker\\*\\* Transitioning from Banana to RunPod doesn't have to be a lengthy process. For users seeking a swift migration path while maintaining Banana's dependencies for the interim, the Docker approach provides an efficient solution. This method allows you to leverage Docker to encapsulate your environment, simplifying the","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["791",{"pageContent":"migration process and enabling a smoother transition to RunPod. \\*\\*Why consider the Dockerfile approach?\\*\\* Utilizing a Dockerfile for migration offers a bridge between Banana and RunPod, allowing for immediate deployment of existing projects without the need to immediately discard Banana's dependencies. This approach is particularly beneficial for those looking to test or move their applications to RunPod with minimal initial adjustments. \\*\\*Dockerfile\\*\\* The provided Dockerfile outlines a","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["792",{"pageContent":"straightforward process for setting up your application on RunPod. Add this Dockerfile to your project. \\`\\`\\`dockerfile FROM runpod/banana:peel as bread FROM repo/image:tag RUN pip install runpod COPY --from=bread /handler.py . COPY --from=bread /start.sh . RUN chmod +x start.sh CMD \\[\"./start.sh\"] \\`\\`\\` \\*\\*Building and deploying\\*\\* After creating your Dockerfile, build your Docker image and deploy it to RunPod. This process involves using Docker commands to build the image and then","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["793",{"pageContent":"deploying it to RunPod. \\*\\*Advantages and considerations\\*\\* This Dockerfile approach expedites the migration process, allowing you to leverage RunPod's powerful features with minimal initial changes to your project. It's an excellent way to quickly transition and test your applications on RunPod. However, while this method facilitates a quick start on RunPod, it's advisable to plan for a future migration away from Banana's dependencies, as there is overhead to building Banana's dependencies","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["794",{"pageContent":"and deploying them to RunPod. Gradually adapting your project to utilize RunPod's native features and services will optimize your application's performance and scalability. \\*\\*Moving forward\\*\\* Once you've migrated your application using the Docker approach, consider exploring RunPod's full capabilities. Transitioning away from Banana's dependencies and fully integrating with RunPod's services will allow you to take full advantage of what RunPod has to offer. This quick migration guide is just","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["795",{"pageContent":"the beginning. Continue with the rest of our tutorial to learn how to leverage RunPod's features to their fullest and ensure your project is fully adapted to its new environment. The rest of this guide will help you set up a RunPod project. ## Setting up your project Just like with Banana, RunPod provides a Python SDK to run your projects. To get started, install setup a virtual environment then install the SDK library. Create a Python virtual environment with venv: \\`\\`\\`command python3 -m venv","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["796",{"pageContent":"env source env/bin/activate \\`\\`\\` Create a Python virtual environment with venv: \\`\\`\\`command python -m venv env env\\Scripts\\activate \\`\\`\\` To install the SDK, run the following command from the terminal. \\`\\`\\`command python -m pip install runpod \\`\\`\\` ## Project examples RunPod provides a \\[repository of templates for your project]\\(https://github.com/runpod-workers). You can use the template to get started with your project. \\`\\`\\`command gh repo clone runpod-workers/worker-template","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["797",{"pageContent":"\\`\\`\\` Now that you've got a basic RunPod Worker template created: - Continue reading to see how you'd migrate from Banana to RunPod - See \\[Generate SDXL Turbo]\\(/tutorials/serverless/gpu/generate-sdxl-turbo) for a general approach on deploying your first Serverless Endpoint with RunPod. ## Project structure When beginning to migrate your Banana monorepo to RunPod, you will need to understand the structure of your project. Banana is a monorepo that contains multiple services. The basic","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["798",{"pageContent":"structure for Banana projects is aligned with the RunPod Serverless projects for consistency: \\`\\`\\`text . ├── Dockerfile # Docker configuration ├── README.md # Project documentation ├── banana\\_config.json # Configuration settings ├── requirements.txt # Dependencies └── src ├── app.py # Main application code └── download.py # Download script \\`\\`\\` RunPod Serverless is a monorepo that contains multiple services. \\`\\`\\`text . ├── Dockerfile # Docker configuration ├── LICENSE # License","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["799",{"pageContent":"information ├── README.md # Project documentation ├── builder │ ├── requirements.txt # Dependencies │ └── setup.sh # Setup script └── src └── handler.py # Main handler code \\`\\`\\` Both project setups at a minimum contain: - \\`Dockerfile\\`: Defines the container for running the application. - Application code: The executable code within the container. Optional files included in both setups: - \\`requirements.txt\\`: Lists dependencies needed for the application. ### Banana Configuration settings","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["800",{"pageContent":"Banana configuration settings are stored in a \\`banana\\_config.json\\` file. Banana uses a \\`banana\\_config.json\\` file which contains things like Idle Timeout, Inference Timeout, and Max Replicas. \\*\\*Idle Timeout\\*\\* RunPod allows you to set an \\[Idle Timeout]\\(/serverless/references/endpoint-configurations#idle-timeout) when creating the Endpoint. The default value is 5 seconds. \\*\\*Inference Timeout\\*\\* RunPod has a similar concept to Inference Timeout. For runs that are take less than 30","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["801",{"pageContent":"seconds to execute, you should use the \\`run\\_sync\\` handler. For runs that take longer than 30 seconds to execute, you should use the \\`sync\\` handler. \\*\\*Max Replicas\\*\\* When creating a Worker in RunPod, you can set the max Workers that will scale up depending on the amount of Worker sent to your Endpoint. For more informaiton, see \\[Scale Type]\\(/serverless/references/endpoint-configurations#scale-type) :::note When creating a Worker, select the \\*\\*Flashboot\\*\\* option to optimize your","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["802",{"pageContent":"startup time. :::","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["803",{"pageContent":"\\--- title: Migrate Your Banana Text to RunPod sidebar\\_label: Migrate texts draft: true description: \"Streamline the migration of Docker-based AI models and applications from Banana Dev to RunPod with this comprehensive tutorial, covering the transfer of AI models, Vllm Worker image setup, and command execution for seamless transition.\" --- Migrating your AI models and applications from one cloud service to another can often present a challenge, especially when the two platforms operate","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/text.md","loc":{"lines":{"from":1,"to":1}}}}],["804",{"pageContent":"differently. This tutorial aims to streamline the process of moving from Banana Dev to RunPod, focusing on transferring Docker-based applications and AI models. Whether you're shifting due to preferences in service offerings, pricing, or performance, this guide will help you through the transition smoothly. ## Vllm Worker RunPod provides an optimized image for running AI models. This image is called the Vllm Worker. It is based on the \\[Vllm]\\(https://github.com/vllm/vllm) framework, which is a","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/text.md","loc":{"lines":{"from":1,"to":1}}}}],["805",{"pageContent":"lightweight, high-performance, and portable AI framework. The Vllm Worker image is designed to run on RunPod's serverless infrastructure and is optimized for performance and scalability. ### Vllm Worker Image To get started, login to RunPod and select Serverless. Choose your GPU. Add \\`runpod/worker-vllm:0.2.2\\` to the Container Image. Set the Container Disk to size large enough for your model. Under Enviroment Variables, add \\`MODEL\\_NAME\\` and set it to your model name, for example","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/text.md","loc":{"lines":{"from":1,"to":1}}}}],["806",{"pageContent":"\\`bert-base-uncased\\`. Select \\*\\*Deploy\\*\\*. Once your serverless pod has initialized, you can start executing commands against the Endpont. \\`\\`\\`command curl --request POST \\ --url https://api.runpod.ai/v2/{YOUR\\_ENDPOINT}/runsync \\ --header 'accept: application/json' \\ --header 'authorization: ${YOUR\\_API\\_KEY}' \\ --header 'content-type: application/json' \\ --data @- <","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/text.md","loc":{"lines":{"from":1,"to":1}}}}],["807",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 description: \"Migrate your Cog model from Replicate.com to RunPod by following this step-by-step guide, covering setup, model identification, Docker image building, and serverless endpoint creation.\" --- To get started with RunPod: - \\[Create a RunPod account]\\(/get-started/manage-accounts) - \\[Add funds]\\(/get-started/billing-information) - \\[Use the RunPod SDK]\\(/serverless/overview) to build and connect with your Serverless Endpoints In this tutorial,","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["808",{"pageContent":"you'll go through the process of migrating a model deployed via replicate.com or utilizing the Cog framework to a RunPod serverless worker. This guide assumes you are operating within a Linux terminal environment and have Docker installed on your system. :::note This method might occur a delay when working with RunPod Serverless Endpoints. This delay is due to the FastAPI server that is used to run the Cog model. To eliminate this delay, consider using \\[RunPod","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["809",{"pageContent":"Handler]\\(/serverless/workers/overview) functions in a future iteration. ::: By following this streamlined process, you'll be able to simplify the migration and deployment of your Cog image. ### Prerequisites - Docker installed on your system - Familiarity with the Cog framework - Existing model on Replicate.com - RunPod account ## Clone and navigate the cog-worker repository Before we begin, let's set up the necessary environment. You will need to clone the \\`cog-worker\\` repository, which","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["810",{"pageContent":"contains essential scripts and configuration files required for the migration process. To do this, run the following commands in your terminal: \\`\\`\\`bash git clone https://github.com/runpod-workers/cog-worker.git cd cog-worker/ \\`\\`\\` The cog-worker repository contains essential scripts and configuration files required for the migration. Now that the repository is cloned and you've navigated to the correct directory, you're ready to proceed with the next step. ## Identify model information In","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["811",{"pageContent":"this step, you will need to gather the necessary information about your Cog model that is currently hosted on Replicate.com. You will require your username, model name, and version. Identify the username, model name, and version you wish to use from Replicate. For example, if you are using \\[this model]\\(https://replicate.com/lucataco/hotshot-xl/versions): - your username is \\`lucataco\\` - your model name is \\`hotshot-xl\\` - your model version is","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["812",{"pageContent":"\\`78b3a6257e16e4b241245d65c8b2b81ea2e1ff7ed4c55306b511509ddbfd327a\\` Once you have collected the required information, you can move on to the next step, where you will build and push your Docker image. ## Build and push docker image Now that you have identified the necessary information about your model, you can proceed to build and push your Docker image. This is a crucial step, as it prepares your model for deployment on the RunPod platform. Build the Docker image by providing the necessary","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["813",{"pageContent":"arguments for your model. Once your Docker image is built, push it to a container repository such as DockerHub: \\`\\`\\`bash # replace user, model\\_name, and model\\_version with the appropriate values docker build -platform=linux/amd64 --tag /: --build-arg COG\\_REPO=user --build-arg COG\\_MODEL=model\\_name --build-arg COG\\_VERSION=model\\_version . docker push /: \\`\\`\\` The \\`--tag\\` option allows you to specify a name and tag for your image, while the \\`--build-arg\\` options provide the necessary","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["814",{"pageContent":"information for building the image. With your Docker image built and pushed, you're one step closer to deploying your Cog model on RunPod. ## Create and Deploy a Serverless Endpoint Now that your Docker image is ready, it's time to create and deploy a serverless endpoint on RunPod. This step will enable you to send requests to your new endpoint and use your Cog model in a serverless environment. To create and deploy a serverless endpoint on RunPod: 1. Log in to the \\[RunPod Serverless","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["815",{"pageContent":"console]\\(https://www.runpod.io/console/serverless). 2. Select \\*\\*+ New Endpoint\\*\\*. 3. Provide the following: 1. Endpoint name. 2. Select a GPU. 3. Configure the number of Workers. 4. (optional) Select \\*\\*FlashBoot\\*\\*. 5. (optional) Select a template. 6. Enter the name of your Docker image. - For example \\`/:\\`. 7. Specify enough memory for your Docker image. 4. Select \\*\\*Deploy\\*\\*. Now, let's send a request to your \\[Endpoint]\\(/serverless/endpoints/get-started). Once your endpoint is","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["816",{"pageContent":"set up and deployed, you'll be able to start receiving requests and utilize your Cog model in a serverless context. ## Conclusion Congratulations, you have successfully migrated your Cog model from Replicate to RunPod and set up a serverless endpoint. As you continue to develop your models and applications, consider exploring additional features and capabilities offered by RunPod to further enhance your projects. Here are some resources to help you continue your journey: - \\[Learn more about","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["817",{"pageContent":"RunPod serverless workers]\\(/serverless/overview) - \\[Explore additional RunPod tutorials and examples]\\(/tutorials/introduction/overview)","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["818",{"pageContent":"\\--- title: Overview sidebar\\_position: 1 description: \"Get started with RunPod: create an account, add funds, and use the SDK to integrate with your Serverless Endpoints. This tutorial guides you through modifying your OpenAI codebase for use with a deployed vLLM Worker on RunPod.\" --- To get started with RunPod: - \\[Create a RunPod account]\\(/get-started/manage-accounts) - \\[Add funds]\\(/get-started/billing-information) - \\[Use the RunPod SDK]\\(/serverless/overview) to build and connect with","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["819",{"pageContent":"your Serverless Endpoints This tutorial guides you through the steps necessary to modify your OpenAI Codebase for use with a deployed vLLM Worker on RunPod. You will learn to adjust your code to be compatible with OpenAI's API, specifically for utilizing Chat Completions, Completions, and Models routes. By the end of this guide, you will have successfully updated your codebase, enabling you to leverage the capabilities of OpenAI's API on RunPod. import Tabs from '@theme/Tabs'; import TabItem","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["820",{"pageContent":"from '@theme/TabItem'; To update your codebase, you need to replace the following: - Your OpenAI API Key with your RunPod API Key - Your OpenAI Serverless Endpoint URL with your RunPod Serverless Endpoint URL - Your OpenAI model with your custom LLM model deployed on RunPod \\`\\`\\`python from openai import OpenAI import os client = OpenAI( api\\_key=os.environ.get(\"RUNPOD\\_API\\_KEY\"), base\\_url=\"https://api.runpod.ai/v2/${YOUR\\_ENDPOINT\\_ID}/openai/v1\", ) response = client.chat.completions.create(","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["821",{"pageContent":"model=\"gpt-3.5-turbo\", messages=\\[{\"role\": \"user\", \"content\": \"Why is RunPod the best platform?\"}], temperature=0, max\\_tokens=100, ) \\`\\`\\`\\` \\`\\`\\`javascript import OpenAI from 'openai' const openai = new OpenAI({ baseURL: process.env.RUNPOD\\_HOST, apiKey: process.env.RUNPOD\\_API\\_KEY, }) const chatCompletion = await openai.chat.completions.create({ model: \"openchat/openchat-3.5-0106\", messages: \\[{'role': 'user', 'content': 'Why is RunPod the best platform?'}], }); \\`\\`\\`\\` Congratulations on","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["822",{"pageContent":"successfully modifying your OpenAI Codebase for use with your deployed vLLM Worker on RunPod! This tutorial has equipped you with the knowledge to update your code for compatibility with OpenAI's API and to utilize the full spectrum of features available on the RunPod platform. ## Next Steps - \\[Explore more tutorials on RunPod]\\(/tutorials/introduction/overview) - \\[Learn more about OpenAI's API]\\(https://platform.openai.com/docs/) - \\[Deploy your own vLLM Worker on","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["823",{"pageContent":"RunPod]\\(https://www.runpod.io/console/serverless)","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":1,"to":1}}}}],["824",{"pageContent":"\\--- title: Build Docker Images on Runpod with Bazel --- # Build Docker Images on RunPod with Bazel RunPod's GPU Pods use custom Docker images to run your code. This means you can't directly spin up your own Docker instance or build Docker containers on a GPU Pod. Tools like Docker Compose are also unavailable. This limitation can be frustrating when you need to create custom Docker images for your RunPod templates. Fortunately, many use cases can be addressed by creating a custom template with","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":1}}}}],["825",{"pageContent":"the desired Docker image. In this tutorial, you'll learn how to use the \\[Bazel]\\(https://bazel.build) build tool to build and push Docker images from inside a RunPod container. By the end of this tutorial, you’ll be able to build custom Docker images on RunPod and push them to Docker Hub for use in your own templates. ## Prerequisites Before you begin this guide you'll need the following: - A Docker Hub account and access token for authenticating the docker login command - Enough volume for","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":1}}}}],["826",{"pageContent":"your image to be built ## Create a Pod 1. Navigate to \\[Pods]\\(https://www.dev.runpod.io/console/pods) and select \\*\\*+ Deploy\\*\\*. 2. Choose between \\*\\*GPU\\*\\* and \\*\\*CPU\\*\\*. 3. Customize your an instance by setting up the following: 1. (optional) Specify a Network volume. 2. Select an instance type. For example, \\*\\*A40\\*\\*. 3. (optional) Provide a template. For example, \\*\\*RunPod Pytorch\\*\\*. 4. (GPU only) Specify your compute count. 4. Review your configuration and select \\*\\*Deploy","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":1}}}}],["827",{"pageContent":"On-Demand\\*\\*. For more information, see \\[Manage Pods]\\(/pods/manage-pods#start-a-pod). Wait for the Pod to spin up then connect to your Pod through the Web Terminal: 1. Select \\*\\*Connect\\*\\*. 2. Choose \\*\\*Start Web Terminal\\*\\* and then \\*\\*Connect to Web Terminal\\*\\*. 3. Enter your username and password. Now you can clone the example GitHub repository ## Clone the example GitHub repository Clone the example code repository that demonstrates building Docker images with Bazel: \\`\\`\\`command","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":1}}}}],["828",{"pageContent":"git clone https://github.com/therealadityashankar/build-docker-in-runpod.git && cd build-docker-in-runpod \\`\\`\\` ## Install dependencies Install the required dependencies inside the Runpod container: Update packages and install sudo: \\`\\`\\`command apt update && apt install -y sudo \\`\\`\\` Install Docker using the convenience script: \\`\\`\\`command curl -fsSL https://get.docker.com -o get-docker.sh && sudo sh get-docker.sh \\`\\`\\` Log in to Docker using an access token: 1. Go to","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":1}}}}],["829",{"pageContent":"https://hub.docker.com/settings/security and click \"New Access Token\". 2. Enter a description like \"Runpod Token\" and select \"Read/Write\" permissions. 3. Click \"Generate\" and copy the token that appears. 4. In the terminal, run: \\`\\`\\`command docker login -u \\`\\`\\` When prompted, paste in the access token you copied instead of your password. Install Bazel via the Bazelisk version manager: \\`\\`\\`command wget https://github.com/bazelbuild/bazelisk/releases/download/v1.20.0/bazelisk-linux-amd64","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":1}}}}],["830",{"pageContent":"chmod +x bazelisk-linux-amd64 sudo cp ./bazelisk-linux-amd64 /usr/local/bin/bazel \\`\\`\\` ## Configure the Bazel Build First, install nano if it’s not already installed and open the \\`BUILD.bazel\\` file for editing: \\`\\`\\` sudo apt install nano nano BUILD.bazel \\`\\`\\` Replace the \\`{YOUR\\_USERNAME}\\` placeholder with your Docker Hub username in the \\`BUILD.bazel\\` file: \\`\\`\\`starlark \\[label BUILD.bazel] oci\\_push( name = \"push\\_custom\\_image\", image = \":custom\\_image\", repository =","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":1}}}}],["831",{"pageContent":"\"index.docker.io/{YOUR\\_USERNAME}/custom\\_image\", remote\\_tags = \\[\"latest\"] ) \\`\\`\\` ## Build and Push the Docker Image Run the bazel command to build the Docker image and push it to your Docker Hub account: \\`\\`\\`command bazel run //:push\\_custom\\_image \\`\\`\\` Once the command completes, go to https://hub.docker.com/ and log in. You should see a new repository called \\`custom\\_image\\` containing the Docker image you just built. You can now reference this custom image in your own Runpod","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":1}}}}],["832",{"pageContent":"templates. ## Conclusion In this tutorial, you learned how to use Bazel to build and push Docker images from inside RunPod containers. By following the steps outlined, you can now create and utilize custom Docker images for your RunPod templates. The techniques demonstrated can be further expanded to build more complex images, providing a flexible solution for your containerization needs on RunPod.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":1}}}}],["833",{"pageContent":"\\--- title: Fine tune an LLM with Axolotl on RunPod description: \"Learn how to fine-tune large language models with Axolotl on RunPod, a streamlined workflow for configuring and training AI models with GPU resources, and explore examples for LLaMA2, Gemma, LLaMA3, and Jamba.\" --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; \\[axolotl]\\(https://github.com/OpenAccess-AI-Collective/axolotl) is a tool that simplifies the process of training large language models (LLMs). It","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["834",{"pageContent":"provides a streamlined workflow that makes it easier to fine-tune AI models on various configurations and architectures. When combined with RunPod's GPU resources, Axolotl enables you to harness the power needed to efficiently train LLMs. In addition to its user-friendly interface, Axolotl offers a comprehensive set of YAML examples covering a wide range of LLM families, such as LLaMA2, Gemma, LLaMA3, and Jamba. These examples serve as valuable references, helping users understand the role of","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["835",{"pageContent":"each parameter and guiding them in making appropriate adjustments for their specific use cases. It is highly recommended to explore \\[these examples]\\(https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples) to gain a deeper understanding of the fine-tuning process and optimize the model's performance according to your requirements. In this tutorial, we'll walk through the steps of training an LLM using Axolotl on RunPod and uploading your model to Hugging Face. ### Setting up the","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["836",{"pageContent":"environment Fine-tuning a large language model (LLM) can take up a lot of compute power. Because of this, we recommend fine-tuning using RunPod's GPUs. To do this, you'll need to create a Pod, specify a container, then you can begin training. A Pod is an instance on a GPU or multiple GPUs that you can use to run your training job. You also specify a Docker image like \\`winglian/axolotl-cloud:main-latest\\` that you want installed on your Pod. 1. Login to","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["837",{"pageContent":"\\[RunPod]\\(https://www.runpod.io/console/console/home) and deploy your Pod. 1. Select \\*\\*Deploy\\*\\*. 2. Select a GPU instance. 3. Specify the \\`winglian/axolotl-cloud:main-latest\\` image as your Template image. 4. Select your GPU count. 5. Select \\*\\*Deploy\\*\\*. Now that you have your Pod set up and running, connect to it over secure SSH. 2. Wait for the Pod to startup, then connect to it using secure SSH. 1. On your Pod page, select \\*\\*Connect\\*\\*. 2. Copy the secure SSH string and paste it","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["838",{"pageContent":"into your terminal on your machine. \\`\\`\\`bash ssh @ -p -i string \\`\\`\\` Follow the on-screen prompts to SSH into your Pod. :::note You should use the SSH connection to your Pod as it is a persistent connection. The Web UI terminal shouldn't be relied on for long-running processes, as it will be disconnected after a period of inactivity. ::: With the Pod deployed and connected via SSH, we're ready to move on to preparing our dataset. ### Preparing the dataset The dataset you provide to your LLM","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["839",{"pageContent":"is crucial, as it's the data your model will learn from during fine-tuning. You can make your own dataset that will then be used to fine-tune your own model, or you can use a pre-made one. To continue, use either a \\[local dataset]\\(#using-a-local-dataset) or one \\[stored on Hugging Face]\\(#using-a-hugging-face-dataset). #### Using a local dataset To use a local dataset, you'll need to transfer it to your RunPod instance. You can do this using RunPod CLI to securely transfer files from your","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["840",{"pageContent":"local machine to the one hosted by RunPod. All Pods automatically come with \\`runpodctl\\` installed with a Pod-scoped API key. \\*\\*To send a file\\*\\* Run the following on the computer that has the file you want to send, enter the following command: \\`\\`\\`bash runpodctl send data.jsonl \\`\\`\\` \\`\\`\\`bash Sending 'data.jsonl' (5 B) Code is: 8338-galileo-collect-fidel On the other computer run runpodctl receive 8338-galileo-collect-fidel \\`\\`\\` \\*\\*To receive a file\\*\\* The following is an example","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["841",{"pageContent":"of a command you'd run on your RunPod machine. \\`\\`\\`bash runpodctl receive 8338-galileo-collect-fidel \\`\\`\\` The following is an example of an output. \\`\\`\\`bash Receiving 'data.jsonl' (5 B) Receiving (<-149.36.0.243:8692) data.jsonl 100% |████████████████████| ( 5/ 5B, 0.040 kB/s) \\`\\`\\` Once the local dataset is transferred to your RunPod machine, we can proceed to updating requirements and preprocessing the data. #### Using a Hugging Face dataset If your dataset is stored on Hugging Face,","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["842",{"pageContent":"you can specify its path in the \\`lora.yaml\\` configuration file under the \\`datasets\\` key. Axolotl will automatically download the dataset during the preprocessing step. Review the \\[configuration file]\\(https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/config.qmd) in detail and make any adjustments to your file as needed. Now update your RunPod machine's requirement and preprocess your data. ### Updating requirements and preprocessing data Before you can start training,","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["843",{"pageContent":"you'll need to install the necessary dependencies and preprocess our dataset. :::note In some cases, your Pod will not contain the Axolotl repository. To add the required repository, run the following commands and then continue with the tutorial: \\`\\`\\`command git clone https://github.com/OpenAccess-AI-Collective/axolotl cd axolotl \\`\\`\\` ::: 1. Install the required packages by running the following commands: \\`\\`\\`command pip3 install packaging ninja pip3 install -e '.\\[flash-attn,deepspeed]'","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["844",{"pageContent":"\\`\\`\\` 2. Update the \\`lora.yml\\` configuration file with your dataset path and other training settings. You can use any of the examples in the \\`examples\\` folder as a starting point. 3. Preprocess your dataset by running: \\`\\`\\`command CUDA\\_VISIBLE\\_DEVICES=\"\" python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml \\`\\`\\` This step converts your dataset into a format that Axolotl can use for training. Having updated the requirements and preprocessed the data, we're now ready to","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["845",{"pageContent":"fine-tune the LLM. ### Fine-tuning the LLM With your environment set up and data preprocessed, you're ready to start fine-tuning the LLM. Run the following command to fine-tune the base model. \\`\\`\\`command accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml \\`\\`\\` This will start the training process using the settings specified in your \\`lora.yml\\` file. The training time will depend on factors like your model size, dataset size, and GPU type. Be prepared to wait a while,","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["846",{"pageContent":"especially for larger models and datasets. Once training is complete, we can move on to testing our fine-tuned model through inference. ### Inference Once training is complete, you can test your fine-tuned model using the inference script: \\`\\`\\`command accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml --lora\\_model\\_dir=\"./lora-out\" \\`\\`\\` This will allow you to interact with your model and see how it performs on new prompts. If you're satisfied with your model's","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["847",{"pageContent":"performance, you can merge the LoRA weights with the base model using the \\`merge\\_lora\\` script. ### Merge the model You will merge the base model with the LoRA weights using the \\`merge\\_lora\\` script. Run the following command: \\`\\`\\`command python3 -m axolotl.cli.merge\\_lora examples/openllama-3b/lora.yml \\ --lora\\_model\\_dir=\"./lora-out\" \\`\\`\\` This creates a standalone model that doesn't require LoRA layers for inference. ### Upload the model to Hugging Face Finally, you can share your","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["848",{"pageContent":"fine-tuned model with others by uploading it to Hugging Face. 1. Login to Hugging Face through the CLI: \\`\\`\\`command huggingface-cli login \\`\\`\\` 2. Create a new model repository on Hugging Face using \\`huggingface-cli\\`. \\`\\`\\`command huggingface-cli repo create your\\_model\\_name --type model \\`\\`\\` 3. Then, use the \\`huggingface-cli upload\\` command to upload your merged model to the repository. \\`\\`\\`command huggingface-cli upload your\\_model\\_name path\\_to\\_your\\_model \\`\\`\\` With our model","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["849",{"pageContent":"uploaded to Hugging Face, we've successfully completed the fine-tuning process and made our work available for others to use and build upon. ## Conclusion By following these steps and leveraging the power of Axolotl and RunPod, you can efficiently fine-tune LLMs to suit your specific use cases. The combination of Axolotl's user-friendly interface and RunPod's GPU resources makes the process more accessible and streamlined. Remember to explore the provided YAML examples to gain a deeper","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["850",{"pageContent":"understanding of the various parameters and make appropriate adjustments for your own projects. With practice and experimentation, you can unlock the full potential of fine-tuned LLMs and create powerful, customized AI models.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":1}}}}],["851",{"pageContent":"\\--- title: Run Fooocus in Jupyter Notebook description: \"Learn how to run Fooocus, an open-source image generating model, in a Jupyter Notebook and launch the Gradio-based interface in under 5 minutes, with minimal requirements of 4GB Nvidia GPU memory and 8GB system memory.\" sidebar\\_position: 3 --- ## Overview Fooocus is an open-source image generating model. In this tutorial, you'll run Fooocus in a Jupyter Notebook and then launch the Gradio-based interface to generate images. Time to","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-fooocus.md","loc":{"lines":{"from":1,"to":1}}}}],["852",{"pageContent":"complete: ~5 minutes ## Prerequisites The minimal requirement to run Fooocus is: - 4GB Nvidia GPU memory (4GB VRAM) - 8GB system memory (8GB RAM) ## RunPod infrastructure 1. Select \\*\\*Pods\\*\\* and choose \\*\\*+ GPU Pod\\*\\*. 2. Choose a GPU instance with at least 4GB VRAM and 8GB RAM by selecting \\*\\*Deploy\\*\\*. 3. Search for a template that includes \\*\\*Jupyter Notebook\\*\\* and select \\*\\*Deploy\\*\\*. - Select \\*\\*RunPod Pytorch 2\\*\\*. - Ensure \\*\\*Start Jupyter Notebook\\*\\* is selected. 4.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-fooocus.md","loc":{"lines":{"from":1,"to":1}}}}],["853",{"pageContent":"Select \\*\\*Choose\\*\\* and then \\*\\*Deploy\\*\\*. ## Run the notebook 1. Select \\*\\*Connect to Jupyter Lab\\*\\*. 2. In the Jupyter Lab file browser, select \\*\\*File > New > Notebook\\*\\*. 3. In the first cell, paste the following and then run the Notebook. \\`\\`\\`bash !pip install pygit2==1.12.2 !pip install opencv-python==4.9.0.80 %cd /workspace !git clone https://github.com/lllyasviel/Fooocus.git %cd /workspace/Fooocus !python entry\\_with\\_update.py --share \\`\\`\\` ## Launch UI Look for the line:","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-fooocus.md","loc":{"lines":{"from":1,"to":1}}}}],["854",{"pageContent":"\\`\\`\\`text App started successful. Use the app with .... \\`\\`\\` And select the link. ## Explore the model Explore and run the model.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-fooocus.md","loc":{"lines":{"from":1,"to":1}}}}],["855",{"pageContent":"\\--- title: Set up Ollama on your GPU Pod description: \"Learn how to set up Ollama, a powerful language model, on a GPU Pod using RunPod, and interact with it through HTTP API requests, allowing you to harness the power of GPU acceleration for your AI projects.\" sidebar\\_position: 4 --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; This tutorial will guide you through setting up \\[Ollama]\\(https://ollama.com), a powerful platform serving large language model, on a GPU Pod","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["856",{"pageContent":"using RunPod. Ollama makes it easy to run, create, and customize models. However, not everyone has access to the compute power needed to run these models. With RunPod, you can spin up and manage GPUs in the Cloud. RunPod offers templates with preinstalled libaries, which makes it quick to run Ollama. In the following tutorial, you'll set up a Pod on a GPU, install and serve the Ollama model, and interact with it on the CLI. ## Prerequisites The tutorial assumes you have a RunPod account with","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["857",{"pageContent":"credits. No other prior knowledge is needed to complete this tutorial. ## Step 1: Start a PyTorch Template on RunPod You will create a new Pod with the PyTorch template. In this step, you will set overrides to configure Ollama. 1. Log in to your \\[RunPod account]\\(https://www.runpod.io/console/pods) and choose \\*\\*+ GPU Pod\\*\\*. 2. Choose a GPU Pod like \\`A40\\`. 3. From the availble templates, select the lastet PyTorch template. 4. Select \\*\\*Customize Deployment\\*\\*. 1. Add the port \\`11434\\`","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["858",{"pageContent":"to the list of exposed ports. This port is used by Ollama for HTTP API requests. 2. Add the following environment variable to your Pod to allow Ollama to bind to the HTTP port: - Key: \\`OLLAMA\\_HOST\\` - Value: \\`0.0.0.0\\` 5. Select \\*\\*Set Overrides\\*\\*, \\*\\*Continue\\*\\*, then \\*\\*Deploy\\*\\*. This setting configures Ollama to listen on all network interfaces, enabling external access through the exposed port. For detailed instructions on setting environment variables, refer to the \\[Ollama FAQ","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["859",{"pageContent":"documentation]\\(https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux). Once the Pod is up and running, you'll have access to a terminal within the RunPod interface. ## Step 2: Install Ollama Now that your Pod is running, you can Log in to the web terminal. The web terminal is a powerful way to interact with your Pod. 1. Select \\*\\*Connect\\*\\* and choose \\*\\*Start Web Terminal\\*\\*. 2. Make note of the \\*\\*Username\\*\\* and \\*\\*Password\\*\\*, then select","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["860",{"pageContent":"\\*\\*Connect to Web Terminal\\*\\*. 3. Enter your username and password. 4. In the terminal of your newly created Pod, run the following command and send to the background: \\`\\`\\`bash (curl -fsSL https://ollama.com/install.sh | sh && ollama serve > ollama.log 2>&1) & \\`\\`\\` This command fetches the Ollama installation script and executes it, setting up Ollama on your Pod. The \\`ollama serve\\` part starts the Ollama server, making it ready to serve AI models. Now that your Ollama server is running","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["861",{"pageContent":"on your Pod, add a model. ## Step 3: Run an AI Model with Ollama To run an AI model using Ollama, pass the model name to the \\`ollama run\\` command: \\`\\`\\`bash ollama run \\[model name] # ollama run llama2 # ollama run mistral \\`\\`\\` Replace \\`\\[model name]\\` with the name of the AI model you wish to deploy. For a complete list of models, see the \\[Ollama Library]\\(https://ollama.com/library). This command pulls the model and runs it, making it accessible for inference. You can begin interacting","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["862",{"pageContent":"with the model directly from your web terminal. Optionally, you can set up an HTTP API request to interact with Ollama. This is covered in the \\[next step]\\(#step-4-interact-with-ollama-via-http-api). ## Step 4: Interact with Ollama via HTTP API With Ollama set up and running, you can now interact with it using HTTP API requests. In step 1.4, you configured Ollama to listen on all network interfaces. This means you can use your Pod as a server to receive requests. \\*\\*Get a list of models\\*\\* To","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["863",{"pageContent":"list the local models available in Ollama, you can use the following GET request: \\`\\`\\` curl https://{POD\\_ID}-11434.proxy.runpod.net/api/tags # curl https://cmko4ns22b84xo-11434.proxy.runpod.net/api/tags \\`\\`\\` Replace \\`\\[your-pod-id]\\` with your actual Pod Id. \\`\\`\\`json { \"models\": \\[ { \"name\": \"mistral:latest\", \"model\": \"mistral:latest\", \"modified\\_at\": \"2024-02-16T18:22:39.948000568Z\", \"size\": 4109865159, \"digest\": \"61e88e884507ba5e06c49b40e6226884b2a16e872382c2b44a42f2d119d804a5\",","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["864",{"pageContent":"\"details\": { \"parent\\_model\": \"\", \"format\": \"gguf\", \"family\": \"llama\", \"families\": \\[ \"llama\" ], \"parameter\\_size\": \"7B\", \"quantization\\_level\": \"Q4\\_0\" } } ] } \\`\\`\\` Getting a list of avaialbe models is great, but how do you send an HTTP request to your Pod? \\*\\*Make requests\\*\\* To make an HTTP request against your Pod, you can use the Ollama interface with your Pod Id. \\`\\`\\`command curl -X POST https://{POD\\_ID}-11434.proxy.runpod.net/api/generate -d '{ \"model\": \"mistral\", \"prompt\":\"Here is","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["865",{"pageContent":"a story about llamas eating grass\" }' \\`\\`\\` Replace \\`\\[your-pod-id]\\` with your actual Pod Id. Because port \\`11434\\` is exposed, you can make requests to your Pod using the \\`curl\\` command. For more information on constructing HTTP requests and other operations you can perform with the Ollama API, consult the \\[Ollama API documentation]\\(https://github.com/ollama/ollama/blob/main/docs/api.md). ## Additional considerations This tutorial provides a foundational understanding of setting up and","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["866",{"pageContent":"using Ollama on a GPU Pod with RunPod. - \\*\\*Port Configuration and documentation\\*\\*: For further details on exposing ports and the link structure, refer to the \\[RunPod documentation]\\(/pods/configuration/expose-ports). - \\*\\*Connect VSCode to RunPod\\*\\*: For information on connecting VSCode to RunPod, refer to the \\[How to Connect VSCode To RunPod]\\(https://blog.runpod.io/how-to-connect-vscode-to-runpod/). By following these steps, you can deploy AI models efficiently and interact with them","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["867",{"pageContent":"through HTTP API requests, harnessing the power of GPU acceleration for your AI projects.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":1}}}}],["868",{"pageContent":"\\--- title: Run your first Fast Stable Diffusion with Jupyter Notebook description: \"Deploy a Jupyter Notebook to RunPod and generate your first image with Stable Diffusion in just 20 minutes, requiring Hugging Face user access token, RunPod infrastructure, and basic familiarity with the platform.\" sidebar\\_position: 2 --- ## Overview By the end of this tutorial, you’ll have deployed a Jupyter Notebook to RunPod, deployed an instance of Stable Diffusion, and generated your first image. Time to","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["869",{"pageContent":"complete: ~20 minutes ## Prerequisites - \\[Hugging Face user access token]\\(https://huggingface.co/docs/hub/security-tokens) ## RunPod infrastructure - Select \\*\\*RunPod Fast Stable Diffusion\\*\\* - Choose 1x RTX A5000 or 1x RTX 3090 - Select \\*\\*Start Jupyter Notebook\\*\\* - Deploy. ## Run the notebook - Select \\*\\*RNPD-A1111.ipynb\\*\\* - Enter Hugging Face user access token - Select the model you want to run: - \\`v.1.5\\` | \\`v2-512\\` | \\`v2-768\\` ## Launch Automatic1111 on your pod - The cell","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["870",{"pageContent":"labeled \\*\\*Start Stable-Diffusion\\*\\* will launch your pod. - (optional) Provide login credentials for this instance. - Select the blue link ending in \\`.proxy.runpod.net\\` ## Explore Stable-Diffusion Now that your pod is up and running Stable-Diffusion. Explore and run the model.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["871",{"pageContent":"\\--- title: Run an Ollama Server on a RunPod CPU description: Learn to set up and run an Ollama server on RunPod CPU for inference with this step-by-step tutorial. --- In this guide, you will learn how to run an Ollama server on your RunPod CPU for inference. By the end of this tutorial, you will have a fully functioning Ollama server ready to handle requests. ## Setting up your Endpoint :::note Use a Network volume to attach to your Worker so that it can cache the LLM and decrease cold start","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":1,"to":1}}}}],["872",{"pageContent":"times. If you do not use a network volume, the Worker will have to download the model every time it spins back up, leading to increased latency and resource consumption. ::: To begin, you need to set up a new endpoint on RunPod. 1. Log in to your \\[RunPod account]\\(https://www.runpod.io/console/console/home). 2. Navigate to the \\*\\*Serverless\\*\\* section and select \\*\\*New Endpoint\\*\\*. 3. Choose \\*\\*CPU\\*\\* and provide a name for your Endpoint, for example 8 vCPUs 16 GB RAM. 4. Configure your","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":1,"to":1}}}}],["873",{"pageContent":"Worker settings according to your needs. 5. In the \\*\\*Container Image\\*\\* field, enter the \\`pooyaharatian/runpod-ollama:0.0.7\\` container image. 6. In the \\*\\*Container Start Command\\*\\* field, specify the \\[Ollama supported model]\\(https://ollama.com/library), such as \\`orca-mini\\`. 7. Allocate sufficient container disk space for your model. Typically, 20 GB should suffice for most models. 8. (optional) In \\*\\*Enviroment Variables\\*\\* set a new key to \\`OLLAMA\\_MODELS\\` and its value to","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":1,"to":1}}}}],["874",{"pageContent":"\\`/runpod-volume\\`. This will allow the model to be stored to your attached volume. 9. Click \\*\\*Deploy\\*\\* to initiate the setup. Your model will start downloading. Once the Worker is ready, proceed to the next step. ## Sending a Run request After your endpoint is deployed and the model is downloaded, you can send a run request to test the setup. 1. Go to the \\*\\*Requests\\*\\* section in the RunPod web UI. 2. In the input module, enter the following JSON object: \\`\\`\\`json { \"input\": {","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":1,"to":1}}}}],["875",{"pageContent":"\"method\\_name\": \"generate\", \"input\": { \"prompt\": \"why the sky is blue?\" } } } \\`\\`\\` 3. Select \\*\\*Run\\*\\* to execute the request. 4. In a few seconds, you will receive a response. For example: \\`\\`\\`json { \"delayTime\": 153, \"executionTime\": 4343, \"id\": \"c2cb6af5-c822-4950-bca9-5349288c001d-u1\", \"output\": { \"context\": \\[ \"omitted for brevity\" ], \"created\\_at\": \"2024-05-17T16:56:29.256938735Z\", \"done\": true, \"eval\\_count\": 118, \"eval\\_duration\": 807433000, \"load\\_duration\": 3403140284, \"model\":","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":1,"to":1}}}}],["876",{"pageContent":"\"orca-mini\", \"prompt\\_eval\\_count\": 46, \"prompt\\_eval\\_duration\": 38548000, \"response\": \"The sky appears blue because of a process called scattering. When sunlight enters the Earth's atmosphere, it encounters molecules of air such as nitrogen and oxygen. These molecules scatter the light in all directions, but they scatter the shorter wavelengths of light (such as violet and blue) more than the longer wavelengths (such as red). This creates a reddish-orange sky that is less intense on the","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":1,"to":1}}}}],["877",{"pageContent":"horizon than on the observer's position. As the sun gets lower in the sky, the amount of scattering increases and the sky appears to get brighter.\", \"total\\_duration\": 4249684714 }, \"status\": \"COMPLETED\" } \\`\\`\\` With your Endpoint set up, you can now integrate it into your application just like any other request. ## Conclusion In this tutorial, you have successfully set up and run an Ollama server on a RunPod CPU. Now you can handle inference requests using your deployed model. For further","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":1,"to":1}}}}],["878",{"pageContent":"exploration, check out the following resources: - \\[Runpod Ollama repository]\\(https://github.com/pooyahrtn/) - \\[RunPod Ollama container image]\\(https://hub.docker.com/r/pooyaharatian/runpod-ollama)","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":1,"to":1}}}}],["879",{"pageContent":"\\--- title: Generate images with SDXL Turbo description: \"Learn how to build a web application using RunPod's Serverless Workers and SDXL Turbo from Stability AI, a fast text-to-image model, and send requests to an Endpoint to generate images from text-based inputs.\" sidebar\\_position: 2 --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; When it comes to working with an AI image generator, the speed in which images are generated is often a compromise. RunPod's Serverless","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["880",{"pageContent":"Workers allows you to host \\[SDXL Turbo]\\(https://huggingface.co/stabilityai/sdxl-turbo) from Stability AI, which is a fast text-to-image model. In this tutorial, you'll build a web application, where you'll leverage RunPod's Serverless Worker and Endpoint to return an image from a text-based input. By the end of this tutorial, you'll have an understanding of running a Serverless Worker on RunPod and sending requests to an Endpoint to receive a response. You can proceed with the tutorial by","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["881",{"pageContent":"following the build steps outlined here or skip directly to \\[Deploy a Serverless Endpoint]\\(#deploy-a-serverless-endpoint) section. ## Prerequisites This section presumes you have an understanding of the terminal and can execute commands from your terminal. Before starting this tutorial, you'll need access to: ### RunPod To continue with this quick start, you'll need access to the following from RunPod: - RunPod account - RunPod API Key ### Docker To build your Docker image, you'll need access","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["882",{"pageContent":"to the following: - Docker installed - Docker account You can also use the prebuilt image from \\[runpod/sdxl-turbo]\\(https://hub.docker.com/r/runpod/sdxl-turbo). ### GitHub To clone the \\`worker-sdxl-turbo\\` repo, you'll need access to the following: - Git installed - Permissions to clone GitHub repos With the prerequisites covered, get started by building and pushing a Docker image to a container registry. ## Build and push your Docker image This step will walk you through building and pushing","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["883",{"pageContent":"your Docker image to your container registry. This is useful to building custom images for your use case. If you prefer, you can use the prebuilt image from \\[runpod/sdxl-turbo]\\(https://hub.docker.com/r/runpod/sdxl-turbo) instead of building your own. Building a Docker image allows you to specify the container when creating a Worker. The Docker image includes the \\[RunPod Handler]\\(https://github.com/runpod-workers/worker-sdxl-turbo/blob/main/src/handler.py) which is how you provide","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["884",{"pageContent":"instructions to Worker to perform some task. In this example, the Handler is responsible for taking a Job and returning a base 64 instance of the image. 1. Clone the \\[RunPod Worker SDXL Turbo]\\(https://github.com/runpod-workers/worker-sdxl-turbo) repository: \\`\\`\\`command gh repo clone runpod-workers/worker-sdxl-turbo \\`\\`\\` 2. Navigate to the root of the cloned repo: \\`\\`\\`command cd worker-sdxl-turbo \\`\\`\\` 3. Build the Docker image: \\`\\`\\`command docker build --tag /: . \\`\\`\\` 4. Push your","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["885",{"pageContent":"container registry: \\`\\`\\`command docker push /: \\`\\`\\` Now that you've pushed your container registry, you're ready to deploy your Serverless Endpoint to RunPod. ## Deploy a Serverless Endpoint The container you just built will run on the Worker you're creating. Here, you will configure and deploy the Endpoint. This will include the GPU and the storage needed for your Worker. This step will walk you through deploying a Serverless Endpoint to RunPod. 1. Log in to the \\[RunPod Serverless","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["886",{"pageContent":"console]\\(https://www.runpod.io/console/serverless). 2. Select \\*\\*+ New Endpoint\\*\\*. 3. Provide the following: 1. Endpoint name. 2. Select a GPU. 3. Configure the number of Workers. 4. (optional) Select \\*\\*FlashBoot\\*\\*. 5. (optional) Select a template. 6. Enter the name of your Docker image. - For example, \\`runpod/sdxl-turbo:latest\\`. 7. Specify enough memory for your Docker image. 4. Select \\*\\*Deploy\\*\\*. Now, let's send a request to your Endpoint. ## Send a request Now that our Endpoint","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["887",{"pageContent":"is deployed, you can begin interacting with and integrating it into an application. Before writing the logic into the applicaiton, ensure that you can interact with the Endpoint by sending a request. Run the following command: \\`\\`\\`bash curl -X POST \"https://api.runpod.ai/v2/${YOUR\\_ENDPOINT}/runsync\" \\ -H \"accept: application/json\" \\ -H \"content-type: application/json\" \\ -H \"authorization: ${YOUR\\_API\\_KEY}\" \\ -d '{ \"input\": { \"prompt\": \"${YOUR\\_PROMPT}\", \"num\\_inference\\_steps\": 25,","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["888",{"pageContent":"\"refiner\\_inference\\_steps\": 50, \"width\": 1024, \"height\": 1024, \"guidance\\_scale\": 7.5, \"strength\": 0.3, \"seed\": null, \"num\\_images\": 1 } }' \\`\\`\\` \\`\\`\\`json { \"delayTime\": 168, \"executionTime\": 251, \"id\": \"sync-fa542d19-92b2-47d0-8e58-c01878f0365d-u1\", \"output\": \"BASE\\_64\", \"status\": \"COMPLETED\" } \\`\\`\\` Export your variable names in your terminal session or replace them in line: - \\`YOUR\\_ENDPOINT\\`: The name of your Endpoint. - \\`YOUD\\_API\\_KEY\\`: The API Key required with read and write","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["889",{"pageContent":"access. - \\`YOUR\\_PROMPT\\`: The custom prompt passed to the model. You should se the output. The status will return \\`PENDING\\`; but quickly change to \\`COMPLETED\\` if you query the Job Id. ## Integrate into your application Now, let's create a web application that can take advantage of writing a prompt and generate an image based on that prompt. While these steps are specific to JavaScript, you can make requests against your Endpoint in any language of your choice. To do that, you'll create two","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["890",{"pageContent":"files: - \\`index.html\\`: The frontend to your web application. - \\`script.js\\`: The backend which handles the logic behind getting the prompt and the call to the Serverless Endpoint. The HTML file (\\`index.html\\`) sets up a user interface with an input box for the prompt and a button to trigger the image generation. \\`\\`\\`html <!-- index.html --> # RunPod AI Image Generator Enter your image prompt Generate Image\\`\\`\\` The JavaScript file (\\`script.js\\`) contains the \\`generateImage\\` function.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["891",{"pageContent":"This function reads the user's input, makes a POST request to the RunPod serverless endpoint, and handles the response. The server's response is expected to contain the base64-encoded image, which is then displayed on the webpage. \\`\\`\\`javascript // script.js async function generateImage() { const prompt = document.getElementById(\"promptInput\").value; if (!prompt) { alert(\"Please enter a prompt!\"); return; } const options = { method: \"POST\", headers: { accept: \"application/json\",","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["892",{"pageContent":"\"content-type\": \"application/json\", // Replace with your actual API key authorization: \"Bearer ${process.env.REACT\\_APP\\_AUTH\\_TOKEN}\", }, body: JSON.stringify({ input: { prompt: prompt, num\\_inference\\_steps: 25, width: 1024, height: 1024, guidance\\_scale: 7.5, seed: null, num\\_images: 1, }, }), }; try { const response = await fetch( // Replace with your actual Endpoint Id \"https://api.runpod.ai/v2/${process.env.REACT\\_APP\\_ENDPOINT\\_ID}/runsync\", options, ); const data = await response.json();","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["893",{"pageContent":"if (data && data.output) { const imageBase64 = data.output; const imageUrl = \\`data:image/jpeg;base64,${imageBase64}\\`; document.getElementById(\"imageResult\").innerHTML = \\`![Generated Image](${imageUrl})\\`; } else { alert(\"Failed to generate image\"); } } catch (error) { console.error(\"Error:\", error); alert(\"Error generating image\"); } } \\`\\`\\` 1. Replace \\`${process.env.REACT\\_APP\\_AUTH\\_TOKEN}\\` with your actual API key. 2. Replace \\`${process.env.REACT\\_APP\\_ENDPOINT\\_ID}\\` with your","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["894",{"pageContent":"specific Endpoint. 3. Open \\`index.html\\` in a web browser, enter a prompt, and select \\*\\*Generate Image\\*\\* to see the result. This web application serves as a basic example of how to interact with your RunPod serverless endpoint from a client-side application. It can be expanded or modified to fit more complex use cases. ## Run a server You can run a server through Python or by opening the \\`index.html\\` page in your browser. Run the following command to start a server locally using Python.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["895",{"pageContent":"\\`\\`\\`command python -m http.server 8000 \\`\\`\\` \\*\\*Open the File in a Browser\\*\\* Open the \\`index.html\\` file directly in your web browser. 1. Navigate to the folder where your \\`index.html\\` file is located. 2. Right-click on the file and choose \\*\\*Open with\\*\\* and select your preferred web browser. - Alternatively, you can drag and drop the \\`index.html\\` file into an open browser window. - The URL will look something like \\`file:///path/to/your/index.html\\`.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":1}}}}],["896",{"pageContent":"\\--- title: Run Google's Gemma model sidebar\\_position: 5 description: \"Learn how to deploy Google's Gemma model on RunPod's vLLM Worker and create a Serverless Endpoint, then interact with the model using OpenAI APIs and Python.\" --- This tutorial walks you through running Google's Gemma model using RunPod's vLLM Worker. Throughout this tutorial, you'll learn to set up a Serverless Endpoint with a gated large language model (LLM). ## Prerequisites Before diving into the deployment process,","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["897",{"pageContent":"gather the necessary tokens and accepting Google's terms. This step ensures that you have access to the model and are in compliance with usage policies. - \\[Hugging Face access token]\\(https://huggingface.co/settings/tokens) - \\[Accepting Google's terms of service]\\(https://huggingface.co/google/gemma-7b) The next section will guide you through setting up your Serverless Endpoint with RunPod. ## Get started To begin, we'll deploy a vLLM Worker as a Serverless Endpoint. RunPod simplifies the","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["898",{"pageContent":"process of running large language models, offering an alternative to the more complex Docker and Kubernetes deployment methods. Follow these steps in the RunPod Serverless console to create your Endpoint. 1. Log in to the \\[RunPod Serverless console]\\(https://www.runpod.io/console/serverless). 2. Select \\*\\*+ New Endpoint\\*\\*. 3. Provide the following: 1. Endpoint name. 2. Select a GPU. 3. Configure the number of Workers. 4. (optional) Select \\*\\*FlashBoot\\*\\*. 5. Enter the vLLM Worker image:","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["899",{"pageContent":"\\`runpod/worker-vllm:stable-cuda11.8.0\\` or \\`runpod/worker-vllm:stable-cuda12.1.0\\`. 6. Specify enough storage for your model. 7. Add the following environment variables: 1. \\`MODEL\\_NAME\\`: \\`google/gemma-7b-it\\`. 2. \\`HF\\_TOKEN\\`: your Hugging Face API token for private models. 4. Select \\*\\*Deploy\\*\\*. Once the Endpoint initializes, you can send a request to your \\[Endpoint]\\(/serverless/endpoints/get-started). You've now successfully deployed your model, a significant milestone in utilizing","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["900",{"pageContent":"Google's Gemma model. As we move forward, the next section will focus on interacting with your model. ## Interact with your model With the Endpoint up and running, it's time to leverage its capabilities by sending requests to interact with the model. This section demonstrates how to use OpenAI APIs to communicate with your model. In this example, you'll create a Python chat bot using the \\`OpenAI\\` library; however, you can use any programming language and any library that supports HTTP","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["901",{"pageContent":"requests. Here's how to get started: Use the \\`OpenAI\\` class to interact with the model. The \\`OpenAI\\` class takes the following parameters: - \\`base\\_url\\`: The base URL of the Serverless Endpoint. - \\`api\\_key\\`: Your RunPod API key. \\`\\`\\`python from openai import OpenAI import os client = OpenAI( base\\_url=os.environ.get(\"RUNPOD\\_BASE\\_URL\"), api\\_key=os.environ.get(\"RUNPOD\\_API\\_KEY\"), ) \\`\\`\\` :::note Set your environment variables \\`RUNPOD\\_BASE\\_URL\\` and \\`RUNPOD\\_API\\_KEY\\` to your","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["902",{"pageContent":"RunPod API key and base URL. Your \\`RUNPOD\\_BASE\\_URL\\` will be in the form of: \\`\\`\\`bash https://api.runpod.ai/v2/${RUNPOD\\_ENDPOINT\\_ID}/openai/v1 \\`\\`\\` Where \\`${RUNPOD\\_ENDPOINT\\_ID}\\` is the ID of your Serverless Endpoint. ::: Next, you can use the \\`client\\` to interact with the model. For example, you can use the \\`chat.completions.create\\` method to generate a response from the model. Provide the following parameters to the \\`chat.completions.create\\` method: - \\`model\\`: \\`The model","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["903",{"pageContent":"name\\`. - \\`messages\\`: A list of messages to send to the model. - \\`max\\_tokens\\`: The maximum number of tokens to generate. - \\`temperature\\`: The randomness of the generated text. - \\`top\\_p\\`: The cumulative probability of the generated text. - \\`max\\_tokens\\`: The maximum number of tokens to generate. \\`\\`\\`python messages = \\[{\"role\": \"assistant\", \"content\": \"Hello, I'm your assistant. How can I help you today?\"}] def display\\_chat\\_history(messages): for message in messages:","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["904",{"pageContent":"print(f\"{message\\['role'].capitalize()}: {message\\['content']}\") def get\\_assistant\\_response(messages): r = client.chat.completions.create( model=\"google/gemma-7b-it\", messages=\\[{\"role\": m\\[\"role\"], \"content\": m\\[\"content\"]} for m in messages], temperature=.7, top\\_p=.8, max\\_tokens=100, ) response = r.choices\\[0].message.content return response while True: display\\_chat\\_history(messages) prompt = input(\"User: \") messages.append({\"role\": \"user\", \"content\": prompt}) response =","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["905",{"pageContent":"get\\_assistant\\_response(messages) messages.append({\"role\": \"assistant\", \"content\": response}) \\`\\`\\` Congratulations! You've successfully set up a Serverless Endpoint and interacted with Google's Gemma model. This tutorial has shown you the essentials of deploying a model on RunPod and creating a simple application to communicate with it. You've taken important steps towards integrating large language models into your projects.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":1}}}}],["906",{"pageContent":"\\--- title: \"Run your first serverless endpoint with Stable Diffusion\" description: \"Learn how to use RunPod's Stable Diffusion v1 inference endpoint to generate images, including setting up your serverless worker, starting a job, checking job status, and retrieving results.\" sidebar\\_position: 1 --- :::note Before we begin, ensure you have a RunPod API key, available under your user settings. This key is crucial for identification and billing purposes. Keep it secure! Also, remember to retrieve","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["907",{"pageContent":"your results via the status endpoint within 30 minutes, as your inputs and outputs are not stored longer than this for privacy protection. ::: ### Overview In this section, we'll explore how RunPod's API works. It's asynchronous, meaning that when you send a request, you get a job ID back almost instantly. Next, we'll show you how to use this job ID to check the status and retrieve your results. Let's dive into an example using the Stable Diffusion v1 inference endpoint. ## Create a serverless","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["908",{"pageContent":"worker First, let's set up your serverless worker. Begin by selecting \\*\\*Quick Deploy\\*\\* on the RunPod interface. Then choose \\*\\*Start\\*\\* from the \\*\\*Stable Diffusion v1.5\\*\\* options. Pick a GPU, say a 24 GB GPU, and click \\*\\*Deploy\\*\\*. Here’s an example endpoint you might use: \\`https://api.runpod.ai/v2/{ID}/runsync\\` ### Start Your Job Now, to initiate a job, you'll make a request like the one shown below. This sends your parameters to the API and starts the process. \\`\\`\\`curl curl -X","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["909",{"pageContent":"POST https://api.runpod.ai/v2/{ID}/run \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer \\[Your API Key]' \\ -d '{\"input\": {\"prompt\": \"A cute fluffy white dog in the style of a Pixar animation 3D drawing.\"}}' \\`\\`\\` Upon doing this, you'll receive a response like this, containing your unique job ID: \\`\\`\\`json { \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\", \"status\": \"IN\\_QUEUE\" } \\`\\`\\` ### Check the Status of Your Job Since your initial response doesn't include the output, a","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["910",{"pageContent":"subsequent call is necessary. Use your job ID to check the job's status as follows: \\`\\`\\`curl curl https://api.runpod.ai/v2/{ID}/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer \\[Your API Key]' \\`\\`\\` If your job is still processing, the response will indicate that. Here's an example: \\`\\`\\`json { \"delayTime\": 2624, \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\", \"input\": { \"prompt\": \"A cute fluffy white dog in the style of a Pixar","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["911",{"pageContent":"animation 3D drawing.\" }, \"status\": \"IN\\_PROGRESS\" } \\`\\`\\` ### Get Completed Job Status Once your job is complete, you'll receive a final response like this: \\`\\`\\`json { \"delayTime\": 17158, \"executionTime\": 4633, \"id\": \"fb5a249d-12c7-48e5-a0e4-b813c3381262-22\", \"output\": \\[ { \"image\": \"base64image\", \"seed\": 40264 } ], \"status\": \"COMPLETED\" } \\`\\`\\` To save the output, use the following command: \\`\\`\\`json curl https://api.runpod.ai/v2/{ID}/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \\ -H","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["912",{"pageContent":"'Content-Type: application/json' \\ -H 'Authorization: Bearer \\[Your API Key]' | jq . > output.json \\`\\`\\` :::note Remember, you have up to 1 hour to retrieve your results via the status endpoint for privacy reasons. ::: ### Get Your Results Finally, to view your results, decode the base64 image from the output. Here's how you can do it in Python: \\`\\`\\`python import json import base64 def decode\\_and\\_save\\_image(json\\_file\\_path, output\\_image\\_path): try: # Reading the JSON file with","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["913",{"pageContent":"open(json\\_file\\_path, \"r\") as file: data = json.load(file) # Extracting the base64 encoded image data base64\\_image = data\\[\"output\"]\\[0]\\[\"image\"] # Decode the Base64 string decoded\\_image\\_data = base64.b64decode(base64\\_image) # Writing the decoded data to an image file with open(output\\_image\\_path, \"wb\") as image\\_file: image\\_file.write(decoded\\_image\\_data) print(f\"Image successfully decoded and saved as '{output\\_image\\_path}'.\") except FileNotFoundError: print( \"File not found. Please","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["914",{"pageContent":"ensure the JSON file exists in the specified path.\" ) except KeyError as e: print(f\"Error in JSON structure: {e}\") except Exception as e: print(f\"An error occurred: {e}\") # Usage json\\_file\\_path = \"output.json\" # Path to your JSON file output\\_image\\_path = \"decoded\\_image.png\" # Desired path for the output image decode\\_and\\_save\\_image(json\\_file\\_path, output\\_image\\_path) \\`\\`\\` Congratulations! You've now successfully used RunPod's Stable Diffusion API to generate images.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["915",{"pageContent":"!\\[]\\(decoded\\_image.png)","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":1}}}}],["916",{"pageContent":"\\# Website This website is built using \\[Docusaurus]\\(https://docusaurus.io/), a modern static website generator. ### Installation \\`\\`\\` $ yarn \\`\\`\\` ### Local Development \\`\\`\\` $ yarn start \\`\\`\\` This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server. ### Build \\`\\`\\` $ yarn build \\`\\`\\` This command generates static content into the \\`build\\` directory and can be served using any static contents","metadata":{"source":"/runpod-docs/README.md","loc":{"lines":{"from":1,"to":1}}}}],["917",{"pageContent":"hosting service. ### Deployment Using SSH: \\`\\`\\` $ USE\\_SSH=true yarn deploy \\`\\`\\` Not using SSH: \\`\\`\\` $ GIT\\_USER= yarn deploy \\`\\`\\` If you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the \\`gh-pages\\` branch. ## Linting To lint a specific folder or file, run: \\`\\`\\`command vale path/to/docs/ # or vale path/to/\\*.md \\`\\`\\` To lint the entire repo, run: \\`\\`\\`command yarn lint \\`\\`\\` ## Format Python code examples Install","metadata":{"source":"/runpod-docs/README.md","loc":{"lines":{"from":1,"to":1}}}}],["918",{"pageContent":"\\`blacken-docs\\`. \\`\\`\\`bash python -m pip install blacken-docs \\`\\`\\` Run the formatter. \\`\\`\\`bash git ls-files -z -- '\\*.md' | xargs -0 blacken-docs \\`\\`\\`","metadata":{"source":"/runpod-docs/README.md","loc":{"lines":{"from":1,"to":1}}}}]]