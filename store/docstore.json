[["0",{"pageContent":"---\ntitle: \"API Endpoints\"\ndescription: \"Unlock the power of RunPod's API Endpoints, manage models without managing pods, and retrieve results via the status endpoint within 30 minutes for privacy protection; rate limits enforced per user.\"\nsidebar_position: 1\n---\n\n:::note\n\nYou will need a RunPod API key which can be generated under your user settings.\nThis API key will identify you for billing purposes, so guard it well!\nYou must retrieve your results via the status endpoint within 30 minutes.\nWe don't keep your inputs or outputs longer than that to protect your privacy!\n\n:::\n\nAPI Endpoints are Endpoints managed by RunPod that you can use to interact with your favorite models without managing the pods yourself.\nThese Endpoints are available to all users.","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":1,"to":17}}}}],["1",{"pageContent":"Overview\n\nThe API Endpoint implementation works asynchronously as well as synchronous.\n\nLet's take a look at the differences between the two different implementations.\n\n### Asynchronous Endpoints\n\nAsynchronous endpoints are useful for long-running jobs that you don't want to wait for. You can submit a job and then check back later to see if it's done.\nWhen you fire an Asynchronous request with the API Endpoint, your input parameters are sent to our endpoint and you immediately get a response with a unique job ID.\nYou can then query the response by passing the job ID to the status endpoint. The status endpoint will give you the job results when completed.\n\n### Synchronous Endpoints\n\nSynchronous endpoints are useful for short-running jobs that you want to wait for.\nYou can submit a job and get the results back immediately.\n\nLet's take the Stable Diffusion v1 inference endpoint, for example.","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":19,"to":36}}}}],["2",{"pageContent":"Start your job\n\nYou would first make a request like the following (remember to replace the \"xxxxxx\"s with your real API key:\n\n```curl\ncurl -X POST https://api.runpod.ai/v2/stable-diffusion-v1/run \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' \\\n    -d '{\"input\": {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}}'\n```\n\nYou would get an immediate response that looks like this:\n\n```json\n{\n  \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\",\n  \"status\": \"IN_QUEUE\"\n}\n```\n\nIn this example, your job ID would be \"c80ffee4-f315-4e25-a146-0f3d98cf024b\".\nYou get a new one for each job, and it is a unique identifier for your job.","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":38,"to":59}}}}],["3",{"pageContent":"Check the status of your job\n\nYou haven't gotten any output, so you must make an additional call to the status endpoint after some time. Your status endpoint uses the job ID to route to the correct job status. In this case, the status endpoint is\n\n```command\nhttps://api.runpod.ai/v1/stable-diffusion-v1/status/c80ffee4-f315-4e25-a146-0f3d98cf024b\n```\n\nNote how the last part of the URL is your job ID. You could request that endpoint like so.\nRemember to use your API key for this request too!\n\n```curl\ncurl https://api.runpod.ai/v2/stable-diffusion-v1/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":61,"to":75}}}}],["4",{"pageContent":"If your job hasn't been completed, you may get something that looks like this back:\n\n```json\n{\n  \"delayTime\": 2624,\n  \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\",\n  \"input\": {\n    \"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"\n  },\n  \"status\": \"IN_PROGRESS\"\n}\n```\n\nThis means to wait a bit longer before you query the status endpoint again.","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":78,"to":91}}}}],["5",{"pageContent":"Get completed job status\n\nEventually, you will get the final results of your job. They would look something like this:\n\n```json\n{\n  \"delayTime\": 123456, // (milliseconds) time in queue\n  \"executionTime\": 1234, // (milliseconds) time it took to complete the job\n  \"gpu\": \"24\", // gpu type used to run the job\n  \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\",\n  \"input\": {\n    \"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"\n  },\n  \"output\": [\n    {\n      \"image\": \"https://job.results1\",\n      \"seed\": 1\n    },\n    {\n      \"image\": \"https://job.results2\",\n      \"seed\": 2\n    }\n  ],\n  \"status\": \"COMPLETED\"\n}\n```\n\n:::note\n\nYou must retrieve your results via the status endpoint within 1 hour. We do not keep your inputs or outputs longer than that to protect your privacy!\n\n:::","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":93,"to":124}}}}],["6",{"pageContent":"Get your stuff\n\nNote how you don't get the images directly in the output. The output contains the URLs to the cloud storage that will let you download each image.\n\nYou've successfully generated your first images with our Stable Diffusion API!\n\n### Rate Limit\n\nRate limits are enforced on a per-user basis.\nIf you exceed the rate limit, you will receive a `429` error code.\n\n`/run` - 1000 requests every 10s.\n\n`/runsync` - 2000 requests every 10s.","metadata":{"source":"/runpod-docs/docs/api/api-endpoints.md","loc":{"lines":{"from":126,"to":139}}}}],["7",{"pageContent":"---\ntitle: \"Overview\"\nslug: \"overview\"\ndescription: \"Create custom APIs for specific use cases with RunPod's serverless support. Bring your own container image and let us handle scaling and other aspects. Convert templates to API endpoints and invoke APIs using the 'run' endpoint, with asynchronous results tracked through the 'status' endpoint.\"\nsidebar_position: 1\n---\n\nYou can create Custom APIs for your specific use cases.\nIf you have a custom use case, you can use RunPod's custom API support to stand up your serverless API.\nYou can bring your own container image, and RunPod will handle the scaling and other aspects.\n\nTo create a custom API, you can navigate to the RunPod Serverless console and click \"New Template\" to add your container image. Once the template is created, you can convert it into an API endpoint by navigating to the APIs section and clicking \"New API\".","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_overview.md","loc":{"lines":{"from":1,"to":12}}}}],["8",{"pageContent":"Please note that running 1 minimum worker is great for debugging purposes, but you will be charged for that worker whether or not you are making requests to your endpoint.\n\nOnce everything is set up, you can invoke your API using the \"run\" endpoint on your API dashboard. RunPod services are currently asynchronous, so you must use the \"status\" endpoint to get the status/results of each run using the ID present in the run response payload.","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_overview.md","loc":{"lines":{"from":14,"to":16}}}}],["9",{"pageContent":"---\ntitle: \"Using Your API\"\nslug: \"using-your-api-copy\"\nexcerpt: \"Okay! Now you have everything set up, but how do you use it?\"\nhidden: false\nmetadata: \nimage: []\nrobots: \"index\"\ncreatedAt: \"Thu Jul 27 2023 10:39:12 GMT+0000 (Coordinated Universal Time)\"\nupdatedAt: \"Fri Oct 27 2023 13:53:56 GMT+0000 (Coordinated Universal Time)\"\ndescription: \"Invoke the RunPod API using the 'run' endpoint, which offers synchronous and asynchronous run mechanisms, and track job status using the 'status' endpoint.\"\n---\n\nOnce everything above is configured, you will be able to invoke your API using the \"run\" endpoint on your API dashboard. Our services are currently asynchronous, so you must use the \"status\" endpoint to get the status/results of each run using the ID present in the run response payload. You can also pass in a webhook URL when invoking \"run\" within the JSON body.","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":1,"to":14}}}}],["10",{"pageContent":"Our own APIs are built using the same tools, so you can take a look at the RunPod API overview. The only difference is that your custom API endpoint only accepts requests using your own account's API key, not any RunPod API key.\n\nWe offer two different kinds of run mechanisms: synchronous responses and asynchronous responses.","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":16,"to":18}}}}],["11",{"pageContent":"Running your API","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":20,"to":20}}}}],["12",{"pageContent":"/runsync\n\n<!-- dprint-ignore-start -->\n```curl cURL\ncurl -X POST https://api.runpod.ai/v2/<your-api-id>/runsync \\\n-H 'Content-Type: application/json'                             \\\n-H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'    \\\n-d '{\"input\": {<your-input-json}}'\n```\n```python\n# this requires the installation of runpod-python\n# with `pip install runpod-python` beforehand\n\nimport runpod\n\nrunpod.api_key = \"xxxxxxxxxxxxxxxxxxxxxx\"  # you can find this in settings\n\nendpoint = runpod.Endpoint(\"ENDPOINT_ID\")\n\nrun_request = endpoint.run_sync({\"your_model_input_key\": \"your_model_input_value\"})\n```\n<!-- dprint-ignore-end -->\n\nhere's a possible example request (taken from our stable diffusion image)","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":22,"to":45}}}}],["13",{"pageContent":"<!-- dprint-ignore-start -->\n```curl cURL\ncurl -X POST https://api.runpod.ai/v2/<your-api-id>/runsync \\\n-H 'Content-Type: application/json'                             \\\n-H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'    \\\n-d '{\"input\": {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}}'\n```\n```python\n# this requires the installation of runpod-python\n# with `pip install runpod-python` beforehand\n\nimport runpod\n\nrunpod.api_key = \"xxxxxxxxxxxxxxxxxxxxxx\"  # you can find this in settings\n\nendpoint = runpod.Endpoint(\"ENDPOINT_ID\")\n\nrun_request = endpoint.run_sync(\n    {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}\n)\n\nprint(run_request)\n```\n<!-- dprint-ignore-end -->\n\nthis should give a direct response if the code runs for \\< 90 seconds, or else it'll give a status response (which you can see below)\n\n**Sample response**","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":47,"to":74}}}}],["14",{"pageContent":"```json\n{\n  \"delayTime\": 123456, // (milliseconds) time in queue\n  \"executionTime\": 1234, // (milliseconds) time it took to complete the job\n  \"gpu\": \"24\", // gpu type used to run the job\n  \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\",\n  \"input\": {\n    \"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"\n  },\n  \"output\": [\n    {\n      \"image\": \"https://job.results1\",\n      \"seed\": 1\n    },\n    {\n      \"image\": \"https://job.results2\",\n      \"seed\": 2\n    }\n  ],\n  \"status\": \"COMPLETED\"\n}\n```","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":76,"to":97}}}}],["15",{"pageContent":"/run\n\n<!-- dprint-ignore-start -->\n```curl cURL\ncurl -X POST https://api.runpod.ai/v2/<your-api-id>/run \\\n-H 'Content-Type: application/json'                             \\\n-H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'    \\\n-d '{\"input\": {<your-input-json}}'\n```\n```python\n# this requires the installation of runpod-python\n# with `pip install runpod-python` beforehand\n\nimport runpod\n\nrunpod.api_key = \"xxxxxxxxxxxxxxxxxxxxxx\"  # you can find this in settings\n\nendpoint = runpod.Endpoint(\"ENDPOINT_ID\")\n\nrun_request = endpoint.run({\"your_model_input_key\": \"your_model_input_value\"})\n\nprint(run_request.status())\n```\n<!-- dprint-ignore-end -->\n\nhere's a possible example request","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":99,"to":124}}}}],["16",{"pageContent":"<!-- dprint-ignore-start -->\n```curl cURL\ncurl -X POST https://api.runpod.ai/v2/<your-api-id>/run \\\n-H 'Content-Type: application/json'                             \\\n-H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'    \\\n-d '{\"input\": {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}}'\n```\n```python\n# this requires the installation of runpod-python\n# with `pip install runpod-python` beforehand\n\nimport runpod\n\nrunpod.api_key = \"xxxxxxxxxxxxxxxxxxxxxx\"  # you can find this in settings\n\nendpoint = runpod.Endpoint(\"ENDPOINT_ID\")\n\nrun_request = endpoint.run(\n    {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}\n)\n\nprint(run_request.status())\n```\n<!-- dprint-ignore-end -->\n\nrunning your api via **/run** runs the code asynchronously, here's a sample response\n\n**sample response (for curl)**\n\n```json\n{\n  \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\",\n  \"status\": \"IN_QUEUE\"\n}\n```","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":126,"to":160}}}}],["17",{"pageContent":"/status\n\n**Sample request**\n\n<!-- dprint-ignore-start -->\n```Text cURL\ncurl https://api.runpod.ai/v2/<your-api-id>/status/<your-status-id>\n```\n```python Start a job and return a status\n# this requires the installation of runpod-python\n# with `pip install runpod-python` beforehand\n\nimport runpod\n\nrunpod.api_key = \"xxxxxxxxxxxxxxxxxxxxxx\"  # you can find this in settings\n\nendpoint = runpod.Endpoint(\"ENDPOINT_ID\")\n\nrun_request = endpoint.run(\n    {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}\n)\n\nprint(run_request.status())\n```\n```python Get the status of a running job\n# Prerequisite: Install runpod-python using `pip install runpod-python`\nimport runpod\n\nrunpod.api_key = \"xxxxxxxxxxxxxxxxxxxxxx\"  # Replace with your API key\nclient = runpod.endpoint.runner.RunPodClient()\n\n\njob = runpod.endpoint.Job(\n    endpoint_id=\"your_endpoint_id\", job_id=\"your_job_id\", client=client\n)\n\nprint(job.status())\n```\n<!-- dprint-ignore-end -->","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":162,"to":200}}}}],["18",{"pageContent":"**sample response (for job in progress)**\n\n```json JSON\n{\n  \"delayTime\": 2624,\n  \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\",\n  \"input\": {\n    \"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"\n  },\n  \"status\": \"IN_PROGRESS\"\n}","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":202,"to":212}}}}],["19",{"pageContent":"**sample response (for completed job)**\n\n```json JSON\n{\n  \"delayTime\": 123456, // (milliseconds) time in queue\n  \"executionTime\": 1234, // (milliseconds) time it took to complete the job\n  \"gpu\": \"24\", // gpu type used to run the job\n  \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\",\n  \"input\": {\n    \"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"\n  },\n  \"output\": [\n    {\n      \"image\": \"https://job.results1\",\n      \"seed\": 1\n    },\n    {\n      \"image\": \"https://job.results2\",\n      \"seed\": 2\n    }\n  ],\n  \"status\": \"COMPLETED\"\n}\n```","metadata":{"source":"/runpod-docs/docs/api/custom-apis/_using-your-api-copy.md","loc":{"lines":{"from":215,"to":238}}}}],["20",{"pageContent":"---\ntitle: Installing runpodctl\ndescription: \"Get started with runpodctl, an open-source CLI, to work with Pods and RunPod projects. Install and configure the tool, then verify the installation and API key setup to start using runpodctl.\"\nsidebar_position: 1\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nrunpodctl is an [open-source command-line interface (CLI)](https://github.com/runpod/runpodctl). You can use runpodctl to work with Pods and RunPod projects.\n\nWhen you create a Pod, it comes with runpodctl installed and configured with a Pod-scoped API key. You can also run runpodctl locally.\n\nTo install runpodctl on your local machine, run the appropriate command for your operating system.\n\n<Tabs>\n  <TabItem value=\"mac\" label=\"macOS\">\n\n```bash\nbrew install runpod/runpodctl/runpodctl\n```\n\n</TabItem>\n  <TabItem value=\"linux\" label=\"Linux\">\n\n```bash\nwget -qO- cli.runpod.net | sudo bash","metadata":{"source":"/runpod-docs/docs/cli/install-runpodctl.md","loc":{"lines":{"from":1,"to":27}}}}],["21",{"pageContent":"</TabItem>\n  <TabItem value=\"windows\" label=\"Windows\">\n\n```bash\nwget https://github.com/runpod/runpodctl/releases/download/v1.12.3/runpodctl-windows-amd64.exe -O runpodctl.exe\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/cli/install-runpodctl.md","loc":{"lines":{"from":30,"to":38}}}}],["22",{"pageContent":"Configuring runpodctl\n\nBefore you can use runpodctl, you must configure an API key. To create a new API key, complete the following steps:\n\n1. In the web interface, go to your [**Settings**](https://www.runpod.io/console/user/settings).\n2. Expand **API Keys** and click the **+ API Key** button.\n3. Select **Read** or **Read & Write** permissions.\n4. Click **Create**.\n\n:::note\n\nKeep your API key secret. Anyone with the key can gain full access to your account.\n\n:::\n\nNow that you've created an API key, run the following command to add it to runpodctl:\n\n```bash\nrunpodctl config --apiKey your-api-key\n```\n\nYou should see something similar to the following output:\n\n```bash\nsaved apiKey into config file: /Users/runpod/.runpod/config.toml\n```\n\nNow that you've configured an API key, check that runpodctl installed successfully. Run the following command:\n\n```bash\nrunpodctl version\n```\n\nYou should see which version is installed.\n\n```bash\nrunpodctl v1.13.0","metadata":{"source":"/runpod-docs/docs/cli/install-runpodctl.md","loc":{"lines":{"from":40,"to":76}}}}],["23",{"pageContent":"If at any point you need help with a command, you can use the `--help` flag to see documentation on the command you're running.\n\n```bash\nrunpodctl --help\n```","metadata":{"source":"/runpod-docs/docs/cli/install-runpodctl.md","loc":{"lines":{"from":79,"to":83}}}}],["24",{"pageContent":"---\ntitle: Get started\nsidebar_position: 2\ndescription: \"Get the IP address of your machine and deploy your code to the RunPod platform with this step-by-step tutorial. Learn how to set up a project environment, run a development server, and interact with your code using the RunPod API.\"\n---\n\nIn this tutorial, we'll explore how to get the IP address of the machine your code is running on and deploy your code to the RunPod platform.\nYou will get the IP address of your local machine, the development server, and the Serverless Endpoint's server.\n\nBy the end, you'll have a solid understanding of how to set up a project environment, interact with your code, and deploy your code to a Serverless Endpoint on the RunPod platform.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":1,"to":10}}}}],["25",{"pageContent":"While this project is scoped to getting the IP address of the machine your code is running on, you can use the RunPod platform to deploy any code you want.\nFor larger projects, bundling large packages a Docker image and making code changes requires multiple steps.\nWith a RunPod development server, you can make changes to your code and test them in a live environment without having to rebuild a Docker image or redeploy your code to the RunPod platform.\n\nThis tutorial takes advantage of making updates to your code and testing them in a live environment.\n\nLet's get started by setting up the project environment.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":12,"to":18}}}}],["26",{"pageContent":"Prerequisites\n\nBefore we begin, you'll need the following:\n\n- RunPod CLI\n- Python 3.8 or later\n\n## Step 1. Set up the project environment\n\nIn this first step, you'll set up your project environment using the RunPod CLI.\n\nSet your API key in the RunPod CLI configuration file.\n\n```bash\nrunpodctl config --apiKey $(RUNPOD_API_KEY)\n```\n\nNext, use the RunPod CLI `project create` command to create a new directory and files for your project.\n\n```bash\nrunpodctl project create\n```\n\nSelect the **Hello World** project and follow the prompts on the screen.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":20,"to":43}}}}],["27",{"pageContent":"Step 2. Write the code\n\nNext, you'll write the code to get the IP address of the machine your code is running on.\n\nUse `httpbin` to retrieve the IP address and test the code locally.\n\nChange directories to the project directory and open the `src/handler.py` file in your text editor.\n\n```bash\ncd my_ip\n```\n\nThe current code is boiler plate text.\nReplace the code with the following:\n\n```python\nimport runpod\nimport requests\n\ndef get_my_ip(job):\n    response = requests.get('https://httpbin.org/ip')\n    return response.json()['origin']\n\n\nrunpod.serverless.start({\"handler\": get_my_ip})\n```\n\nThis uses `httpbin` to get the IP address of the machine your code is running on.\n\nRun this code locally to get the IP address of your machine, for example:\n\n```bash\npython3 src/handler.py --test_input '{\"input\": {\"prompt\": \"\"}}'","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":45,"to":77}}}}],["28",{"pageContent":"```text\n--- Starting Serverless Worker |  Version 1.6.1 ---\nINFO   | test_input set, using test_input as job input.\nDEBUG  | Retrieved local job: {'input': {'prompt': ''}, 'id': 'local_test'}\nINFO   | local_test | Started.\nDEBUG  | local_test | Handler output: 174.21.174.xx\nDEBUG  | local_test | run_job return: {'output': '174.21.174.xx'}\nINFO   | Job local_test completed successfully.\nINFO   | Job result: {'output': '174.21.174.xx'}\nINFO   | Local testing complete, exiting.`\n```\n\nThis testing environment works for smaller projects, but for larger projects, you will want to use the RunPod CLI to deploy your code to run on the RunPod platform.\n\nIn the next step, you'll see how to deploy your code to the RunPod platform.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":80,"to":94}}}}],["29",{"pageContent":"Step 3. Run a development server\n\nNow let's run the code you've written using RunPod's development server.\nYou'll start a development server using the RunPod CLI `project dev` command.\n\nRunPod provides a development server that allows you to quickly make changes to your code and test these changes in a live environment.\nYou don't need to rebuild a Docker image or redeploy your code to the RunPod platform just because you made a small change or added a new dependency.\n\nTo run a development server, use the RunPod CLI `project dev` command and select a Network volume.\n\n```bash\nrunpodctl project dev","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":96,"to":107}}}}],["30",{"pageContent":"This starts a development server on a Pod.\nThe logs shows the status of your Pod as well as the port number your Pod is running on.\n\nThe development server watches for changes in your code and automatically updates the Pod with changes to your code and files like `requirements.txt`.\n\nWhen the Pod is running you should see the following logs:\n\n```text\nConnect to the API server at:\n[lug43rcd07ug47] >  https://lug43rcd07ug47-8080.proxy.runpod.net\n[lug43rcd07ug47]\n[lug43rcd07ug47] Synced venv to network volume\n[lug43rcd07ug47] --- Starting Serverless Worker |  Version 1.6.2 ---\n```\n\nThe `[lug43rcd07ug47]` is your Worker Id.\nThe `https://lug43rcd07ug47-8080.proxy.runpod.net` is the URL to access your Pod with the 8080 port exposed.\nYou can interact with this URL like you would any other Endpoint.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":110,"to":127}}}}],["31",{"pageContent":"Step 4. Interact with your code\n\nIn this step, you'll interact with your code by running a `curl` command to fetch the IP address from the development server.\nYou'll learn how to include dependencies in your project and how to use the RunPod API to run your code.\n\nYou might have noticed that the function to get an IP address uses a third-party dependency `requests`.\nThis means by default it's not included in Python or the RunPod environment.\n\nTo include this dependency, you need to add it to the `requirements.txt` file in the root of your project.\n\n```text\nrunpod\nrequests\n```\n\nWhen you save your file, notice that the development server automatically updates the Pod with the dependencies.\n\nDuring this sync, your Pod is unable to receive requests.\nWait until you see the following logs:\n\n```text\nRestarted API server with PID: 701\n--- Starting Serverless Worker |  Version 1.6.2 ---\nINFO   | Starting API server.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":129,"to":152}}}}],["32",{"pageContent":"Now you can interact with your code.\n\nWhile the Pod is still running, create a new terminal session and run the following command:\n\n```bash\ncurl -X 'POST' \\\n  'https://${YOUR_ENDPOINT}-8080.proxy.runpod.net/runsync' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"input\": {}\n}'\n```\n\nThis command uses the `runsync` method on the RunPod API to run your code synchronously.\n\nThe previous command returns a response:\n\n```text\n{\n  \"id\": \"test-9613c9be-3fed-401f-8cda-6b5f354417f8\",\n  \"status\": \"COMPLETED\",\n  \"output\": \"69.30.85.70\"\n}","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":155,"to":178}}}}],["33",{"pageContent":"The output is the IP address of the Pod your code is running on and not your local machine.\nEven though you're executing code locally, you can see that it's running on a Pod.\n\nNow, what if you wanted this function to run as a Serverless Endpoint?\nMeaning, you didn't want to keep the Pod running all the time.\nYou only wanted it to turn on when you sent a request to it.\n\nIn the next step, you'll learn to deploy your code to the Serverless platform and get the IP address of that machine.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":181,"to":188}}}}],["34",{"pageContent":"Step 5. Deploy your code\n\nNow that you've tested your code in the development environment, you'll deploy it to the RunPod platform using the RunPod CLI `project deploy` command.\nThis will make your code available as a [Serverless Endpoint](/serverless/endpoints/overview).\n\nStop the development server by pressing `Ctrl + C` in the terminal.\n\nTo deploy your code to the RunPod platform, use the RunPod CLI `project deploy` command.\n\n```bash\nrunpodctl project deploy\n```\n\nSelect your network volume and wait for your Endpoint to deploy.\n\nAfter deployment, you will see the following logs:\n\n```text\nThe following URLs are available:\n    - https://api.runpod.ai/v2/${YOUR_ENDPOINT}/runsync\n    - https://api.runpod.ai/v2/${YOUR_ENDPOINT}/run\n    - https://api.runpod.ai/v2/${YOUR_ENDPOINT}/health\n```\n\n:::note\n\nYou can follow the logs to see the status of your deployment.\nYou may notice that the logs show the Pod being created and then the Endpoint being created.\n\n:::","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":190,"to":219}}}}],["35",{"pageContent":"Step 6. Interact with your Endpoint\n\nFinally, you'll interact with your Endpoint by running a `curl` command to fetch the IP address from the deployed Serverless function.\nYou'll see how your code runs as expected and tested in the development environment.\n\nWhen the deployment completes, you can interact with your Endpoint as you would any other Endpoint.\n\nReplace the previous Endpoint URL and specify the new one and add your API key.\n\nThen, run the following command:\n\n```bash\ncurl -X 'POST' \\\n  'https://api.runpod.ai/v2/${YOUR_ENDPOINT}/runsync' \\\n  -H 'accept: application/json' \\\n  -H  'authorization: ${YOUR_API_KEY}' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"input\": {}\n}'\n```\n\nThe previous command returns a response:\n\n```text\n{\n  \"delayTime\": 249,\n  \"executionTime\": 88,\n  \"id\": \"sync-b2188a79-3f9f-4b99-b4d1-18273db3f428-u1\",\n  \"output\": \"69.30.85.69\",\n  \"status\": \"COMPLETED\"\n}\n```\n\nThe output is the IP address of the Pod your code is running on.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":221,"to":255}}}}],["36",{"pageContent":"Conclusion\n\nIn this tutorial, you've learned how to get the IP address of the machine your code is running on and deploy your code to the RunPod platform.\nYou've also learned how to set up a project environment, run a development server, and interact with your code using the RunPod API.\nWith this knowledge, you can now use this code as a Serverless Endpoint or continue developing your project, testing, and deploying it to the RunPod platform.","metadata":{"source":"/runpod-docs/docs/cli/projects/get-started.md","loc":{"lines":{"from":257,"to":261}}}}],["37",{"pageContent":"---\ntitle: Managing Projects\ndescription: Create and deploy projects on RunPod's infrastructure with ease, using commands like 'runpod project create' and 'runpodctl project deploy' to develop, deploy, and build your project as a serverless endpoint or Dockerfile.\nsidebar_position: 2\n---\n\nProjects enable you to develop and deploy endpoints entirely on RunPod's infrastructure.","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":1,"to":7}}}}],["38",{"pageContent":"Create a project\n\nA RunPod project is a folder with everything you need to run a development session on a Pod.\n\n1. To create a new project, run the following command.\n\n```command\nrunpod project create\n```\n\n2. Select a starter project. Starter projects include preliminary settings for different kinds of project environments, such as LLM or image diffusion development.\n3. Check the [base image](https://github.com/runpod/containers/tree/main/official-templates/base) for included dependencies.\n4. (Optional) If you need dependencies that are not included or added by your starter project, add them to the generated `requirements.txt` file.\n5. Save your changes.\n\nYou've customized your project, and now you're ready to run a development session.","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":9,"to":24}}}}],["39",{"pageContent":"Run a development session\n\nA development session is the active connection between your local environment and the project environment on your Pod. During a development session, local changes to your project propagate to the project environment in real time.\n\n1. To start a development session, run the following command.\n\n```command\nrunpodctl project dev\n```\n\n2. When you're done developing, press `ctrl` + `c` to end the session. Your Pod will terminate automatically when the session ends.\n\n:::tip\n\nYou can resume developing at any time by running `runpodctl project dev` again.\n\n:::\n\nNow that you've developed your project, you can deploy an endpoint directly to RunPod or build a Dockerfile to create a portable image.","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":26,"to":44}}}}],["40",{"pageContent":"Deploy a project\n\nWhen you deploy a project, RunPod creates a serverless endpoint with access to saved project data on your network volume.\n\nTo deploy a project, run the following command.\n\n```command\nrunpodctl project deploy\n```\n\nYour project is now deployed to a Serverless Endpoint.\nYou can interact with this Endpoint like you would any other Serverless Endpoint.\nFor more information, see [Endpoints](/serverless/endpoints/overview).\n\n## Build a project\n\nYou have the option to build your project instead of deploying it as an endpoint. When you build a project, RunPod emits a Dockerfile.\n\nTo build a project, run the following command.\n\n```command\nrunpodctl project build\n```\n\nYou can use the generated Dockerfile to build an image, then deploy the image to any API server.","metadata":{"source":"/runpod-docs/docs/cli/projects/manage-projects.md","loc":{"lines":{"from":46,"to":70}}}}],["41",{"pageContent":"---\ntitle: Overview\ndescription: \"Streamline your development process with RunPod's Dockerless workflow, enabling you to deploy and manage endpoints without Docker or container image management, perfect for rapid prototyping and testing.\"\nsidebar_position: 1\n---\n\nRunPod projects enable you to develop and deploy endpoints entirely on RunPod's infrastructure. That means you can get a worker up and running without knowing Docker or needing to structure handler code. This Dockerless workflow also streamlines the development process: you don't need to rebuild and push container images or edit your endpoint to use the new image each time you change your code.\n\nTo get started, see [Managing Projects](/cli/projects/manage-projects).","metadata":{"source":"/runpod-docs/docs/cli/projects/overview.md","loc":{"lines":{"from":1,"to":9}}}}],["42",{"pageContent":"---\ntitle: API keys\ndescription: \"Generate API keys with Read and Write or Read permission to authenticate requests to RunPod. Create and revoke keys from the console under Settings > API Keys.\"\nsidebar_position: 4\n---\n\nAPI keys authenticate requests to RunPod.\nYou can generate an API key with **Read and Write** permission or **Read** permission.\n\n## Generate\n\nTo create an API key:\n\n1. From the console, select **Settings**.\n2. Under **API Keys**, choose **+ API Keys**.\n3. Select the permission and choose **Create**.\n\n:::note\n\nOnce your API key is generated, keep it secure. Treat it like a password and avoid sharing it in insecure environments.\n\n:::\n\n## Revoke\n\nTo delete an API key:\n\n1. From the console, select **Settings**.\n2. Under **API Keys**, select the trash can icon and select **Yes**.","metadata":{"source":"/runpod-docs/docs/get-started/api-keys.md","loc":{"lines":{"from":1,"to":29}}}}],["43",{"pageContent":"---\ntitle: Billing information\ndescription: \"Learn about RunPod's billing and payment methods, including credit cards, crypto payments, and business invoicing options, with guidance on common issues and troubleshooting tips.\"\nsidebar_position: 2\n---\n\nAll billing, including per-hour compute and storage billing, is charged per minute.\n\nFor more information on billing questions, see [Billing FAQ](docs/references/faq/faq.md#billing).","metadata":{"source":"/runpod-docs/docs/get-started/billing-information.md","loc":{"lines":{"from":1,"to":9}}}}],["44",{"pageContent":"Payment methods\n\nRunPod accepts several payment methods for funding your account:\n\n1. **Credit Card**: You can use your credit card to fund your RunPod account. However, be aware that card declines are more common than you might think, and the reasons for them might not always be clear. If you're using a prepaid card, it's recommended to deposit in transactions of at least $100 to avoid unexpected blocks due to Stripe's minimums for prepaid cards.\n   For more information, review [cards accepted by Stripe](https://stripe.com/docs/payments/cards/supported-card-brands?ref=blog.runpod.io).\n\n<!-- [source](https://blog.runpod.io/how-to-manage-funding-your-runpod-account#my-card-keeps-getting-declined) -->","metadata":{"source":"/runpod-docs/docs/get-started/billing-information.md","loc":{"lines":{"from":11,"to":18}}}}],["45",{"pageContent":"1. **Crypto Payments**: RunPod also accepts crypto payments. It's recommended to set up a [crypto.com](https://crypto.com/?ref=blog.runpod.io) account and go through any KYC checks they may require ahead of time. This provides an alternate way of funding your account in case you run into issues with card payment.\n\n<!-- [source](https://blog.runpod.io/how-to-manage-funding-your-runpod-account#crypto-payments) -->\n\n3. **Business Invoicing**: For large transactions (over $5,000), RunPod offers business invoicing through ACH, credit card, Coinbase, and local and international wire transfers.\n\n<!-- [source](https://blog.runpod.io/how-to-manage-funding-your-runpod-account#invoicing) -->\n\nIf you're having trouble with your card payments, you can contact [RunPod support](https://www.runpod.io/contact) for assistance.","metadata":{"source":"/runpod-docs/docs/get-started/billing-information.md","loc":{"lines":{"from":20,"to":28}}}}],["46",{"pageContent":"---\ntitle: Connect to RunPod\ndescription: \"Interact with RunPod through multiple interfaces: web, CLI, and SDKs. Access the web interface at runpod.io/console/login, use the CLI runpodctl for management and development, or leverage SDKs for GraphQL, JavaScript, and Python programming languages.\"\nsidebar_position: 5\n---\n\nThere are many ways to interact with RunPod:\n\n- Web interface\n- CLI\n- SDKs\n\n## Web interface\n\n[Create an account](/get-started/connect-to-runpod) and then log into the web interface at the following address: [runpod.io/console/login](https://www.runpod.io/console/login).\n\n## CLI\n\nYou can use RunPod's CLI [runpodctl](https://github.com/runpod/runpodctl) to manage Pods and for development.\n\nAll Pods come with `runpodctl` installed with a Pod-scoped API key, which makes managing your Pods easier through the command line.","metadata":{"source":"/runpod-docs/docs/get-started/connect-to-runpod.md","loc":{"lines":{"from":1,"to":21}}}}],["47",{"pageContent":"SDKs\n\nRunPod provides SDKs for the following programming languages:\n\n- [GraphQL](/sdks/graphql/manage-pods)\n- [JavaScript](/sdks/javascript/overview)\n- [Python](/sdks/python/overview)","metadata":{"source":"/runpod-docs/docs/get-started/connect-to-runpod.md","loc":{"lines":{"from":23,"to":29}}}}],["48",{"pageContent":"---\ntitle: Manage accounts\nsidebar_position: 1\ndescription: \"Create an account or get invited by a team member to use RunPod, then convert personal to team account or invite others to join, with various role options including Basic, Billing, Dev, and Admin for customized access and permissions.\"\n---\n\nYou will need to create an account or get invited by a team member to use RunPod.\n\n## Create an account\n\nSign up for an account at [RunPod.io](https://www.runpod.io).\n\n### Convert personal account to a team account\n\nYou can convert a personal account to a team account at anytime.\n\n1. From the console, select **Convert to Team Account**.\n2. Set a team name and confirm the conversion.","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":1,"to":18}}}}],["49",{"pageContent":"Get invited by a team member\n\n1. Accept the link sent by a team member.\n2. Select **Join Team**.\n\nFor information on how to send an invitation, see [Invite a user](#invite-a-user).\n\n### Invite a user\n\nTo invite a user to your team on RunPod, you'll need a team account.\n\n1. To invite users, navigate to your Team page, and select the \"Invite New Member\" button at the top of the \"Members\" section.\n2. Select the role you want to provide the user.\n3. After you create the invite, you can copy the invite link from the pending invites section.\n4. Send a user your invite link and they will be able to go to it to join your team.","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":20,"to":34}}}}],["50",{"pageContent":"Role types\n\nThe following roles and permissions are available to you:\n\n### Basic role\n\nLimited access, primarily for account usage and existing pod connections.\n\n**Permissions**:\n\n- Use the account.\n- Connect to and use Pods.\n- Restricted from viewing billing information.\n- No permissions to start, stop, or create Pods.\n\n### Billing role\n\nSpecialized role focused on managing billing aspects.\n\n**Permissions**:\n\n- Access and manage billing information.\n- Restricted from other account features and Pod management.\n\n### Dev role\n\nEnhanced privileges suitable for development tasks.\n\n**Permissions**:\n\n- All \"Basic\" role permissions (use account, connect to Pods).\n- Start, stop, and create Pods.\n- No access to billing information.","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":36,"to":68}}}}],["51",{"pageContent":"Admin role\n\nFull control over the account, ideal for administrators.\n\n**Permissions**:\n\n- Complete access to all account features and settings.\n- Manage billing and access billing information.\n- Start, stop, and create Pods.\n- Modify account settings and user permissions.\n- Full control over all account resources and users.","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":70,"to":80}}}}],["52",{"pageContent":"Audit logs\n\nRunPod includes audit logs to help you understand which actions were used.\nGo to the [Audit logs](https://www.runpod.io/console/user/audit-logs) settings.\n\nYou can view and filter the audit logs by date range, user, resource, resource ID, and action.","metadata":{"source":"/runpod-docs/docs/get-started/manage-accounts.md","loc":{"lines":{"from":82,"to":87}}}}],["53",{"pageContent":"---\ntitle: Referral programs\nsidebar_position: 5\ndescription: \"Learn how to earn additional revenue with RunPod's referral programs and template program, offering credits for referring users and template creators.\"\n---\n\nRunPod offers two referral programs and a template program that allow users to earn additional revenue in the form of RunPod Credits.\nThis document provides an overview of these programs and instructions on how to participate.","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":1,"to":8}}}}],["54",{"pageContent":"Referral Programs\n\n### 1. Serverless Referral Program (BETA)\n\nThe Serverless Referral Program rewards both the referrer and the referred user with RunPod Credits when the referred user spends a certain amount on Serverless.\n\n#### Rewards\n\n- Referrer: Earns $500 in RunPod Credits\n- Referred User: Earns $500 in RunPod Credits\n\n#### Eligibility\n\n- The referred user must spend $1000 on Serverless.\n\n### 2. Referral Program (BETA)\n\nThe Referral Program allows users to earn a percentage of the money spent by their referred users for the lifetime of their account.\n\n#### Rewards\n\n- Referrer: Earns 2% in RunPod Credits for every penny spent by the referred user.\n\n#### Example\n\n- If 20 referrals spend $100 each, the referrer earns $40.\n\n#### Eligibility\n\n- The referrer must have at least 25 referrals and a minimum of $500 in referral spend.\n- To be eligible, [contact RunPod](https://contact.runpod.io/hc/en-us/requests/new) once these criteria are met.","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":10,"to":40}}}}],["55",{"pageContent":"Template Program (BETA)\n\nThe Template Program allows users to earn a percentage of the money spent by users who use their Pod Template.\n\n#### Rewards\n\n- Template Creator: Earns 1% for runtime in RunPod Credits for every penny spent using their template.\n\n#### Example\n\n- If 20 users use a Pod Template at $0.54/hr for a week, the template creator earns $18.14.\n\n#### Eligibility\n\n- The template must have at least 1 day of runtime.\n\n## How to Participate\n\n### Serverless Referral Program\n\n- Follow the instructions on the Serverless Referral Program page to refer users.\n\n### Referral Program\n\n1. Access your [referral dashboard](https://www.runpod.io/console/user/referrals).\n2. Locate your unique referral link. For example `https://runpod.io?ref=5t99c9je`.\n3. Share your referral link with potential users.","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":42,"to":68}}}}],["56",{"pageContent":"Support\n\nIf you have any questions or need assistance with the referral or template programs, [please contact](https://contact.runpod.io/hc/en-us/requests/new) the RunPod support team.\n\nRunPod allows referrers to keep the earnings made before activation once they meet the eligibility criteria and are accepted into the program.","metadata":{"source":"/runpod-docs/docs/get-started/referrals.md","loc":{"lines":{"from":70,"to":74}}}}],["57",{"pageContent":"---\ntitle: Glossary\nsidebar_position: 1\ndescription: RunPod is a cloud computing platform for AI and machine learning applications, offering GPU and CPU instances, serverless computing, and SDKs for seamless integration.\n---\n\n## Community Cloud\n\nGPU instances connect individual compute providers to consumers through a vetted, secure peer-to-peer system.\n\n## Datacenter\n\nA data center is a secure location where RunPod's cloud computing services, such as Secure Cloud and GPU Instances, are hosted. These data centers are equipped with redundancy and data backups to ensure the safety and reliability of your data.\n\n## Endpoint\n\nAn Endpoint refers to a specific URL where your serverless applications or services can be accessed. These endpoints provide standard functionality for submitting jobs and retrieving the output from job requests.","metadata":{"source":"/runpod-docs/docs/glossary.md","loc":{"lines":{"from":1,"to":17}}}}],["58",{"pageContent":"GPU Instance\n\nGPU Instance is a container-based GPU instance that you can deploy.\nThese instances spin up in seconds using both public and private repositories.\nThey are available in two different types:\n\n- Secure Cloud\n- Community Cloud\n\n## Handler\n\nA Handler is a function that is responsible for processing submitted inputs and generating the resulting output.\n\n## RunPod\n\nRunPod is a cloud computing platform primarily designed for AI and machine learning applications.\n\n## SDKs\n\nRunPod provides several Software Development Kits (SDKs) you can use to interact with the RunPod platform.\nThese SDKs enable you to create serverless functions, manage infrastructure, and interact with APIs.\n\n## Secure Cloud\n\nGPU instances that run in T3/T4 data centers, providing high reliability and security.","metadata":{"source":"/runpod-docs/docs/glossary.md","loc":{"lines":{"from":19,"to":43}}}}],["59",{"pageContent":"Serverless CPU\n\nServerless CPU is a pay-per-second serverless CPU computing solution.\nIt is designed to bring autoscaling to your production environment, meaning it can dynamically adjust computational resources based on your application's needs.\n\n## Serverless GPU\n\nServerless GPU is a pay-per-second serverless GPU computing solution.\nIt is designed to bring autoscaling to your production environment, meaning it can dynamically adjust computational resources based on your application's needs.\n\n## Serverless CPU\n\nServerless CPU is a pay-per-second serverless CPU computing solution. It is designed to bring autoscaling to your production environment, meaning it can dynamically adjust computational resources based on your application's needs.\n\n## Template\n\nA RunPod template is a Docker container image paired with a configuration.","metadata":{"source":"/runpod-docs/docs/glossary.md","loc":{"lines":{"from":45,"to":61}}}}],["60",{"pageContent":"Throttled Worker\n\nRunPod proactively caches Workers to minimize cold start time. If your GPU selections are not available at the time your Worker starts up, the Worker is throttled until resources are available.","metadata":{"source":"/runpod-docs/docs/glossary.md","loc":{"lines":{"from":63,"to":65}}}}],["61",{"pageContent":"---\ntitle: \"Burn Testing\"\ndescription: \"Before listing a machine on the RunPod platform, thoroughly test it with a burn test, verifying memory, CPU, and disk capabilities, and ensure compatibility with popular templates by self-renting the machine after verifying its performance.\"\n---\n\nMachines should be thoroughly tested before they are listed on the RunPod platform.\nHere is a simple guide to running a burn test for a few days.\n\nStop the RunPod agent by running:\n\n```command\nsudo systemctl stop runpod\n```\n\nThen you can kick off a gpu-burn run by typing:\n\n```command\ndocker run --gpus all --rm jorghi21/gpu-burn-test 172800\n```\n\nYou should also verify that your memory, CPU, and disk are up to the task.\nYou can use the [ngstress library](https://wiki.ubuntu.com/Kernel/Reference/stress-ngstress) to accomplish this.\n\nWhen everything is verified okay, start the RunPod agent again by running\n\n```command\nsudo systemctl start runpod","metadata":{"source":"/runpod-docs/docs/hosting/burn-testing.md","loc":{"lines":{"from":1,"to":27}}}}],["62",{"pageContent":"Then, on your [machine dashboard](https://www.runpod.io/console/host/machines), self rent your machine to ensure it's working well with most popular templates.","metadata":{"source":"/runpod-docs/docs/hosting/burn-testing.md","loc":{"lines":{"from":30,"to":30}}}}],["63",{"pageContent":"---\ntitle: \"Maintenance and reliability\"\ndescription: \"Schedule maintenance with at least one-week notice to minimize disruptions. Automated reminders sent to users. RunPod aims for 99.99% uptime, calculating reliability with a 10-minute buffer and rolling 30-day window. Excessive maintenance may result in penalties.\"\n---","metadata":{"source":"/runpod-docs/docs/hosting/maintenance-and-reliability.md","loc":{"lines":{"from":1,"to":4}}}}],["64",{"pageContent":"Maintenance\n\nHosts must currently schedule maintenance at least one week in advance and are able to program flash maintenance in the case their server is unrented.\nUsers will get email reminders of upcoming maintenance that will occur on their active pods.\nPlease contact RunPod on Discord or Slack if you are scheduling maintenance on more than a few machines so that we are aware of any major impacts to our customers.\n\nHere are some things to keep in mind.","metadata":{"source":"/runpod-docs/docs/hosting/maintenance-and-reliability.md","loc":{"lines":{"from":6,"to":12}}}}],["65",{"pageContent":"- Uptime/reliability will not be affected during scheduled maintenance.\n- ALL other events that may impact customer workloads will result in a reliability score decrease. This includes unlisted machines.\n- All machines that have maintenance scheduled will be automatically unlisted 4 days prior to the scheduled maintenance start time to minimize disruption for clients.\n- Excessive maintenance will result in further penalties.\n- You are allowed to bring down machines that have active users on them provided that you are in a maintenance window.","metadata":{"source":"/runpod-docs/docs/hosting/maintenance-and-reliability.md","loc":{"lines":{"from":14,"to":18}}}}],["66",{"pageContent":"Reliability calculations\n\nRunPod aims to partner with datacenters that offer **99.99%** uptime.\nReliability is currently calculated as follows:\n\n`( total minutes + small buffer ) / total minutes in interval`\n\nThis means that if you have 30 minutes of network downtime on the first of the month, your reliability will be calculated as:\n\n`( 43200 - 30 + 10 ) / 43200 = 99.95%`\n\nBased on approximately 43200 minutes per month and a 10 minute buffer.\nWe include the buffer because we do incur small single-minute uptime dings once in a while due to agent upgrades and such.\nIt will take an entire month to regenerate back to 100% uptime given no further downtimes in the month, considering it it calculated based on a 30 days rolling window.\n\nMachines with less than **98%** reliability are **automatically removed** from the available GPU pool and can only be accessed by clients that already had their data on it.","metadata":{"source":"/runpod-docs/docs/hosting/maintenance-and-reliability.md","loc":{"lines":{"from":20,"to":35}}}}],["67",{"pageContent":"---\ntitle: Overview\ndescription: \"RunPod offers GPU hosting opportunities through proprietary servers and community collaboration, with a 24% service fee, minimum bid pricing, and KYC verification for trusted hosts with at least 20 GPUs, offering customized dashboard management and revenue insights.\"\nsidebar_position: 1\n---\n\n## RunPod GPU hosting opportunity\n\nRunPod offers a diverse range of GPUs, made possible through proprietary servers and collaboration with trusted community members.\nIf you're interested in integrating your hardware into the RunPod ecosystem, follow the steps below.\n\n## How to join as a host\n\n1. Check Eligibility: Make sure you adhere to our [minimum requirements](/hosting/partner-requirements).\n2. Connect with us: Currently, we onboard hosts through a manual vetting process.\n   If you have high-quality machines that satisfy our hosting requirements, and at least 20 GPUs in total, please fill out [this form](https://share.hsforms.com/1GYpMeNlSQc6n11toAlgNngecykq).","metadata":{"source":"/runpod-docs/docs/hosting/overview.md","loc":{"lines":{"from":1,"to":16}}}}],["68",{"pageContent":"Additional hosting information\n\n- Service Fee: RunPod charges a 24% service fee. This encompasses:\n  - Approximately 4% for Stripe payment fees.\n  - 2% for our referral program and 1% for our template program. For more information, see [Refer a friend](https://www.runpod.io/refer-a-friend).\n\n- Pricing: While GPU on-demand prices are consistent, hosts can define a minimum bid price for spot rental. Even though we try as much as possible to maintain stable prices over time, we need to adjust to market trends.\n\n- Safety & Trust: We mandate KYC (Know Your Customer) verification for all hosts to safeguard our users and combat fraud. For larger providers, we require a Provider Agreement and a Service Level Agreement to be completed.\n\n- Hosting Experience: As one of our trusted providers, you have access to a fully customized dashboard to manage your resources that you can leverage to deploy hardware and plan your expansion.","metadata":{"source":"/runpod-docs/docs/hosting/overview.md","loc":{"lines":{"from":18,"to":28}}}}],["69",{"pageContent":"- Rental Rates: We do not make utilization data publicly available.\n  However, we are more than happy to provide statistics and information about popular GPU models when directly discussing with you.\n  Furthermore, lots of different variables can impact occupancy.\n  We are more than happy to provide you with in-dept data about how different hardware quality levels can impact your revenue.","metadata":{"source":"/runpod-docs/docs/hosting/overview.md","loc":{"lines":{"from":30,"to":33}}}}],["70",{"pageContent":"---\ntitle: \"Requirements\"\ndescription: \"Meet the minimum requirements for a secure and scalable AI cloud infrastructure: Ubuntu Server 22.04 LTS, remote SSH access, Nvidia drivers, CUDA Toolkit, and more.\"\n---\n\nThe following requirements are minimal and are subject to change.","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":1,"to":6}}}}],["71",{"pageContent":"Software specifications\n\n- Ubuntu Server 22.04 LTS:\n  - Basic Linux proficiency.\n  - Ability to remotely connect via SSH.\n\n### Operating system\n\n- Ubuntu Server 22.04 LTS\n  - Use the same file as 22.04, but select HWE during install.\n  - That way, Kernel 6.5.0-15 is installed (please replace by any more recent production version if available).\n\n### BIOS\n\n- For non-VM systems, make sure IOMMU is disabled in the BIOS.\n- Another good practice is to update the server BIOS to the latest stable version when facing compatibility issues.\n\n### Drivers\n\n- Nvidia drivers 550.54.15 (please replace by any more recent production version if available).\n  - Never use beta or new feature branch drivers except if you have been instructed otherwise.\n- CUDA 12.4 (please replace by any more recent production version if available).\n- Nvidia Persistence should be activated for GPUs of 48 GB or more.","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":8,"to":30}}}}],["72",{"pageContent":"HGX SXM Systems\n\n- Nvidia Fabric Manager needs to be installed, activated, running, and tested.\n  - Mandatory: Fabric Manager version = Nvidia drivers version = Kernel drivers headers.\n  - A p2p bandwidth test should be passed.\n- CUDA Toolkit, Nvidia NSCQ and Nvidia DCGM need to be installed.\n- Ensure the topology of the NVLINK switch is right by leveraging nvidia-smi and dcgmi.\n  - Ensure the SXM is performing well leveraging the dcgmi diagnostic tool.","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":32,"to":39}}}}],["73",{"pageContent":"Minimal Secure Cloud Specifications\n\n### GPU Models\n\n- Latest Nvidia GPUs are required with models of at least 30xx or RTX A4000 or more recent.\n- Demand is highest for SXM 80 GB, PCIe 80 GB, Ada 6000 48 GB, Ada 5000 32 GB, 4090 24 GB, L4 24 GB, A5000 24 GB, and Ada 4000.\n\n### Quantity\n\n- **Option 1**: At least 100 GPUs in total, each with a minimum of 12 GB VRAM.\n- **Option 2**: At least 32 GPUs in total, each with a minimum of 80 GB of VRAM.\n\nWe also require 2 GPU per server at minimum.\n8x configuration is recommended.\n\n### CPU\n\n- Minimum of 4 Physical CPU Cores per GPU + 2 for system operations.\n- You should prioritize CPU core clock as fast as possible over more providing more cores.\n- For example, a 24-cores CPU clocking at 5.0 GHz is preferred to a 128-cores CPU clocking at 3.0 GHz for a 4x GPU configuration.\n- Genoa CPU are often a good option for these reasons.","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":41,"to":61}}}}],["74",{"pageContent":"Bus Bandwidth\n\n- Minimum banwitdh per GPU is PCIe 3.0 x16 for 8 GB | 10 GB | 12 GB | 16 GB GPUs.\n- Minimum banwitdh per GPU is PCIe 4.0 x16 for 20 GB | 24 GB | 32 GB | 40 GB | 48 GB | 80 GB GPUs.\n- PCIe 5.0 x16 is recommended for 80 GB GPUs.\n\n### Memory\n\n- Your RAM should at minimum equals your total VRAM of all GPUs + 12 GB for system operations.\n  - 1024 GB+ of RAM recommended for 8x 80 GB VRAM GPU configurations.\n  - 512 GB+ of RAM recommended for 8x 24 GB VRAM GPU configurations.\n  - 256 GB+ of RAM recommended for 8x 16 GB VRAM GPU configurations.\n- DDR4 minimum. DDR5 is recommended.\n- Memory should be ECC compatible.","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":63,"to":76}}}}],["75",{"pageContent":"Storage\n\n- Absolute minimum of 1 TB+ of NVME space per GPU for each server (excluding the OS drives). Recommended storage is 2 TB+ of NVME space per GPU for 24 GB and 48 GB GPU, and is 4 TB+ of NVME space per GPU for 80 GB GPUs.\n  - We recommend 2 smaller NVME disk in RAID 1 for the operating system (2x 500 GB or 2x 1 TB is fine).\n  - For the data drives, keep one larger NVME unpartitioned and unformatted.\n    If several data drives are provided, you need to create a LVM volume for those.\n    For higher number of drives, Raid 10 is the ideal scenario.\n    When installing RunPod, you will have to mention that LVM or Raid volume.\n- A read/write speed of 3,000 mbps is required.\n  - PCIe 5.0 x4 NVME SSD are an asset for 80 GB and newer 48 GPUs.\n- Ability to deploy network storage clusters if needed.","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":78,"to":88}}}}],["76",{"pageContent":"Networking\n\n- 10 gbps Bidirectional Internet Speed as backbone.\n  - ISP access of minimum 1 gbps symmetrical per server.\n- Static Public IP.\n  - A single IP can be shared between up between groups of up to 20 servers.\n  - Find how to activate Public IP [here](https://www.runpod.io/console/host/docs/config-options).\n- Access and ability to port forward.\n  - Minimum of 30 ports forwarded per GPU per server. For an 8x GPU server, 300 ports forwarded is recommended.\n- Minimum interconnect speed of 25 gbps between servers.\n  - Recommended interconnect speed of 50 gbps between servers.\n  - Recommended interconnect speed of 200 gbps between servers for 80 GB GPUs.\n  - A100 HGX SXM4 80 GB and H100 HGX SXM5 80 GB see higher demand if on high-speed InfiniBand that are 1200 gbps to 3600 gbps.","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":90,"to":102}}}}],["77",{"pageContent":"Compliance\n\n- Abide by Tier III+ Datacenter Standards.\n- Robust Uninterruptible Power Supply and backup generators.\n- Switches and PDU redundancy.\n- Internet Service Provider redundancy.\n- 24/7 on-site security and technical staff.\n- All maintenance or downtime need to be scheduled one week in advance. See more about it [here](https://docs.runpod.io/hosting/maintenance-and-reliability).","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":104,"to":111}}}}],["78",{"pageContent":"Most importantly\n\n- The ability to scale GPU supply over time.\n- Interest for less purely transactional relationship, and eagerness for a partnership centered around building the future of AI Cloud Infrastructure.","metadata":{"source":"/runpod-docs/docs/hosting/partner-requirements.md","loc":{"lines":{"from":113,"to":116}}}}],["79",{"pageContent":"---\ntitle: Overview\ndescription: \"RunPod is a cloud computing platform for AI, machine learning, and general compute, offering GPU and CPU resources, serverless computing, and a Command Line Interface for easy deployment and development.\"\nsidebar_position: 1\n---\n\nThis page provides an overview for RunPod and its related features.\n\n**RunPod** is a cloud computing platform designed for AI, machine learning applications, and general compute.\n\nExecute your code utilizing both GPU and CPU resources through [Pods](/pods/overview), as well as [Serverless](/serverless/overview) computing options.","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":1,"to":11}}}}],["80",{"pageContent":"What are Pods?\n\n**Pods** allows you to run your code on GPU and CPU instances with containers.\n\nPods are available in two different types: [Secure Cloud and Community Cloud](references/faq/#secure-cloud-vs-community-cloud).\nThe Secure Cloud runs in T3/T4 data centers providing high reliability and security, while the Community Cloud connects individual compute providers to consumers through a vetted, secure peer-to-peer system.\n\n## What is Serverless?\n\n**Serverless** offers pay-per-second serverless computing, bringing autoscaling to your production environment.\nThe Serverless offering allows users to define a Worker, create a REST API Endpoint for it which queue jobs and autoscales to fill demand.\nThis service, part of our Secure Cloud offering, guarantees low cold-start times and stringent security measures.\n\nYou can get started with:\n\n- [Quick deploys](/serverless/quick-deploys)\n- [Build your own](/serverless/workers/overview)","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":13,"to":29}}}}],["81",{"pageContent":"CLI\n\nAdditionally, RunPod has developed a Command Line Interface (CLI) tool designed specifically for quickly developing and deploying custom endpoints on the RunPod serverless platform.\n\n### Our mission\n\nRunPod is committed to making cloud computing accessible and affordable to all without compromising on features, usability, or experience. We strive to empower individuals and enterprises with cutting-edge technology, enabling them to unlock the potential of AI and cloud computing.\n\nFor any general inquiries, we recommend browsing through our documentation.\nOur team is also available on [Discord](https://discord.gg/cUpRmau42V), our support chat, and by [email](mailto:support@runpod.io).\n\nMore information can be found on our [contact page](https://www.runpod.io/contact).","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":31,"to":42}}}}],["82",{"pageContent":"Where do I go next?\n\nLearn more about RunPod by:\n\n- [Create an account](/get-started/manage-accounts)\n- [Add funds to your account](/get-started/billing-information)\n- [Run your first tutorial](/tutorials/introduction/overview)","metadata":{"source":"/runpod-docs/docs/overview.md","loc":{"lines":{"from":44,"to":50}}}}],["83",{"pageContent":"---\ntitle: Choose a Pod\ndescription: \"Choose the right Pod instance for your RunPod deployment by considering VRAM, RAM, vCPU, and storage, both Temporary and Persistent, to ensure optimal performance and efficiency.\"\nsidebar_position: 3\n---\n\nSelecting the appropriate Pod instance is a critical step in planning your RunPod deployment. The choice of VRAM, RAM, vCPU, and storage, both Temporary and Persistent, can significantly impact the performance and efficiency of your project.\n\nThis page gives guidance on how to choose your Pod configuration. However, these are general guidelines. Keep your specific requirements in mind and plan accordingly.","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":1,"to":9}}}}],["84",{"pageContent":"Overview\n\nIt's essential to understand the specific needs of your model. You can normally find detailed information in the model card’s description on platforms like Hugging Face or in the `config.json` file of your model.\n\nThere are tools that can help you assess and calculate your model’s specific requirements, such as:\n\n- [Hugging Face's Model Memory Usage Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)\n- [Vokturz’ Can it run LLM calculator](https://huggingface.co/spaces/Vokturz/can-it-run-llm)\n- [Alexander Smirnov’s VRAM Estimator](https://vram.asmirnov.xyz)\n\nUsing these resources should give you a clearer picture of what to look for in a Pod.\n\nWhen transitioning to the selection of your Pod, you should focus on the following main factors:\n\n- **GPU**\n- **VRAM**\n- **Disk Size**","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":11,"to":27}}}}],["85",{"pageContent":"Each of these components plays a crucial role in the performance and efficiency of your deployment. By carefully considering these elements along with the specific requirements of your project as shown in your initial research, you will be well-equipped to determine the most suitable Pod instance for your needs.","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":29,"to":29}}}}],["86",{"pageContent":"GPU\n\nThe type and power of the GPU directly affect your project's processing capabilities, especially for tasks involving graphics processing and machine learning.\n\n### Importance\n\nThe GPU in your Pod plays a vital role in processing complex algorithms, particularly in areas like data science, video processing, and machine learning. A more powerful GPU can significantly speed up computations and enable more complex tasks.\n\n### Selection criteria\n\n- **Task Requirements**: Assess the intensity and nature of the GPU tasks in your project.\n- **Compatibility**: Ensure the GPU is compatible with your software and frameworks.\n- **Energy Efficiency**: Consider the power consumption of the GPU, especially for long-term deployments.\n\n### VRAM\n\nVRAM (Video RAM) is crucial for tasks that require heavy graphical processing and rendering. It is the dedicated memory used by your GPU to store image data that is displayed on your screen.","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":31,"to":47}}}}],["87",{"pageContent":"Importance\n\nVRAM is essential for intensive tasks. It serves as the memory for the GPU, allowing it to store and access data quickly. More VRAM can handle larger textures and more complex graphics, which is crucial for high-resolution displays and advanced 3D rendering.\n\n### Selection criteria\n\n- **Graphics Intensity**: More VRAM is needed for graphically intensive tasks such as 3D rendering, gaming, or AI model training that involves large datasets.\n- **Parallel Processing Needs**: Tasks that require simultaneous processing of multiple data streams benefit from more VRAM.\n- **Future-Proofing**: Opting for more VRAM can make your setup more adaptable to future project requirements.\n\n### Storage\n\nAdequate storage, both temporary and persistent, ensures smooth operation and data management.\n\n### Importance\n\nDisk size, including both temporary and persistent storage, is critical for data storage, caching, and ensuring that your project has the necessary space for its operations.","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":49,"to":65}}}}],["88",{"pageContent":"Selection criteria\n\n- **Data Volume**: Estimate the amount of data your project will generate and process.\n- **Speed Requirements**: Faster disk speeds can improve overall system performance.\n- **Data Retention Needs**: Determine the balance between temporary (volatile) and persistent (non-volatile) storage based on your data retention policies.","metadata":{"source":"/runpod-docs/docs/pods/choose-a-pod.md","loc":{"lines":{"from":67,"to":71}}}}],["89",{"pageContent":"---\ntitle: Export data\ndescription: Export RunPod data to various cloud providers, including Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage, Backblaze B2 Cloud Storage, and Dropbox, with secure key and access token management.\nsidebar_position: 6\n---\n\nYou can export your Pod's data to any of the following cloud providers:\n\n- Amazon S3\n- Google Cloud Storage\n- Microsoft Azure Blob Storage\n- Dropbox\n- Backblaze B2 Cloud Storage\n\nRemember to keep your keys and access tokens confidential to maintain the security of your resources.","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":1,"to":15}}}}],["90",{"pageContent":"Amazon S3\n\nYou can review a video guide on the process [here](https://www.youtube.com/watch?v=2ZuOKwFR9pc&t=1s).","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":17,"to":19}}}}],["91",{"pageContent":"Creating a Bucket within Amazon S3\n\n1. **Access the Bucket Creation Form:**\n   - Navigate to the Amazon S3 bucket creation form by visiting [this link](https://s3.console.aws.amazon.com/s3/bucket/create?region=us-east-1).\n\n2. **Name Your Bucket:**\n   - Provide a descriptive name for your bucket. Choose a name that is easy to remember and reflects the contents or purpose of the bucket.\n\n3. **Select AWS Region:**\n   - Ensure you select your preferred AWS Region. This is important for data storage locations and can affect access speeds.\n\n4. **Adjust Public Access Settings:**\n   - Uncheck the **Block All Public Access** option at the bottom of the form if you need your bucket to be publicly accessible.\n\n5. **Access Key and Secret Access Key:**\n   - Go to Security Credentials in your AWS account.\n   - Create an Access Key on the Security Credentials page.\n   - Note that your Secret Access Key will be displayed during this process. Keep it secure.\n\n![](/img/docs/7fa9781-image.png)","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":21,"to":40}}}}],["92",{"pageContent":"Sending Data from RunPod to AWS S3\n\n1. **Access CloudSync in RunPod:**\n   - In RunPod, navigate to the CloudSync section.\n\n2. **Enter Key IDs and Bucket Information:**\n   - Enter your Access Key and Secret Access Key.\n   - Specify the AWS Region where your bucket is located.\n   - Provide the path of your bucket as shown in the interface.\n\n3. **Initiate Data Transfer:**\n   - Select the **Copy to AWS S3** option.\n   - This action will start copying your pod contents to the specified Amazon S3 bucket.\n\n4. **Monitor Transfer:**\n   - Once you select Copy, your pod contents should begin copying over to Amazon S3.\n   - You can monitor the transfer process through RunPod’s interface to ensure that the data transfer completes successfully.\n\n![](/img/docs/8fec5c5-image.png)\n\nRemember to keep your Access Key and Secret Access Key confidential to maintain the security of your AWS resources.","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":42,"to":62}}}}],["93",{"pageContent":"Google Cloud Storage\n\n### Creating a Bucket within Google Cloud Storage\n\n1. **Access the Bucket Creation Interface:**\n   - Navigate to the Google Cloud Storage dashboard and click on \"Buckets -> Create\" to access the bucket creation interface.\n\n2. **Name Your Bucket:**\n   - Assign a unique, descriptive name to your bucket that reflects its contents or purpose.\n\n3. **Configure Bucket Settings:**\n   - Leave most options as default. Important: Uncheck \"Enforce Public Access Prevention On This Bucket\" if you need your bucket to be publicly accessible.\n\n4. **Organize Your Bucket:**\n   - Once the bucket is created, consider creating a folder within the bucket for better organization, especially if managing multiple pods.\n\n![](/img/docs/4450288-image.png)","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":64,"to":80}}}}],["94",{"pageContent":"Transferring Data from RunPod to Google Cloud Storage\n\n1. **Access CloudSync in RunPod:**\n   - Within RunPod, go to the CloudSync section and select \"Google Cloud Storage -> Copy to Google Cloud Storage.\"\n\n2. **Service Account JSON Key:**\n   - Obtain your Service Account JSON key. If unsure how to do this, consult [this guide](https://cloud.google.com/iam/docs/keys-create-delete).\n   - In the provided field on RunPod, paste the entire contents of your Service Account JSON key.\n\n3. **Specify Transfer Details:**\n   - Enter the destination path in your bucket.\n   - Choose the folder from your pod that you wish to copy.\n\n4. **Initiate and Monitor Transfer:**\n   - Start the data transfer process by selecting the relevant options.\n   - Monitor the transfer in the RunPod interface to ensure successful completion.\n\n![](/img/docs/a4fcf38-image.png)","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":82,"to":99}}}}],["95",{"pageContent":"Troubleshooting\n\n- If your bucket is not publicly viewable and you encounter errors, refer to [Google Cloud Storage's documentation on making data public](https://cloud.google.com/storage/docs/access-control/making-data-public) for necessary adjustments.\n\nRemember to keep your Service Account JSON key confidential to maintain the security of your Google Cloud resources.","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":101,"to":105}}}}],["96",{"pageContent":"Azure Blob Storage Setup and Data Transfer with RunPod","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":107,"to":107}}}}],["97",{"pageContent":"Creating a Storage Account in Azure\n\n1. **Create a Resource Group in Azure:**\n   - Go to [Resource Groups](https://portal.azure.com/#view/HubsExtension/BrowseResourceGroups) and click the Create button.\n   - Name the resource group, which will be used to organize your Azure resources.\n\n2. **Set Up a Storage Account:**\n   - Under [Storage Accounts](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Storage%2FStorageAccounts), click Create.\n   - Provide a name for your storage account and assign it to the newly created resource group.\n\n3. **Retrieve Access Key:**\n   - Navigate to Access Keys under Security + Networking in your storage account to get the key needed for authentication.\n\n4. **Create a Blob Container:**\n   - In the Storage Browser, select Blob Containers, then click Add Container.\n   - Optionally, create folders within this container for better organization.\n\n![](/img/docs/dcc8c23-image.png)","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":109,"to":126}}}}],["98",{"pageContent":"Transferring Data from RunPod to Azure Blob Storage\n\n1. **Access Cloud Sync in RunPod:**\n   - Go to your pod in My Pods on RunPod.\n   - Select Cloud Sync, then choose \"Azure Blob Storage\" and \"Copy to Azure Blob Storage.\"\n\n2. **Input Storage Details:**\n   - Enter your Azure account name and account key.\n   - Specify the desired path in the blob storage where the data will be transferred.\n\n3. **Initiate Transfer:**\n   - Click on \"Copy to Azure Blob Storage\" to start the process.\n   - Your RunPod data will begin copying over to the specified location in Azure Blob Storage.\n\n![](/img/docs/55e94f0-image.png)\n\nEnsure secure handling of your Azure account key to maintain the integrity and security of your data during the transfer process.","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":128,"to":144}}}}],["99",{"pageContent":"Backblaze B2 Cloud Storage Setup\n\n### Creating a Bucket in Backblaze B2\n\n1. **Navigate to Bucket Creation:**\n   - Go to [B2 Cloud Storage Buckets](https://secure.backblaze.com/b2_buckets.htm) and click \"Create a Bucket.\"\n   - Make sure to set the bucket visibility to Public.\n\n2. **Generate Application Key:**\n   - Visit [App Keys](https://secure.backblaze.com/app_keys.htm) to create a new application key. This key will be used for authenticating access to your bucket.\n\n![](/img/docs/8aff108-image.png)","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":146,"to":157}}}}],["100",{"pageContent":"Transferring Data from RunPod to Backblaze B2\n\n1. **Access CloudSync in RunPod:**\n   - On your My Pods screen in RunPod, select Cloud Sync, then choose \"Backblaze B2.\"\n\n2. **Enter Credentials:**\n   - Input your KeyID in the first field.\n   - Enter your applicationKey in the second field.\n   - Specify your bucket name as illustrated in the interface.\n\n3. **Initiate Transfer:**\n   - Click \"Copy to Backblaze B2\" to start the transfer process. Your pod's contents will begin transferring to the specified Backblaze B2 bucket.\n\n![](/img/docs/5c12c2f-image.png)\n\nRemember to securely manage your KeyID and applicationKey to ensure the safety of your data in Backblaze B2 Cloud Storage.","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":159,"to":174}}}}],["101",{"pageContent":"Dropbox Setup and Data Transfer with RunPod\n\n### Setting Up Dropbox\n\n1. **Create an App on Dropbox:**\n   - Go to the [DBX Platform](https://www.dropbox.com/developers/apps/create) and create an app.\n   - Choose \"Scoped Access\" under API options and \"Full Dropbox\" for the type of access. Then, name your app.\n\n2. **Configure App Permissions:**\n   - In the Dropbox App Console, under the Permissions tab, make sure to enable the required checkboxes for reading and writing access.\n\n3. **Generate Access Token:**\n   - Return to the Settings tab of your app.\n   - In the OAuth2 section, click \"Generate\" under Generated Access Token to create an access key.\n   - Save this key securely, as it is crucial for integrating with RunPod and will not be visible after leaving the page.\n\n4. **Create a Dropbox Folder (Optional):**\n   - Although not mandatory, it's advisable to create a dedicated folder in Dropbox for organizing the data synced from RunPod.\n\n![](/img/docs/e73bced-image.png)","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":176,"to":195}}}}],["102",{"pageContent":"Transferring Data from RunPod to Dropbox\n\n1. **Access Cloud Sync in RunPod:**\n   - In RunPod, navigate to the Cloud Sync option and select Dropbox.\n\n2. **Enter Access Token and Path:**\n   - Input your Dropbox Access Token.\n   - Specify the remote path in Dropbox where you want to send the data.\n\n3. **Start Data Sync:**\n   - Click \"Copy to Dropbox\" to initiate the data syncing process. Your RunPod data will begin transferring to the specified location in Dropbox.\n\n![](/img/docs/2281560-image.png)\n\nEnsure the safekeeping of your Dropbox Access Token to maintain the security of your data during the sync process.","metadata":{"source":"/runpod-docs/docs/pods/configuration/export-data.md","loc":{"lines":{"from":197,"to":211}}}}],["103",{"pageContent":"---\ntitle: Expose ports\ndescription: \"Exposing ports on your pod to the outside world: Learn how to expose ports via RunPod's Proxy or TCP Public IP, and discover the benefits and limitations of each method, including symmetrical port mapping requests.\"\nsidebar_position: 8\n---\n\nThere are a few ways to expose ports on your pod to the outside world. The first thing that you should understand is that the publicly exposed port is most likely NOT going to be the same as the port that you expose on your container. Let's look at an example to illustrate this.\n\nLet's say that I want to run a public API on my pod using uvicorn with the following command:\n\n```\nuvicorn main:app --host 0.0.0.0 --port 4000\n```\n\nThis means that uvicorn would be listening on all interfaces on port 4000. Let's now expose this port to the public internet using two different methods.","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":1,"to":15}}}}],["104",{"pageContent":"Through RunPod's Proxy\n\nIn this case, you would want to make sure that the port you want to expose (4000 in this case) is set on the [Template](https://www.runpod.io/console/user/templates) or [Pod](https://www.runpod.io/console/pods) configuration page. You can see here that I have added 4000 to the HTTP port list in my pod config. You can also do this on your template definition.\n\n![](/img/docs/1386a3c-image.png)\n\nOnce you have done this, and your server is running, you should be able to hit your server using the pod's proxy address, which is formed in this programmatic way, where the pod ID is the unique ID of your pod, and the internal port in this case is 4000:\n\n```text\nhttps://{POD_ID}-{INTERNAL_PORT}.proxy.runpod.net\n```\n\nKeep in mind that this exposed to the public internet. While your pod ID can act as a password of sorts, it's not a replacement for real authentication, which should be implemented at your API level.","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":17,"to":29}}}}],["105",{"pageContent":"Through TCP Public IP\n\nIf your pod supports a public IP address, you can also expose your API over public TCP. In this case, you would add the port to the TCP side of the configuration.\n\n![](/img/docs/49ebb9a-image.png)\n\nThe only difference here is that you will receive an external port mapping and a public IP address to access your service.\nFor example, your connect menu may look something like this:\n\n![](/img/docs/5e76c21-image.png)\n\nIn this case, you would be hitting your service running on 4000 with the following ip:port combination\n\n```text\n73.10.226.56:10027\n```\n\nBe aware that the public IP could potentially change when using Community Cloud, but should not change when using Secure Cloud. The port will change if your pod gets reset.","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":31,"to":48}}}}],["106",{"pageContent":"Requesting a Symmetrical Port Mapping\n\nFor some applications, asymmetrical port mappings are not ideal. In the above case, we have external port 10027 mapping to internal port 4000. If you need to have a symmetrical port mapping, you can request them by putting in ports above 70000 in your TCP port field.\n\n![](/img/docs/23c4178-image.png)\n\nOf course, 70000 isn't a valid port number, but what this does is it tells RunPod that you don't care what the actual port number is on launch, but to rather give you a symmetrical mapping. You can inspect the actual mapping via your connect menu:\n\n![](/img/docs/92e4f90-image.png)\n\nIn this case, I have requested two symmetrical ports and they ended up being 10030:10030 and 10031:10031. If you need programmatic access to these in your pod, you can access them via environment variable:\n\n```text\nRUNPOD_TCP_PORT_70001=10031\nRUNPOD_TCP_PORT_70000=10030\n```","metadata":{"source":"/runpod-docs/docs/pods/configuration/expose-ports.md","loc":{"lines":{"from":50,"to":65}}}}],["107",{"pageContent":"---\ntitle: \"Override public key\"\nsidebar_position: 9\ndescription: \"Configure public key authentication for secure access via terminal, or override at the pod level using the RUNPOD_SSH_PUBLIC_KEY environment variable.\"\n---\n\nWe attempt to inject the public key that you configure in your account's settings page for authentication using basic terminal.\nIf you want to override this at a pod level, you can manually supply a public key as the `RUNPOD_SSH_PUBLIC_KEY` environment variable.","metadata":{"source":"/runpod-docs/docs/pods/configuration/override-public-keys.md","loc":{"lines":{"from":1,"to":8}}}}],["108",{"pageContent":"---\ntitle: \"Use SSH\"\nsidebar_position: 9\ndescription: \"Set up secure SSH access to RunPod using public/private key pairs, ensuring compatibility with ed25519 keys, and troubleshoot common issues like incorrect key copying and file path errors.\"\n---\n\nThe basic terminal SSH access that RunPod exposes is not a full SSH connection and, therefore, does not support commands like SCP. If you want to have full SSH capabilities, then you will need to rent an instance that has public IP support and run a full SSH daemon in your Pod.","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":1,"to":7}}}}],["109",{"pageContent":"Setup\n\n1. Generate your public/private SSH key pair on your local machine with `ssh-keygen -t ed25519 -C \"your_email@example.com\"`. This will save your public/private key pair to `~/.ssh/id_ed25519.pub` and `~/.ssh/id_ed25519`, respectively.\\\n   :::note\n   if you're using command prompt in Windows rather than the Linux terminal or WSL, your public/private key pair will be saved to `C:\\users\\{yourUserAccount}\\.ssh\\id_ed25519.pub` and `C:\\users\\{yourUserAccount}\\.ssh\\id_ed25519`, respectively.\n   :::\n\n![](/img/docs/4655a01-1.png)\n\n2. Add your public key to your [RunPod user settings](https://www.runpod.io/console/user/settings).\n\n![](/img/docs/4972691-2.png)\n\n![](/img/docs/c340553-image.png)\n\n3. Start your Pod. Make sure of the following things:","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":9,"to":24}}}}],["110",{"pageContent":"- Your Pod supports a public IP, if you're deploying in Community Cloud.\n- An SSH daemon is started. If you're using a RunPod official template such as RunPod Stable Diffusion, you don't need to take any additional steps. If you're using a custom template, make sure your template has TCP port 22 exposed and use the following Docker command. If you have an existing start command, replace `sleep infinity` at the end with your existing command:\n\n```bash\nbash -c 'apt update;DEBIAN_FRONTEND=noninteractive apt-get install openssh-server -y;mkdir -p ~/.ssh;cd $_;chmod 700 ~/.ssh;echo \"$RUNPOD_SSH_PUBLIC_KEY\" >> authorized_keys;chmod 700 authorized_keys;service ssh start;sleep infinity'","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":26,"to":30}}}}],["111",{"pageContent":"![](/img/docs/97823c6-image.png)\n\nOnce your Pod is done initializing, you'll be able to SSH into it by running the SSH over exposed TCP command in the Pod's Connection Options menu on your local machine.\n\n:::note\n\n- if you're using the Windows Command Prompt rather than the Linux terminal or WSL, and you've used the default key location when generating your public/private key pair (i.e., you didn't specify a different file path when prompted), you'll need to modify the file path in the provided SSH command after the `-i` flag to `C:\\users\\{yourUserAccount}\\.ssh\\id_ed25519`.\n- If you've saved your key to a location other than the default, specify that path you chose when generating your key pair after the `-i` flag instead.\n  :::\n\n![](/img/docs/3d51ed8-image.png)\n\n![](/img/docs/ff71847-image.png)","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":33,"to":45}}}}],["112",{"pageContent":"What's the SSH password?\n\nIf you're being prompted for a password when you attempt to connect, something is amiss. We don't require a password for SSH connections. Some common mistakes that cause your SSH client to prompt for a password include:\n\n- Copying and pasting the key _fingerprint_ (beginning with `SHA256:`) into your RunPod user settings instead of the public key itself (the contents of the `id_ed25519.pub` file when viewed from a text editor)\n- Omitting the encryption type from the beginning of the key when copying and pasting into your RunPod user settings (i.e., copying the random text, but not the `ssh-ed25519` which precedes it)\n- Not separating different public keys in your RunPod user settings with a newline between each one (this would result in the first public/private key pair functioning as expected, but each subsequent key pair would not work)\n- Specifying an incorrect file path to your private key file:\n\n![](/img/docs/10cbfa6-image.png)","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":47,"to":56}}}}],["113",{"pageContent":"- Attempting to use a private key that other users on the machine have permissions for:\n\n![](/img/docs/7a5cf85-image.png)\n\n- Incorrect Private Key being used locally in SSH config file.\n  There should be a config file on your local machine in your ~/.ssh folder. You want to ensure that the IdentityFile in the config file points to the private key of the public key you used to make this connection. If you are not pointing to the correct private key in the config file, when you make a connection request using your public key, you will get a mismatch and be prompted for a password. Once the correct private key is set in your config file, you can connect without a password.\n\n![private-key-fix](https://github.com/runpod/docs/assets/19496114/1f3db241-72a1-4d29-be36-ea5bab945b0a)","metadata":{"source":"/runpod-docs/docs/pods/configuration/use-ssh.md","loc":{"lines":{"from":58,"to":65}}}}],["114",{"pageContent":"---\ntitle: Connect to a Pod\nsidebar_position: 4\n---\n\nYou can connect to a Pod through various methods, depending on your requirements, preferences, and templates used.","metadata":{"source":"/runpod-docs/docs/pods/connect-to-a-pod.md","loc":{"lines":{"from":1,"to":6}}}}],["115",{"pageContent":"SSH terminal\n\nConnecting to a Pod using an SSH terminal is a secure and reliable method, suitable for long-running processes and critical tasks.\n\nEvery Pod contains the ability to connect through SSH.\n\nTo do this, you need to have an SSH client installed on your local machine.\n\n1. Open the terminal on your local machine.\n2. Choose one of the following commands and then enter it into your machine's terminal:\n\n```bash\n# No support for SCP & SFTP\nssh <username>@<pod-ssh-hostname> -i <path-to-ssh-key>\n\n# Supports SCP & SFTP\nssh <username>@<pod-ip-address> -p <ssh-port> -i <path-to-ssh-key>\n```\n\nReplace the placeholders with the following:\n\n- `<username>`: Your assigned username for the Pod\n- `<pod-ssh-hostname>`: The SSH hostname provided for your Pod\n- `<pod-ip-address>`: The IP address of your Pod\n- `<ssh-port>`: The designated SSH port for your Pod\n- `<path-to-ssh-key>`: The path to your SSH private key file\n\nYou now have a secure SSH terminal to your Pod.","metadata":{"source":"/runpod-docs/docs/pods/connect-to-a-pod.md","loc":{"lines":{"from":8,"to":35}}}}],["116",{"pageContent":"Web terminal\n\n:::note\n\nDepending on your Pod's template will provide the ability to connect to the web terminal.\n\n:::\n\nThe web terminal is a convenient, web-based terminal for quickly connecting to your Pod and running commands.\n\nThis shouldn't be relied on for long-running process such as training an LLM or other critical tasks.\n\nThe web terminal is useful for quickly logging in to your Pod and running commands.\n\n1. On your Pod's page, select **Connect**.\n2. Select **Start Web Terminal** then choose **Connect to Web Terminal** in a new window.\n3. Enter the **Username** and **Password**.","metadata":{"source":"/runpod-docs/docs/pods/connect-to-a-pod.md","loc":{"lines":{"from":37,"to":53}}}}],["117",{"pageContent":"---\ntitle: Access Logs\ndescription: \"Get insights into your pods' activities with logs, including container logs and system logs, detailing console output, formation, and status updates, accessible through the Pods dashboard's Logs button.\"\nsidebar_position: 5\n---\n\nPods provide two types of logs.\n\n- **Container logs** include anything typically sent to your console standard out.\n\n- **System logs** include information on your container's formation and current status, including download, extraction, start, and stop.\n\nTo access your logs, go the Pods dashboard and click the **Logs** button on your Pod.","metadata":{"source":"/runpod-docs/docs/pods/logs.md","loc":{"lines":{"from":1,"to":13}}}}],["118",{"pageContent":"---\ntitle: Manage Pods\ndescription: \"Learn how to start, stop, and manage Pods with RunPod, including creating and terminating Pods, and using the command line interface to manage your Pods.\"\nid: manage-pods\nsidebar_position: 3\n---\n\nLearn how to start, stop, and manage Pods with RunPod, including creating and terminating Pods, and using the command line interface to manage your Pods.\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n### Prerequisites\n\nIf you are using the [RunPod CLI](/cli/install-runpodctl), you'll need to set your API key in the configuration.\n\n```bash\nrunpodctl config --apiKey $RUNPOD_API_KEY\n```\n\nReplace `$RUNPOD_API_KEY` with your RunPod API key.\n\nOnce your API key is set, you can manage your infrastructure.\n\nIf you're not sure which Pod meets your needs, see [Choose a Pod](/pods/choose-a-pod).","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":1,"to":25}}}}],["119",{"pageContent":"Create Pods\n\n<Tabs groupId=\"interface\">\n\n<TabItem value=\"web-ui\" label=\"Web\" default>\n\n1. Navigate to [Pods](https://www.dev.runpod.io/console/pods) and select **+ Deploy**.\n2. Choose between **GPU** and **CPU**.\n3. Customize your an instance by setting up the following:\n   1. (optional) Specify a Network volume.\n   2. Select an instance type. For example, **A40**.\n   3. (optional) Provide a template. For example, **RunPod Pytorch**.\n   4. (GPU only) Specify your compute count.\n4. Review your configuration and select **Deploy On-Demand**.\n\n</TabItem>\n\n<TabItem value=\"cli\" label=\"Command line\">\n\nTo create a Pod using the CLI, use the `runpodctl create pods` command.\n\n```bash\nrunpodctl create pods \\\n  --name hello-world \\\n  --gpuType \"NVIDIA A40\" \\\n  --imageName \"runpod/pytorch:3.10-2.0.0-117\" \\\n  --containerDiskSize 10 \\\n  --volumeSize 100 \\\n  --args \"bash -c 'mkdir /testdir1 && /start.sh'\"","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":27,"to":55}}}}],["120",{"pageContent":"</TabItem>\n</Tabs>\n\n:::tip\n\nRunPod supports custom [templates](/pods/templates/overview) that allow you to specify your own Dockerfile.\nBy creating a Dockerfile, you can build a [custom Docker image](/tutorials/introduction/containers/overview) with your specific dependencies and configurations.\nThis ensures that your applications are reliable and portable across different environments.\n\n:::\n\nCharges occur after the Pod build is complete.","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":58,"to":69}}}}],["121",{"pageContent":"Stop a Pod\n\n<Tabs groupId=\"interface\">\n\n<TabItem value=\"web-ui\" label=\"Web\" default>\n  1. Click the stop icon.\n  2. Confirm by clicking the **Stop Pod** button.\n  </TabItem>\n\n<TabItem value=\"cli\" label=\"Command line\">\n    To stop a Pod, enter the following command.\n\n    ```bash\n    runpodctl stop pod $RUNPOD_POD_ID\n    ```\n\n</TabItem>\n\n</Tabs>","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":71,"to":89}}}}],["122",{"pageContent":"Stop a Pod after a specific time\n\nYou can also stop a Pod after a specific amount of time.\nFor example, the following command sleeps for 2 hours, and then stops the Pod.\n\n      <Tabs>\n        <TabItem value=\"ssh\" label=\"SSH\">\n\n          Use the following command to stop a Pod after 2 hours:\n\n          ```bash\n          sleep 2h; runpodctl stop pod $RUNPOD_POD_ID &\n          ```\n          This command uses sleep to wait for 2 hours before executing the `runpodctl stop pod` command to stop the Pod.\n          The `&` at the end runs the command in the background, allowing you to continue using the SSH session.\n        </TabItem>\n        <TabItem value=\"web-terminal\" label=\"Web terminal\">\n\n          To stop a Pod after 2 hours using the web terminal, enter:\n\n          ```bash\n          nohup bash -c \"sleep 2h; runpodctl stop pod $RUNPOD_POD_ID\" &\n          ```\n          `nohup` ensures the process continues running if you close the web terminal window.\n        </TabItem>\n      </Tabs>","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":91,"to":116}}}}],["123",{"pageContent":":::warning\n\nYou are charged for storing idle Pods.\nIf you do not need to store your Pod, be sure to terminate it next.\n\n:::","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":118,"to":123}}}}],["124",{"pageContent":"Start a Pod\n\nYou can resume a pod that has been stopped.\n\n<Tabs groupId=\"interface\">\n\n<TabItem value=\"web-ui\" label=\"Web\" default>\n\n1. Navigate to the **Pods** page.\n2. Select your Pod you want to resume.\n3. Select **Start**.\n\nYour Pod will resume.\n\n</TabItem>\n\n<TabItem value=\"cli\" label=\"Command line\">\n  To start a single Pod, enter the command `runpodctl start pod`. You can pass the environment variable `RUNPOD_POD_ID` to identify each Pod.\n\n```bash\nrunpodctl start pod $RUNPOD_POD_ID\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":125,"to":149}}}}],["125",{"pageContent":"Terminate a Pod\n\n:::danger\n\nTerminating a Pod permanently deletes all data outside your [Network volume](/pods/storage/create-network-volumes).\nBe sure you've saved any data you want to access again.\n\n:::\n\n<Tabs groupId=\"interface\">\n\n<TabItem value=\"web-ui\" label=\"Web\" default>\n\n1. Select the hamburger menu at the bottom of the Pod you want to terminate.\n2. Click **Terminate Pod**.\n3. Confirm by clicking the **Yes** button.\n\n</TabItem>\n\n<TabItem value=\"cli\" label=\"Command line\">\n\nTo remove a single Pod, enter the following command.\n\n```bash\nrunpodctl remove pod $RUNPOD_POD_ID\n```\n\nYou can also remove Pods in bulk. For example, the following command terminates up to 40 pods with the name `my-bulk-task`.\n\n```bash\nrunpodctl remove pods my-bulk-task --podCount 40\n```\n\n</TabItem>\n\n</Tabs>\n\n## List Pods\n\nIf you're using the command line, enter the following command to list your pods.\n\n```bash\nrunpodctl get pod\n```","metadata":{"source":"/runpod-docs/docs/pods/manage-pods.md","loc":{"lines":{"from":151,"to":194}}}}],["126",{"pageContent":"---\ntitle: Overview\ndescription: \"Run containers as Pods with a container registry, featuring compatible architectures, Ubuntu Linux, and persistent storage, with customizable options for GPU type, system disk size, and more.\"\nsidebar_position: 1\n---\n\nPods are running container instances.\nYou can pull an instance from a container registry such as Docker Hub, GitHub Container Registry, Amazon Elastic Container Registry, or another compatible registry.\n\n:::note\n\nWhen building an image for RunPod on a Mac (Apple Silicon), use the flag `--platform linux/amd64` to ensure your image is compatible with the platform. This flag is necessary because RunPod currently only supports the `linux/amd64` architecture.\n\n:::","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":1,"to":14}}}}],["127",{"pageContent":"Understanding Pod components and configuration\n\nA Pod is a server container created by you to access the hardware, with a dynamically generated assigned identifier.\nFor example, `2s56cp0pof1rmt` identifies the instance.\n\nA Pod comprises a container volume with the operating system and temporary storage, a disk volume for permanent storage, an Ubuntu Linux container, allocated vCPU and system RAM, optional GPUs or CPUs for specific workloads, a pre-configured template for easy software access, and a proxy connection for web access.\n\nEach Pod encompasses a variety of components:","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":16,"to":23}}}}],["128",{"pageContent":"- A container volume that houses the operating system and temporary storage.\n  - This storage is volatile and will be lost if the Pod is halted or rebooted.\n- A disk volume for permanent storage, preserved for the duration of the Pod's lease, akin to a hard disk.\n  - This storage is persistent and will be available even if the Pod is halted or rebooted.\n- Network storage, similar to a volume but can be moved between machines.\n  - When using network storage, you can only delete the Pod.\n- An Ubuntu Linux container, capable of running almost any software that can be executed on Ubuntu.\n- Assigned vCPU and system RAM dedicated to the container and any processes it runs.\n- Optional GPUs or CPUs, tailored for specific workloads like CUDA or AI/ML tasks, though not mandatory for starting the container.\n- A pre-configured template that automates the installation of software and settings upon Pod creation, offering straightforward, one-click access to various packages.","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":25,"to":34}}}}],["129",{"pageContent":"- A proxy connection for web access, allowing connectivity to any open port on the container.\n  - For example, `https://[pod-id]-[port number].proxy.runpod.net`, or `https://2s56cp0pof1rmt-7860.proxy.runpod.net/`).","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":35,"to":36}}}}],["130",{"pageContent":"To get started, see how to [Choose a Pod](/pods/choose-a-pod) then see the instructions on [Manage Pods](/pods/manage-pods).","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":38,"to":38}}}}],["131",{"pageContent":"Learn more\n\nYou can jump straight to a running Pod by starting from a [template](/pods/templates/overview). For more customization, you can configure the following:\n\n- [GPU Type](/references/gpu-types) and quantity\n- System Disk Size\n- Start Command\n- [Environment Variables](/pods/references/environment-variables)\n- [Expose HTTP/TCP ports](/pods/configuration/expose-ports)\n- [Persistent Storage Options](/category/storage)\n\nTo get started, see how to [Choose a Pod](/pods/choose-a-pod) then see the instructions on [Manage Pods](/pods/manage-pods).","metadata":{"source":"/runpod-docs/docs/pods/overview.md","loc":{"lines":{"from":40,"to":51}}}}],["132",{"pageContent":"---\ntitle: Pod environment variables\ndescription: \"Configure and manage your pods with these essential environment variables, including pod ID, API key, host name, GPU and CPU count, public IP, SSH port, data center ID, volume ID, CUDA version, current working directory, PyTorch version, and public SSH key.\"\nsidebar_position: 4\n---\n\nYou can store the following environment variables in your Pods.","metadata":{"source":"/runpod-docs/docs/pods/references/environment-variables.md","loc":{"lines":{"from":1,"to":7}}}}],["133",{"pageContent":"| Variable              | Description                                                                               |\n| :-------------------- | :---------------------------------------------------------------------------------------- |\n| `RUNPOD_POD_ID`       | The unique identifier for your pod.                                                       |\n| `RUNPOD_API_KEY`      | Used to make RunPod API calls to the specific pod. It's limited in scope to only the pod. |\n| `RUNPOD_POD_HOSTNAME` | Name of the host server the pod is running on.                                            |\n| `RUNPOD_GPU_COUNT`    | Number of GPUs available to the pod.                                                      |\n| `RUNPOD_CPU_COUNT`    | Number of CPUs available to the pod.                                                      |\n| `RUNPOD_PUBLIC_IP`    | If available, the publicly accessible IP for the pod.                                     |","metadata":{"source":"/runpod-docs/docs/pods/references/environment-variables.md","loc":{"lines":{"from":9,"to":16}}}}],["134",{"pageContent":"| `RUNPOD_TCP_PORT_22`  | The public port SSH port 22.                                                              |\n| `RUNPOD_DC_ID`        | The data center where the pod is located.                                                 |\n| `RUNPOD_VOLUME_ID`    | The ID of the volume connected to the pod.                                                |\n| `CUDA_VERSION`        | The installed CUDA version.                                                               |\n| `PWD`                 | Current working directory.                                                                |\n| `PYTORCH_VERSION`     | Installed PyTorch Version.                                                                |\n| `PUBLIC_KEY`          | The SSH public keys to access the pod over SSH.                                           |","metadata":{"source":"/runpod-docs/docs/pods/references/environment-variables.md","loc":{"lines":{"from":17,"to":23}}}}],["135",{"pageContent":"---\ntitle: \"Savings plans\"\ndescription: \"Maximize your RunPod experience with Savings Plans, a cost-saving feature that offers upfront discounts on uninterrupted instances, flexible savings, instant activation, easy management, and clear visibility.\"\nsidebar_position: 9\n---\n\nSavings Plans are a powerful cost-saving feature designed to optimize your RunPod experience. Take advantage of upfront payments to unlock discounts on uninterrupted instances, maximize cost efficiency, and get the most out of specific card types.\n\n## Get started\n\nTo start saving with RunPod's Savings Plans, ensure you have sufficient RunPod credits in your account.\n\nThere are two ways to create a savings plan.\n\n- Add a Savings Plan to your existing Pod from the Pod dashboard.\n- Initiate a Savings Plan during Pod deployment.\n\nRegularly check the **Savings Plans** section to track your Savings Plans and associated pods.\n\n![](/img/docs/f58bad9-image.png)\n![](/img/docs/0eb087a-image.png)","metadata":{"source":"/runpod-docs/docs/pods/savings-plans.md","loc":{"lines":{"from":1,"to":21}}}}],["136",{"pageContent":"Benefits\n\n**Reduced Costs**: By paying upfront for a Savings Plan, you can enjoy discounted rates on uninterrupted instances. This means significant cost savings for your RunPod deployments.\n\n**Flexible Savings**: When you stop a Pod, the Savings Plan associated with it applies to your next deployment of the same card. This means you continue to benefit from your savings commitment even after temporary pauses in your pod usage.\n\n:::warning\n\nStopping your Pod(s) does not extend your Savings Plan. Each Savings Plan has a fixed expiration date, which you set at when you buy the plan.\n\n:::\n\n**Instant Activation**: Savings Plans kick in immediately upon activation and remain active for the duration of your committed period. You can start saving from the moment you initiate a Savings Plan.","metadata":{"source":"/runpod-docs/docs/pods/savings-plans.md","loc":{"lines":{"from":23,"to":35}}}}],["137",{"pageContent":"**Easy Management**: Adding a Savings Plan to your existing running Pod is a breeze through the Pod dashboard. Alternately, Savings Plans are automatically started when you deploy a new Pod, simplifying the process and ensuring you don't miss out on potential savings.\n\n**Clear Visibility**: Stay on top of your savings commitments and associated Pods by navigating to the **Savings Plan** page. This shows a comprehensive overview of your Savings Plans for effective monitoring and management.\n\nEmpower your RunPod deployments with Savings Plans and unlock the potential for cost optimization and enhanced savings. Begin maximizing your RunPod experience today!","metadata":{"source":"/runpod-docs/docs/pods/savings-plans.md","loc":{"lines":{"from":37,"to":41}}}}],["138",{"pageContent":"---\ntitle: \"Create a network volume\"\ndescription: \"Create a network volume in Secure Cloud to access high-performance storage and flexibility for multiple pods, with options for data center selection, naming, and size allocation, and enjoy cost-effective storage solutions with robust infrastructure and NVME SSDs.\"\nsidebar_position: 9\n---\n\nNetwork Volumes are a feature specific to Secure Cloud that allows you to create a volume that multiple pods can interact with. This gives you an extra amount of flexibility to keep working--especially if you are working with a high demand GPU pool that may not always be available--as you can simply create a pod in a different pool while you wait for an option to free up. This can also save you time by downloading frequently used models or other large files to a volume and holding them for later use, rather than having to re-download them every time you spin up a new pod.\n\n**How to Create a Volume**","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":1,"to":9}}}}],["139",{"pageContent":"Under the Secure Cloud page, click the option to create a volume.\n\n![](/img/docs/797cfdc-image.png)\n\nThe pricing for the volume will be shown, and you can select a data center and provide a name and the requested size.\n\n![](/img/docs/596a4f5-image.png)\n\nOnce you create the volume, it will appear in your list.\n\n![](/img/docs/c4eea31-image.png)\n\nOnce you click the Deploy option, your container size will be locked to the size of your network volume. Note that there will be a nominal cost for network volume storage, in lieu of the disk cost normally quoted. Also note that you can link many pods to one singular network volume, and you will enjoy an overall cost saving even with just two pods sharing one volume (as opposed to setting up two pods with separate volumes), despite the presence of this additional cost.\n\n![](/img/docs/8f69d41-image.png)\n\n**What's the infrastructure behind Network Volume**","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":11,"to":27}}}}],["140",{"pageContent":"When you choose to create a Network Volume, you gain access to our robust infrastructure, which includes state-of-the-art storage servers located in the same datacenters where you rent GPU servers from us. These servers are connected via a high-speed 25Gbps local network, up to 200Gbps in some locations, guaranteeing efficient data transfer and minimal latency. Everything is stored on high-speed NVME SSDs to ensure best performance.\n\nIf you're interested in harnessing the advantages of Network Volume and its cost-effective storage solutions, we invite you to read our detailed [blog article](https://blog.runpod.io/four-reasons-to-set-up-a/). It explores the benefits and features of Network Volume, helping you make an informed decision about your storage needs.","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":29,"to":31}}}}],["141",{"pageContent":"**Please note that if your machine-based storage or network volume is terminated due to lack of funds, that disk space is immediately freed up for use by other clients, and RunPod is unable to assist in recovering lost storage.**","metadata":{"source":"/runpod-docs/docs/pods/storage/create-network-volumes.md","loc":{"lines":{"from":33,"to":33}}}}],["142",{"pageContent":"---\ntitle: \"Sync a volume to a cloud provider\"\nsidebar_position: 9\ndescription: \"Sync your volume to a cloud provider by clicking 'Cloud Sync' on your My Pods page, then follow provider-specific instructions from the dropdown menu.\"\n---\n\nYou can sync your volume to a cloud provider by clicking the Cloud Sync option under your My Pods page.\nClick the dropdown on the left to get specific instructions for your specific provider.","metadata":{"source":"/runpod-docs/docs/pods/storage/sync-volumes.md","loc":{"lines":{"from":1,"to":8}}}}],["143",{"pageContent":"---\ntitle: \"Transfer files with SCP\"\nsidebar_position: 9\ndescription: \"Transfer files to and from your Pod using SCP and rsync commands. Prerequisites include a Linux or WSL instance, SSH configured, and rsync installed. Follow syntax guides for secure file transfer and option flags for customization.\"\n---\n\nLearn to transfer files to and from RunPod with Secure Copy Protocol (SCP).\n\n## Prerequisites\n\n- Make sure your Pod is configured to use real SSH.\n  For more information, see [use SSH](/pods/configuration/use-ssh).\n\n- If you intend to use rsync, make sure it's installed on both your local machine and your Pod with `apt install rsync`.\n\n- Note the public IP address and external port from the SSH over exposed TCP command (you'll need these for the SCP/rsync commands).","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":1,"to":16}}}}],["144",{"pageContent":"Transferring with SCP\n\nThe general syntax for sending files to a Pod with SCP is as follows (execute this on your local machine, and replace the x's with your Pod's external TCP port and IP; for this example, they are 43201 and 194.26.196.6, respectively):\n\n```shell\nscp -P 43201 -i ~/.ssh/id_ed25519 /local/file/path root@194.26.196.6:/destination/file/path\n```\n\n:::note\n\nIf your private key file is in a location other than `~/.ssh/id_ed25519` or you're using the Windows Command Prompt, make sure you update this path accordingly in your command.\n\n:::\n\nExample of sending a file to a Pod:\n\n```shell\nscp -P 43201 -i ~/.ssh/id_ed25519 ~/documents/example.txt root@194.26.196.6:/root/example.txt\n```\n\nIf you want to receive a file from your Pod, switch the source and destination arguments:\n\n```shell\nscp -P 43201 -i ~/.ssh/id_ed25519 root@194.26.196.6:/root/example.txt ~/documents/example.txt","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":18,"to":41}}}}],["145",{"pageContent":"If you need to transfer a directory, use the `-r` flag to recursively copy files and subdirectories (this will follow any symbolic links encountered as well):\n\n```shell\nscp -r -P 43201 -i ~/.ssh/id_ed25519 ~/documents/example_dir root@194.26.196.6:/root/example_dir\n```","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":44,"to":48}}}}],["146",{"pageContent":"Transferring with rsync\n\n:::note\n\nYour local machine must be running Linux or a [WSL instance](https://learn.microsoft.com/en-us/windows/wsl/about) in order to use rsync.\n\n:::\n\nThe general syntax for sending files to a Pod with rsync is as follows (execute this on your local machine, and replace the x's with your Pod's external TCP port and IP):\n\n```shell\nrsync -e \"ssh -p 43201\" /source/file/path root@194.26.196.6:/destination/file/path","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":50,"to":61}}}}],["147",{"pageContent":"Some helpful flags include:\n\n- `-a`/`--archive` - archive mode (ensures that permissions, timestamps, and other attributes are preserved during the transfer; use this when transferring directories or their contents)\n- `-d`/`--delete` - deletes files in the destination directory that are not present in the source\n- `-p`/`--progress` - displays file transfer progress\n- `-v`/`--verbose` - verbose output\n- `-z`/`--compress` - compresses data as it's being sent and uncompresses as it's received (heavier on your CPU, but easier on your network connection)\n\nExample of sending a file to a Pod using rsync:\n\n```shell\nrsync -avz -e \"ssh -p 43201\" ~/documents/example.txt root@194.26.196.6:/root/example.txt\n```\n\nIf you want to receive a file from your Pod, switch the source and destination arguments:\n\n```shell\nrsync -avz -e \"ssh -p 43201\" root@194.26.196.6:/root/example.txt ~/documents/example.txt","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":64,"to":81}}}}],["148",{"pageContent":"To transfer the contents of a directory (without transferring the directory itself), use a trailing slash in the file path:\n\n```shell\nrsync -avz -e \"ssh -p 43201\" ~/documents/example_dir/ root@194.26.196.6:/root/example_dir/\n```\n\nWithout a trailing slash, the directory itself is transferred:\n\n```shell\nrsync -avz -e \"ssh -p 43201\" ~/documents/example_dir root@194.26.196.6:/root/","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":84,"to":93}}}}],["149",{"pageContent":"An advantage of rsync is that files that already exist at the destination aren't transferred again if you attempt to copy them twice (note the minimal data transfer after the second execution):\n\n```shell\nrsync -avz -e \"ssh -p 43201\" ~/documents/example.txt root@194.26.196.6:/root/example.txt\nsending incremental file list\nexample.txt\n             119 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=0/1)\n\nsent 243 bytes  received 35 bytes  185.33 bytes/sec\ntotal size is 119  speedup is 0.43\n\n$ rsync -avz -e \"ssh -p 43201\" ~/documents/example.txt root@194.26.196.6:/root/example.txt\nsending incremental file list\n\nsent 120 bytes  received 12 bytes  88.00 bytes/sec\ntotal size is 119  speedup is 0.90\n```","metadata":{"source":"/runpod-docs/docs/pods/storage/transfer-files-with-scp.md","loc":{"lines":{"from":96,"to":112}}}}],["150",{"pageContent":"---\ntitle: Storage types\nsidebar_position: 8\n---\n\nThe following section describes the different types of storage and volume options.\n\n## Container volume\n\nA container volume is a type of storage that houses the operating system and provides temporary storage for a Pod.\nIt is created when a Pod is launched and is tightly coupled with the Pod's lifecycle.\n\n**Key characteristics:**\n\n- Volatile storage that is lost if the Pod is halted or rebooted\n- Suitable for storing temporary data or files that are not required to persist beyond the Pod's lifecycle\n- Capacity is determined by the selected Pod configuration\n- Provides fast read and write speeds as it is locally attached to the Pod","metadata":{"source":"/runpod-docs/docs/pods/storage/types.md","loc":{"lines":{"from":1,"to":18}}}}],["151",{"pageContent":"Disk volume\n\nA disk volume is a type of persistent storage that is preserved for the duration of the Pod's lease.\nIt functions similarly to a hard disk, allowing you to store data that needs to be retained even if the Pod is halted or rebooted.\n\n**Key characteristics:**\n\n- Persistent storage that remains available throughout the Pod's lease period\n- Suitable for storing data, models, or files that need to be preserved across Pod restarts or reconfigurations\n- Capacity can be selected based on storage requirements\n- Provides reliable data persistence but may have slightly slower read and write speeds compared to container volumes","metadata":{"source":"/runpod-docs/docs/pods/storage/types.md","loc":{"lines":{"from":20,"to":30}}}}],["152",{"pageContent":"Network storage\n\nNetwork storage is a type of storage that is similar to a disk volume but offers the flexibility to be moved between different machines.\nIt provides a way to store and access data across multiple Pods or instances.\n\n**Key characteristics:**\n\n- Persistent storage that can be attached to different Pods or machines\n- Suitable for scenarios where data needs to be shared or accessed by multiple Pods\n- Allows for data portability and facilitates collaboration between different instances\n- Provides data persistence and the ability to move storage between Pods\n- When using network storage, you can only delete the Pod, as the storage is managed separately","metadata":{"source":"/runpod-docs/docs/pods/storage/types.md","loc":{"lines":{"from":32,"to":43}}}}],["153",{"pageContent":"---\ntitle: Volumes\ndescription: \"Explore the concept of volumes in computing, where data can be stored as persistent or ephemeral resources, each with its own unique characteristics and applications.\"\n---\n\nVolumes can be:\n\n## Persistent\n\n## Ephemeral","metadata":{"source":"/runpod-docs/docs/pods/storage/_volume.md","loc":{"lines":{"from":1,"to":10}}}}],["154",{"pageContent":"---\ntitle: Manage Pod Templates\ndescription: \"Discover and create custom templates for your pods, define environment variables, and use RunPod's API to launch and manage your applications with ease.\"\nsidebar_position: 3\n---\n\n## Explore Templates\n\nYou can explore Templates managed by RunPod and Community Templates in the **[Explore](https://www.runpod.io/console/explore)** section of the Web interface.\n\nYou can explore Templates managed by you or your team in the **[Templates](https://www.runpod.io/console/user/templates)** section of the Web interface.\n\nLearn to create your own Template in the following section.","metadata":{"source":"/runpod-docs/docs/pods/templates/manage-templates.md","loc":{"lines":{"from":1,"to":13}}}}],["155",{"pageContent":"Creating a Template\n\nTemplates are used to launch images as a Pod: within a template, you define the required container disk size, volume, volume path, and ports needed.\n\n### Web interface\n\n![](/img/docs/8418b2b-image.png)\n\n### cURL\n\nYou can also create or modify a template using the RunPod API.\n\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \\\"sleep infinity\\\", env: [ { key: \\\"key1\\\", value: \\\"value1\\\" }, { key: \\\"key2\\\", value: \\\"value2\\\" } ], imageName: \\\"ubuntu:latest\\\", name: \\\"Generated Template\\\", ports: \\\"8888/http,22/tcp\\\", readme: \\\"## Hello, World!\\\", volumeInGb: 15, volumeMountPath: \\\"/workspace\\\" }) { containerDiskInGb dockerArgs env { key value } id imageName name ports readme volumeInGb volumeMountPath } }\"}'\n```","metadata":{"source":"/runpod-docs/docs/pods/templates/manage-templates.md","loc":{"lines":{"from":15,"to":32}}}}],["156",{"pageContent":"Environment variables\n\nEnvironment variables in RunPod templates are key-value pairs that are accessible within your pod. Define a variable by setting a name with the `key` and then what it should contain with the `value`.\n\nUse environment variables to pass configuration settings and secrets to your container. For example, environment variables can store the path to a database or API keys used by your application.\n\n![](/img/docs/b7670dd-image.png)\n\nRunPod also provides a set of predefined [environment variables](/pods/references/environment-variables) that provide information about the pod, such as the unique identifier for your pod (`RUNPOD_POD_ID`), the API key used to make RunPod API calls to the specific pod (`RUNPOD_API_KEY`), the name of the host server the pod is running on (`RUNPOD_POD_HOSTNAME`), and more.\n\nYou can references [Secrets](/pods/templates/secrets) in your Pod templates.","metadata":{"source":"/runpod-docs/docs/pods/templates/manage-templates.md","loc":{"lines":{"from":34,"to":44}}}}],["157",{"pageContent":"---\ntitle: Overview\ndescription: \"Docker templates: pre-configured images with customizable settings for deploying Pods, environment variables, and port management, with options for official, community, and custom templates.\"\nsidebar_position: 2\n---\n\nTemplates are Docker containers images paired with a configuration.\n\nThey are used to launch images as Pods, define the required container disk size, volume, volume paths, and ports needed.\n\nYou can also define environment variables within the Template.","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":1,"to":11}}}}],["158",{"pageContent":"Template types\n\nThere a few types of Templates:\n\n- **Managed by RunPod**: Also known as offical Templates; these templates are created and maintained by RunPod.\n- **Custom Templates**:\n  - **Community Templates**: Custom Templates shared by the community.\n  - **Private Templates**: Custom Templates created by you or if using a team account, shared inside your team.\n\n### Custom Templates","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":13,"to":22}}}}],["159",{"pageContent":"Customizing Container Start Command\n\nYou can customize the Docker command to run additional commands or modify the default behavior.\n\nThe Docker command is specified in the **Container Start Command** field.\n\n**Default Docker Command**\n\nThe default Docker command is:\n\n```bash\nbash -c '/start.sh'\n```\n\nThis command runs the `/start.sh` script at the end of the container startup process.\n\nYou can customize the Docker command to run additional commands or modify the default behavior.\n\nFor example, you can add a command to run before `/start.sh`:\n\n```bash\nbash -c 'mkdir /testdir1 && /start.sh'\n```\n\nThis command creates a directory `/testdir1` before running `/start.sh`.\n\n**Using the `entrypoint` Field**\n\nYou can also specify a JSON string with `cmd` and `entrypoint` as the keys.\n\nThe `entrypoint` field allows you to specify a command to run at the beginning of the container startup process. For example:\n\n```json\n{ \"cmd\": [\"echo foo && /start.sh\"], \"entrypoint\": [\"bash\", \"-c\"] }","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":24,"to":57}}}}],["160",{"pageContent":"This command runs the `echo` command and then runs `/start.sh`.\n\n**Important Considerations**\n\nWhen using the `entrypoint` field, be aware that the command will run twice: once as the entrypoint and again as part of the `cmd` field.\n\nThis can cause issues if the command errors when run a second time.\nFor example:\n\n```json\n{ \"cmd\": [\"mkdir /testdir11 && /start.sh\"], \"entrypoint\": [\"bash\", \"-c\"] }\n```\n\nThis command will run `mkdir` twice, which can cause errors if the directory already exists.\n\n**Tips and Examples**\n\nHere are some working examples to try in dev:\n\n- Command only: `bash -c 'mkdir /testdir1 && /start.sh'`\n- Command only: `{\"cmd\": [\"bash\", \"-c\", \"mkdir /testdir8 && /start.sh\"]}`\n- Command and Entrypoint: `{\"cmd\": [\"test-echo-test-echo\"], \"entrypoint\": [\"echo\"]}`\n- Cmmand and Entrypoint: `{\"cmd\": [\"mkdir -p /testdir12 && /start.sh\"], \"entrypoint\": [\"bash\", \"-c\"]}`\n\n:::note\n\nRemember to use `mkdir -p` to avoid errors when creating directories.\n\n:::","metadata":{"source":"/runpod-docs/docs/pods/templates/overview.md","loc":{"lines":{"from":60,"to":88}}}}],["161",{"pageContent":"---\ntitle: Secrets\ndescription: \"Manage sensitive data with RunPod Secrets, encrypted strings for storing passwords, API keys, and more, via the Web interface or API, with options to create, modify, view, and delete Secrets for secure use in Pods and templates.\"\nsidebar_position: 4\n---\n\nYou can add Secrets to your Pods and templates.\nSecrets are encrypted strings of text that are used to store sensitive information, such as passwords, API keys, and other sensitive data.","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":1,"to":8}}}}],["162",{"pageContent":"Create a Secret\n\nYou can create a Secret using the RunPod Web interface or the RunPod API.\n\n1. Login into the RunPod Web interface and select [Secrets](https://www.runpod.io/console/user/secrets).\n2. Choose **Create Secret** and provide the following:\n   1. **Secret Name**: The name of the Secret.\n   2. **Secret Value**: The value of the Secret.\n   3. **Description**: (optional) A description of the Secret.\n3. Select **Create Secret**.\n\n:::note\n\nOnce a Secret is created, its value cannot be viewed.\nIf you need to change the Secret, you must create a new one or [modify the Secret Value](#modify-a-secret).\n\n:::\n\n## Modify a Secret\n\nYou can modify an existing Secret using the RunPod Web interface.\n\n1. Login into the RunPod Web interface and select [Secrets](https://www.runpod.io/console/user/secrets).\n2. Select the name of the Secret you want to modify.\n3. Select the configuration icon and choose **Edit Secret Value**.\n   1. Enter your new Secret Value.\n4. Select **Save Changes**.","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":10,"to":36}}}}],["163",{"pageContent":"View Secret details\n\nYou can view the details of an existing Secret using the RunPod Web interface.\nYou can't view the Secret Value.\n\n1. Login into the RunPod Web interface and select [Secrets](https://www.runpod.io/console/user/secrets).\n2. Select the name of the Secret you want to view.\n3. Select the configuration icon and choose **View Secret**.","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":38,"to":45}}}}],["164",{"pageContent":"Use a Secret in a Pod\n\nWith your Secrets setup, you can now reference them in your Pods.\n\nYou can reference your Secret directly or select it from the Web interface when creating or modifying a Pod template.\n\n**Reference your Secret directly**\n\nYou can reference your Secret directly in the [Environment Variables](/pods/references/environment-variables) section of your Pod template.\nTo reference your Secret, reference it's key appended to the `RUNPOD_SECRET_` prefix.\nFor example:\n\n```yml\n{{ RUNPOD_SECRET_hello_world }}\n```\n\nWhere `hello_world` is the value of your Secret Name.\n\n**Select your Secret from the Web interface**\n\nAlternatively, you can select your Secret from the Web interface when creating or modifying a Pod template.","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":47,"to":67}}}}],["165",{"pageContent":"Delete a Secret\n\nYou can delete an existing Secret using the RunPod Web interface.\n\n1. Login into the RunPod Web interface and select [Secrets](https://www.runpod.io/console/user/secrets).\n2. Select the name of the Secret you want to delete.\n3. Select the configuration icon and choose **Delete Secret**.\n4. Enter the name of the Secret to confirm deletion.\n5. Select **Confirm Delete**.","metadata":{"source":"/runpod-docs/docs/pods/templates/secrets.md","loc":{"lines":{"from":69,"to":77}}}}],["166",{"pageContent":"---\ntitle: Serverless CPU types\n---\n\nThe following list contains all CPU types available on RunPod.\n\n<!--\nTable last generated: 2024-06-04\n-->","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":1,"to":9}}}}],["167",{"pageContent":"| displayName                                     | cores | threadsPerCore |\n| :---------------------------------------------- | ----: | -------------: |\n| 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz   |     6 |              2 |\n| 11th Gen Intel(R) Core(TM) i5-11400F @ 2.60GHz  |     6 |              2 |\n| 11th Gen Intel(R) Core(TM) i7-11700 @ 2.50GHz   |     8 |              2 |\n| 11th Gen Intel(R) Core(TM) i7-11700F @ 2.50GHz  |     8 |              2 |\n| 11th Gen Intel(R) Core(TM) i7-11700K @ 3.60GHz  |     8 |              2 |\n| 11th Gen Intel(R) Core(TM) i7-11700KF @ 3.60GHz |     8 |              2 |\n| 11th Gen Intel(R) Core(TM) i9-11900K @ 3.50GHz  |     8 |              2 |\n| 11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz |     8 |              2 |\n| 12th Gen Intel(R) Core(TM) i3-12100             |     4 |              2 |\n| 12th Gen Intel(R) Core(TM) i7-12700F            |    12 |              1 |\n| 12th Gen Intel(R) Core(TM) i7-12700K            |    12 |              1 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":11,"to":23}}}}],["168",{"pageContent":"| 13th Gen Intel(R) Core(TM) i3-13100F            |     4 |              2 |\n| 13th Gen Intel(R) Core(TM) i5-13600K            |    14 |              1 |\n| 13th Gen Intel(R) Core(TM) i7-13700K            |    16 |              1 |\n| 13th Gen Intel(R) Core(TM) i7-13700KF           |    16 |              1 |\n| 13th Gen Intel(R) Core(TM) i9-13900F            |    24 |              1 |\n| 13th Gen Intel(R) Core(TM) i9-13900K            |    24 |              1 |\n| 13th Gen Intel(R) Core(TM) i9-13900KF           |    24 |              1 |\n| AMD EPYC 7251 8-Core Processor                  |     8 |              2 |\n| AMD EPYC 7252 8-Core Processor                  |     8 |              2 |\n| AMD EPYC 7272 12-Core Processor                 |    12 |              2 |\n| AMD EPYC 7281 16-Core Processor                 |    16 |              2 |\n| AMD EPYC 7282 16-Core Processor                 |    16 |              2 |\n| AMD EPYC 7302 16-Core Processor                 |    16 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":24,"to":36}}}}],["169",{"pageContent":"| AMD EPYC 7302P 16-Core Processor                |    16 |              2 |\n| AMD EPYC 7313 16-Core Processor                 |    16 |              2 |\n| AMD EPYC 7313P 16-Core Processor                |    16 |              2 |\n| AMD EPYC 7343 16-Core Processor                 |    16 |              2 |\n| AMD EPYC 7351P 16-Core Processor                |    16 |              2 |\n| AMD EPYC 7352 24-Core Processor                 |    24 |              2 |\n| AMD EPYC 7402 24-Core Processor                 |    24 |              2 |\n| AMD EPYC 7402P 24-Core Processor                |    24 |              2 |\n| AMD EPYC 7413 24-Core Processor                 |    24 |              2 |\n| AMD EPYC 7443 24-Core Processor                 |    48 |              1 |\n| AMD EPYC 7443P 24-Core Processor                |    24 |              2 |\n| AMD EPYC 7452 32-Core Processor                 |    32 |              2 |\n| AMD EPYC 7453 28-Core Processor                 |    28 |              1 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":37,"to":49}}}}],["170",{"pageContent":"| AMD EPYC 7502 32-Core Processor                 |    32 |              1 |\n| AMD EPYC 7502P 32-Core Processor                |    32 |              1 |\n| AMD EPYC 7513 32-Core Processor                 |    32 |              2 |\n| AMD EPYC 7532 32-Core Processor                 |    32 |              2 |\n| AMD EPYC 7542 32-Core Processor                 |    32 |              2 |\n| AMD EPYC 7543 32-Core Processor                 |    28 |              1 |\n| AMD EPYC 7543P 32-Core Processor                |    32 |              2 |\n| AMD EPYC 7551 32-Core Processor                 |    32 |              2 |\n| AMD EPYC 7551P 32-Core Processor                |    32 |              2 |\n| AMD EPYC 7552 48-Core Processor                 |    48 |              2 |\n| AMD EPYC 75F3 32-Core Processor                 |    32 |              2 |\n| AMD EPYC 7601 32-Core Processor                 |    32 |              2 |\n| AMD EPYC 7642 48-Core Processor                 |    48 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":50,"to":62}}}}],["171",{"pageContent":"| AMD EPYC 7643 48-Core Processor                 |    48 |              2 |\n| AMD EPYC 7663 56-Core Processor                 |    56 |              2 |\n| AMD EPYC 7702 64-Core Processor                 |    64 |              2 |\n| AMD EPYC 7702P 64-Core Processor                |    64 |              2 |\n| AMD EPYC 7713 64-Core Processor                 |    64 |              1 |\n| AMD EPYC 7742 64-Core Processor                 |    64 |              2 |\n| AMD EPYC 7763 64-Core Processor                 |    64 |              2 |\n| AMD EPYC 7773X 64-Core Processor                |    64 |              2 |\n| AMD EPYC 7B12 64-Core Processor                 |    64 |              2 |\n| AMD EPYC 7B13 64-Core Processor                 |    64 |              1 |\n| AMD EPYC 7F32 8-Core Processor                  |     8 |              2 |\n| AMD EPYC 7F72 24-Core Processor                 |    24 |              2 |\n| AMD EPYC 7H12 64-Core Processor                 |    64 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":63,"to":75}}}}],["172",{"pageContent":"| AMD EPYC 7R32 48-Core Processor                 |    48 |              2 |\n| AMD EPYC 7T83 64-Core Processor                 |   127 |              1 |\n| AMD EPYC 7V13 64-Core Processor                 |    24 |              1 |\n| AMD EPYC 9124 16-Core Processor                 |    16 |              2 |\n| AMD EPYC 9254 24-Core Processor                 |    24 |              2 |\n| AMD EPYC 9354 32-Core Processor                 |    32 |              2 |\n| AMD EPYC 9354P 32-Core Processor                |    32 |              2 |\n| AMD EPYC 9374F 32-Core Processor                |    32 |              1 |\n| AMD EPYC 9474F 48-Core Processor                |    48 |              2 |\n| AMD EPYC 9554 64-Core Processor                 |   126 |              1 |\n| AMD EPYC 9654 96-Core Processor                 |    96 |              2 |\n| AMD EPYC 9754 128-Core Processor                |   128 |              2 |\n| AMD EPYC Processor                              |     1 |              1 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":76,"to":88}}}}],["173",{"pageContent":"| AMD EPYC Processor (with IBPB)                  |    16 |              1 |\n| AMD EPYC-Rome Processor                         |    16 |              1 |\n| AMD Eng Sample: 100-000000053-04_32/20_N        |    48 |              1 |\n| AMD Ryzen 3 2200G with Radeon Vega Graphics     |     4 |              1 |\n| AMD Ryzen 3 3200G with Radeon Vega Graphics     |     4 |              1 |\n| AMD Ryzen 3 4100 4-Core Processor               |     4 |              2 |\n| AMD Ryzen 5 1600 Six-Core Processor             |     6 |              2 |\n| AMD Ryzen 5 2600 Six-Core Processor             |     6 |              2 |\n| AMD Ryzen 5 2600X Six-Core Processor            |     6 |              2 |\n| AMD Ryzen 5 3600 6-Core Processor               |     6 |              2 |\n| AMD Ryzen 5 3600X 6-Core Processor              |     6 |              2 |\n| AMD Ryzen 5 5500                                |     6 |              2 |\n| AMD Ryzen 5 5600G with Radeon Graphics          |     6 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":89,"to":101}}}}],["174",{"pageContent":"| AMD Ryzen 5 7600 6-Core Processor               |     6 |              2 |\n| AMD Ryzen 5 PRO 2600 Six-Core Processor         |     6 |              2 |\n| AMD Ryzen 7 1700 Eight-Core Processor           |     8 |              2 |\n| AMD Ryzen 7 1700X Eight-Core Processor          |     8 |              2 |\n| AMD Ryzen 7 5700G with Radeon Graphics          |     8 |              2 |\n| AMD Ryzen 7 5700X 8-Core Processor              |     8 |              2 |\n| AMD Ryzen 7 5800X 8-Core Processor              |     8 |              2 |\n| AMD Ryzen 7 7700 8-Core Processor               |     8 |              2 |\n| AMD Ryzen 9 3900X 12-Core Processor             |    12 |              2 |\n| AMD Ryzen 9 5950X 16-Core Processor             |    16 |              2 |\n| AMD Ryzen 9 7950X 16-Core Processor             |    16 |              2 |\n| AMD Ryzen Threadripper 1900X 8-Core Processor   |     8 |              2 |\n| AMD Ryzen Threadripper 1920X 12-Core Processor  |    12 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":102,"to":114}}}}],["175",{"pageContent":"| AMD Ryzen Threadripper 1950X 16-Core Processor  |    16 |              2 |\n| AMD Ryzen Threadripper 2920X 12-Core Processor  |    12 |              2 |\n| AMD Ryzen Threadripper 2950X 16-Core Processor  |    16 |              2 |\n| AMD Ryzen Threadripper 2970WX 24-Core Processor |    24 |              1 |\n| AMD Ryzen Threadripper 2990WX 32-Core Processor |    32 |              2 |\n| AMD Ryzen Threadripper 3960X 24-Core Processor  |    24 |              2 |\n| AMD Ryzen Threadripper PRO 3975WX 32-Cores      |    32 |              2 |\n| AMD Ryzen Threadripper PRO 3995WX 64-Cores      |    64 |              2 |\n| AMD Ryzen Threadripper PRO 5945WX 12-Cores      |    12 |              2 |\n| AMD Ryzen Threadripper PRO 5955WX 16-Cores      |    16 |              2 |\n| AMD Ryzen Threadripper PRO 5965WX 24-Cores      |    24 |              2 |\n| AMD Ryzen Threadripper PRO 5975WX 32-Cores      |    32 |              2 |\n| AMD Ryzen Threadripper PRO 5995WX 64-Cores      |    18 |              1 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":115,"to":127}}}}],["176",{"pageContent":"| AMD Ryzen Threadripper PRO 7985WX 64-Cores      |   112 |              1 |\n| Common KVM processor                            |    28 |              1 |\n| Genuine Intel(R) CPU $0000%@                    |    24 |              2 |\n| Genuine Intel(R) CPU @ 2.20GHz                  |    14 |              2 |\n| Intel Xeon Processor (Icelake)                  |    40 |              2 |\n| Intel(R) Celeron(R) CPU G3900 @ 2.80GHz         |     2 |              1 |\n| Intel(R) Celeron(R) G5905 CPU @ 3.50GHz         |     2 |              1 |\n| Intel(R) Core(TM) i3-10100F CPU @ 3.60GHz       |     4 |              2 |\n| Intel(R) Core(TM) i3-10105F CPU @ 3.70GHz       |     4 |              2 |\n| Intel(R) Core(TM) i3-6100 CPU @ 3.70GHz         |     2 |              2 |\n| Intel(R) Core(TM) i3-9100F CPU @ 3.60GHz        |     4 |              1 |\n| Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        |     6 |              2 |\n| Intel(R) Core(TM) i5-10400F CPU @ 2.90GHz       |     6 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":128,"to":140}}}}],["177",{"pageContent":"| Intel(R) Core(TM) i5-10600 CPU @ 3.30GHz        |     6 |              2 |\n| Intel(R) Core(TM) i5-4570 CPU @ 3.20GHz         |     4 |              1 |\n| Intel(R) Core(TM) i5-6400 CPU @ 2.70GHz         |     4 |              1 |\n| Intel(R) Core(TM) i5-6500 CPU @ 3.20GHz         |     4 |              1 |\n| Intel(R) Core(TM) i5-7400 CPU @ 3.00GHz         |     4 |              1 |\n| Intel(R) Core(TM) i5-9400F CPU @ 2.90GHz        |     6 |              1 |\n| Intel(R) Core(TM) i7-10700F CPU @ 2.90GHz       |     8 |              2 |\n| Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz       |     8 |              2 |\n| Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz         |     4 |              2 |\n| Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz         |     4 |              2 |\n| Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz         |     4 |              2 |\n| Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz        |     4 |              2 |\n| Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz        |     6 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":141,"to":153}}}}],["178",{"pageContent":"| Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz        |     4 |              2 |\n| Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz         |     6 |              2 |\n| Intel(R) Core(TM) i7-9700 CPU @ 3.00GHz         |     8 |              1 |\n| Intel(R) Core(TM) i9-10940X CPU @ 3.30GHz       |    14 |              2 |\n| Intel(R) Core(TM) i9-14900K                     |    24 |              1 |\n| Intel(R) Pentium(R) CPU G3260 @ 3.30GHz         |     2 |              1 |\n| Intel(R) Pentium(R) CPU G4560 @ 3.50GHz         |     2 |              2 |\n| Intel(R) Xeon(R) Bronze 3204 CPU @ 1.90GHz      |     6 |              1 |\n| Intel(R) Xeon(R) CPU X5660 @ 2.80GHz            |     6 |              2 |\n| Intel(R) Xeon(R) CPU E3-1220 v3 @ 3.10GHz       |     4 |              1 |\n| Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz       |     4 |              1 |\n| Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz       |     6 |              2 |\n| Intel(R) Xeon(R) CPU E5-2603 v3 @ 1.60GHz       |     6 |              1 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":154,"to":166}}}}],["179",{"pageContent":"| Intel(R) Xeon(R) CPU E5-2609 0 @ 2.40GHz        |     4 |              1 |\n| Intel(R) Xeon(R) CPU E5-2609 v3 @ 1.90GHz       |     1 |              1 |\n| Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz       |     8 |              2 |\n| Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz        |     6 |              2 |\n| Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz       |     6 |              2 |\n| Intel(R) Xeon(R) CPU E5-2637 v2 @ 3.50GHz       |     4 |              2 |\n| Intel(R) Xeon(R) CPU E5-2643 0 @ 3.30GHz        |     4 |              1 |\n| Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz       |    10 |              2 |\n| Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz       |    12 |              2 |\n| Intel(R) Xeon(R) CPU E5-2660 v2 @ 2.20GHz       |    10 |              2 |\n| Intel(R) Xeon(R) CPU E5-2667 v2 @ 3.30GHz       |     1 |              1 |\n| Intel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz       |     8 |              2 |\n| Intel(R) Xeon(R) CPU E5-2667 v4 @ 3.20GHz       |     1 |              1 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":167,"to":179}}}}],["180",{"pageContent":"| Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz        |     8 |              2 |\n| Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz       |    10 |              2 |\n| Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz       |    20 |              2 |\n| Intel(R) Xeon(R) CPU E5-2678 v3 @ 2.50GHz       |    12 |              2 |\n| Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz       |    12 |              2 |\n| Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz       |    14 |              2 |\n| Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz       |    16 |              2 |\n| Intel(R) Xeon(R) CPU E5-2690 0 @ 2.90GHz        |     8 |              2 |\n| Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz       |    14 |              2 |\n| Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz       |    18 |              2 |\n| Intel(R) Xeon(R) CPU E5-2696 v3 @ 2.30GHz       |    18 |              2 |\n| Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz       |    22 |              2 |\n| Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz       |    16 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":180,"to":192}}}}],["181",{"pageContent":"| Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz       |    20 |              2 |\n| Intel(R) Xeon(R) CPU E5-2699 v3 @ 2.30GHz       |     1 |              1 |\n| Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz       |    22 |              2 |\n| Intel(R) Xeon(R) CPU E5-4667 v3 @ 2.00GHz       |    16 |              2 |\n| Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz        |    12 |              2 |\n| Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz       |    20 |              2 |\n| Intel(R) Xeon(R) Gold 5320 CPU @ 2.20GHz        |    26 |              2 |\n| Intel(R) Xeon(R) Gold 6133 CPU @ 2.50GHz        |    40 |              1 |\n| Intel(R) Xeon(R) Gold 6136 CPU @ 3.00GHz        |    12 |              2 |\n| Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz        |    20 |              2 |\n| Intel(R) Xeon(R) Gold 6150 CPU @ 2.70GHz        |    18 |              2 |\n| Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz        |    12 |              2 |\n| Intel(R) Xeon(R) Gold 6238R CPU @ 2.20GHz       |    28 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":193,"to":205}}}}],["182",{"pageContent":"| Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz       |    24 |              2 |\n| Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz       |    16 |              1 |\n| Intel(R) Xeon(R) Gold 6252 CPU @ 2.10GHz        |    24 |              1 |\n| Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz       |    22 |              2 |\n| Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz        |    24 |              2 |\n| Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz        |    28 |              2 |\n| Intel(R) Xeon(R) Gold 6448Y                     |    32 |              2 |\n| Intel(R) Xeon(R) Platinum 8160 CPU @ 2.10GHz    |    24 |              2 |\n| Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz   |    26 |              2 |\n| Intel(R) Xeon(R) Platinum 8173M CPU @ 2.00GHz   |    28 |              2 |\n| Intel(R) Xeon(R) Platinum 8176M CPU @ 2.10GHz   |    28 |              2 |\n| Intel(R) Xeon(R) Platinum 8180 CPU @ 2.50GHz    |    28 |              2 |\n| Intel(R) Xeon(R) Platinum 8352Y CPU @ 2.20GHz   |    32 |              2 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":206,"to":218}}}}],["183",{"pageContent":"| Intel(R) Xeon(R) Platinum 8452Y                 |    36 |              2 |\n| Intel(R) Xeon(R) Platinum 8468                  |    48 |              2 |\n| Intel(R) Xeon(R) Platinum 8470                  |    52 |              2 |\n| Intel(R) Xeon(R) Platinum 8480+                 |    56 |              2 |\n| Intel(R) Xeon(R) Platinum 8480C                 |    56 |              2 |\n| Intel(R) Xeon(R) Platinum 8480CL                |    56 |              2 |\n| Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz      |    10 |              2 |\n| Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz      |    10 |              2 |\n| Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz      |    24 |              1 |\n| Intel(R) Xeon(R) Silver 4310T CPU @ 2.30GHz     |    10 |              2 |\n| Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz      |    16 |              2 |\n| Intel(R) Xeon(R) W-2223 CPU @ 3.60GHz           |     4 |              2 |\n| QEMU Virtual CPU version 2.5+                   |    16 |              1 |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":219,"to":231}}}}],["184",{"pageContent":"| Ryzen 5 5600X                                   |     6 |              2 |\n| Ryzen 9 5900X                                   |    12 |              2 |\n| Ryzen Threadripper PRO 3955WX                   |    16 |              2 |\n| unknown                                         |     0 |              0 |\n| nan                                             |   nan |            nan |\n| nan                                             |   nan |            nan |","metadata":{"source":"/runpod-docs/docs/references/cpu-types.md","loc":{"lines":{"from":232,"to":237}}}}],["185",{"pageContent":"---\ntitle: \"FAQ\"\ndescription: \"RunPod offers two cloud computing services: Secure Cloud and Community Cloud. Secure Cloud provides high-reliability, while Community Cloud offers peer-to-peer GPU computing. On-Demand Pods run continuously, while Spot Pods use spare compute capacity.\"\n---","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":1,"to":4}}}}],["186",{"pageContent":"Secure Cloud vs Community Cloud\n\nRunPod provides two cloud computing services: [Secure Cloud](https://www.runpod.io/console/gpu-secure-cloud) and [Community Cloud.](https://www.runpod.io/console/gpu-cloud)\n\n**Secure Cloud** runs in T3/T4 data centers by our trusted partners. Our close partnership comes with high-reliability with redundancy, security, and fast response times to mitigate any downtimes. For any sensitive and enterprise workloads, we highly recommend Secure Cloud.\n\n**Community Cloud** brings power in numbers and diversity spanning the whole world. Through our decentralized platform, we can offer peer-to-peer GPU computing that connects individual compute providers to compute consumers. Our Community Cloud hosts are invite-only and vetted by us, and still have to abide by our standards. Even though their associated infrastructure might not offer as much redundancy for power and networking, they still offer good servers that combine quality and affordability.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":6,"to":12}}}}],["187",{"pageContent":"Both solutions offer far more competitive prices than large cloud providers such as AWS or GCP.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":14,"to":14}}}}],["188",{"pageContent":"On-Demand vs. Spot Pod\n\n**On-Demand Pods** can run forever without interruptions with resources dedicated to your Pod. They do incur higher costs than Spot Pods.\n\n**Spot Pods** use spare compute capacity, allowing you to bid for those compute resources. Resources are dedicated to your Pod, but someone else can bid higher or start an On-Demand Pod that will stop your Pod. When this happens, your Pod is given a signal to stop 5 seconds prior with SIGTERM, and eventually, the kill signal SIGKILL after 5 seconds. You can use volumes to save any data to the disk in that 5s period or push data to the cloud periodically.\n\n### How does RunPod work?\n\nRunPod leverages technologies like [Docker](https://www.docker.com/) to containerize and isolate guest workloads on a host machine. We have built a decentralized platform where thousands of servers can be connected to offer a seamless experience for all users.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":16,"to":24}}}}],["189",{"pageContent":"Where can I go for help?\n\nWe'd be happy to help! Join our community on [Discord](https://discord.gg/pJ3P2DbUUq), message us in our support chat, or email us at [help@runpod.io](mailto:help@runpod.io).\n\n### What is RunPod's policy on refunds and credits?\n\nIf you aren't sure if RunPod is for you, feel free to hang out in our [Discord](https://discord.gg/cUpRmau42V) to ask questions or email [help@runpod.io](mailto:help@runpod.io) You can load as little as $10 into your account to try things out. We don't currently offer refunds or trial credits due to the overhead of processing these requests. Please plan accordingly!","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":26,"to":32}}}}],["190",{"pageContent":"What are Pods?\n\n---\n\n### What is an On-Demand instance?\n\n**On-Demand instances** are for non-interruptible workloads.\nYou pay the On-Demand price and cannot be displaced by other users if you have funds to keep your Pod running.\n\n### What is a Spot instance?\n\nA **Spot instance** is an interruptible instance that can generally be rented much cheaper than an On-Demand one.\nSpot instances are great for stateless workloads like an API or for workloads you can periodically save to a volume disk.\nYour volume disk is retained even if your Spot instance is interrupted.\n\n### What is a Savings Plan?\n\nSavings Plans are a way for you to pay up-front and get a discount for it.\nThis is great for when you know you will need prolonged access to compute.\nYou can learn more on the about [Savings Plans here](/pods/savings-plans).","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":34,"to":53}}}}],["191",{"pageContent":"Billing\n\nAll billing, including per-hour compute and storage billing, is charged per minute.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":55,"to":57}}}}],["192",{"pageContent":"How does Pod billing work?\n\nEvery Pod has an hourly cost based on GPU type. Your RunPod credits are charged for the Pod every minute as long as the Pod is running. If you ever run out of credits, your Pods will be automatically stopped, and you will get an email notification. Eventually, Pods will be terminated if you don't refill your credit. **We pre-emptively stop all of your Pods if you get down to 10 minutes of remaining run time. This gives your account enough balance to keep your data volumes around in the case you need access to your data. Please plan accordingly.**\n\nOnce a balance has been completely drained, all pods are subject to deletion at the discretion of the service.\nAn attempt will be made to hold the pods for as long as possible, but this should not be relied upon!\nWe highly recommend setting up [automatic payments](https://www.runpod.io/console/user/billing) to ensure balances are automatically topped up as needed.\n\n:::note","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":59,"to":67}}}}],["193",{"pageContent":"You must have at least one hour's worth of time in your balance to rent a Pod at your given spec.\nIf your balance is insufficient to rent a Pod, then consider renting the Pod on Spot, depositing additional funds, or lowering your GPU spec requirements.\n\n:::","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":69,"to":72}}}}],["194",{"pageContent":"How does storage billing work?\n\nWe currently charge $0.10 GB per month for all storage on running Pods and $0.20 GB per month for volume storage on stopped Pods.\nStorage is tied to compute servers, and we want to ensure active users have enough space to run their workloads.\nStorage is charged per minute, and we never charge users if the host machine is down or unavailable from the public internet.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":74,"to":78}}}}],["195",{"pageContent":"How does Network Volume billing work?\n\nFor storage requirements below 1TB, we charge a competitive rate of $0.07/GB/Month. If your storage requirements exceed 1TB, we provide a cost-effective pricing of $0.05/GB/Month. This ensures that you receive significant savings as your data storage scales.\n\nWhen you choose to create a Network Volume, you gain access to our robust infrastructure, which includes state-of-the-art storage servers located in the same datacenters where you rent GPU servers from us. These servers are connected via a high-speed 25Gbps local network, up to 200 Gbps in some locations, guaranteeing efficient data transfer and minimal latency. Everything is stored on high-speed NVME SSDs to ensure best performance.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":80,"to":84}}}}],["196",{"pageContent":"Network volumes are billed on a per-hour basis. Please note that if your machine-based storage or network volume is terminated due to lack of funds, that disk space is immediately freed up for use by other clients, and RunPod is unable to assist in recovering lost storage.\nRunPod is also not designed to be a cloud storage system; storage is provided in the pursuit of running tasks using its GPUs, and not meant to be a long-term backup solution.\nIt is highly advisable to continually back up anything you want to save offsite locally or to a cloud provider.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":86,"to":88}}}}],["197",{"pageContent":"Security\n\n---\n\n### Is my data protected from other clients?\n\nYes. Your data is run in a multi-tenant environment where other clients can't access your pod. For sensitive workloads requiring the best security, please use Secure Cloud.\n\n### Is my data protected from the host of the machine my Pod is running on?\n\nData privacy is important to us at RunPod.\nOur Terms of Service prohibit hosts from trying to inspect your Pod data or usage patterns in any way.\nIf you want the highest level of security, use Secure Cloud.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":90,"to":102}}}}],["198",{"pageContent":"Usability\n\n---\n\n### What can I do in a RunPod Pod?\n\nYou can run any Docker container available on any publicly reachable container registry. If you are not well versed in containers, we recommend sticking with the default run templates like our RunPod PyTorch template. However, if you know what you are doing, you can do a lot more!\n\n### Can I run my own Docker daemon on RunPod?\n\nYou can't currently spin up your own instance of Docker, as we run Docker for you! Unfortunately, this means that you cannot currently build Docker containers on RunPod or use things like Docker Compose. Many use cases can be solved by creating a custom template with the Docker image that you want to run.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":104,"to":114}}}}],["199",{"pageContent":"My Pod is stuck on initializing. What gives?\n\nUsually, this happens for one of several reasons. If you can't figure it out, [contact us](https://www.runpod.io/contact), and we'll gladly help you.\n\n1. You are trying to run a Pod to SSH into, but you did not give the Pod an idle job to run like \"sleep infinity.\"\n2. You have given your Pod a command that it doesn't know how to run. Check the logs to make sure that you don't have any syntax errors, etc.\n\n### Can I run Windows?\n\nWe don't currently support Windows.\nWe want to do this in the future, but we do not have a solid timeframe for Windows support.\n\n### How do I find a reliable server in Community Cloud?\n\nRunPod needs to provide you with reliable servers. All of our listed servers must meet minimum reliability, and most are running in a data center! However, if you want the highest level of reliability and security, use Secure Cloud. RunPod calculates server reliability by maintaining a heartbeat with each server in real-time.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":116,"to":130}}}}],["200",{"pageContent":"Why do I have zero GPUs assigned to my Pod?\n\nIf you want to avoid this, using network volumes is the best choice. [Read about it here.](https://blog.runpod.io/four-reasons-to-set-up-a/)\n\n[Learn how to use them here](https://docs.runpod.io/docs/create-a-network-volume).\n\nMost of our machines have between 4 and 8 GPUs per physical machine. When you start a Pod, it is locked to a specific physical machine. If you keep it running (On-Demand), then that GPU cannot be taken from you. However, if you stop your Pod, it becomes available for a different user to rent. When you want to start your Pod again, your specific machine may be wholly occupied! In this case, we give you the option to spin up your Pod with zero GPUs so you can retain access to your data.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":132,"to":138}}}}],["201",{"pageContent":"Remember that this does not mean there are no more GPUs of that type available, just none on the physical machine that specific Pod is locked to. Note that transfer Pods have limited computing capabilities, so transferring files using a UI may be difficult, and you may need to resort to terminal access or cloud sync options.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":140,"to":140}}}}],["202",{"pageContent":"What are Network Volumes?\n\nNetwork volumes allow you to share data between Pods and generally be more mobile with your important data. This feature is only available in specific secure cloud data centers, but we are actively rolling it out to more and more of our secure cloud footprint. If you use network volumes, you should rarely run into situations where you cannot use your data with a GPU without a file transfer!\n\n[Read about it here](https://blog.runpod.io/four-reasons-to-set-up-a/).","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":142,"to":146}}}}],["203",{"pageContent":"What if?\n\n---\n\n### What if I run out of funds?\n\nAll your Pods are stopped automatically when you don't have enough funds to keep your Pods running for at least ten more minutes. When your Pods are stopped, your container disk data will be lost, but your volume data will be preserved. Pods are scheduled for removal if adequate credit balance is not maintained. If you fail to do so, your Pods will be terminated, and Pod volumes will be removed.\n\nAfter you add more funds to your account, you can start your Pod if you wish (assuming enough GPUs are available on the host machine).\n\n### What if the machine that my Pod is running loses power?\n\nIf the host machine loses power, it will attempt to start your Pod again when it returns online. Your volume data will be preserved, and your container will run the same command as it ran the first time you started renting it. Your container disk and anything in memory will be lost!","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":148,"to":160}}}}],["204",{"pageContent":"What if my Pod loses internet connectivity?\n\nThe host machine continues to run your Pod to the best of its ability, even if it is not connected to the internet. If your job requires internet connectivity, then it will not function. You will not be charged if the host loses internet connectivity, even if it continues to run your job. You may, of course, request that your Pod exit while the host is offline, and it will exit your Pod when it regains network connectivity.\n\n### What if it says that my spending limit has been exceeded?\n\nWe implement a spending limit for newer accounts that will grow over time. This is because we have found that sometimes scammers try to interfere with the natural workings of the platform. We believe that this limit should not impact normal usage. We would be delighted to up your spending limit if you [contact us](https://www.runpod.io/contact) and share your use case.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":162,"to":168}}}}],["205",{"pageContent":"Legal\n\n---\n\n### Do you have some legal stuff I can look at?\n\nSure, do! Take a look at our [legal page](https://www.runpod.io/legal).","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":170,"to":176}}}}],["206",{"pageContent":"GDPR Compliance\n\nAt Runpod, we take data protection and privacy seriously.\nWe have implemented robust policies, procedures, and technical measures to ensure compliance with the GDPR requirements.\n\n### Is RunPod compliant with GDPR for data processed in Europe?\n\nYes, RunPod is fully compliant with the General Data Protection Regulation (GDPR) requirements for any data processed within our European data center regions.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":178,"to":185}}}}],["207",{"pageContent":"What measures does RunPod take to ensure GDPR compliance?\n\nFor servers hosted in GDPR-compliant regions like the European Union, we ensure:","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":187,"to":189}}}}],["208",{"pageContent":"- **Data processing procedures**: We have established clear procedures for the collection,\n  storage, processing, and deletion of personal data, ensuring transparency and accountability in\n  our data processing activities.\n- **Data protection measures**: We have implemented appropriate technical and organizational\n  measures to safeguard personal data against unauthorized access, disclosure, alteration, and\n  destruction.\n- **Consent mechanisms**: We obtain and record consent from individuals for the processing of\n  their personal data in accordance with GDPR requirements, and we provide mechanisms for\n  individuals to withdraw consent if desired.\n- **Rights of data subjects**: We facilitate the rights of data subjects under the GDPR, including\n  the right to access, rectify, erase, or restrict the processing of their personal data, and we handle\n  data subject requests promptly and efficiently.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":191,"to":202}}}}],["209",{"pageContent":"- **Data transfer mechanisms**: We ensure lawful and secure transfer of personal data outside the\n  EU, where applicable, in compliance with GDPR requirements, utilizing appropriate mechanisms\n  such as adequacy decisions, standard contractual clauses, or binding corporate rules.\n- **Compliance monitoring**: We regularly monitor and review our GDPR compliance to ensure\n  ongoing effectiveness and adherence to regulatory requirements, conducting data protection\n  impact assessments and internal audits as needed.","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":203,"to":208}}}}],["210",{"pageContent":"For any inquiries or concerns regarding our GDPR compliance or our data protection practices, reach out to our team through email at [support@runpod.io](mailto:support@runpod.io).","metadata":{"source":"/runpod-docs/docs/references/faq/faq.md","loc":{"lines":{"from":210,"to":210}}}}],["211",{"pageContent":"---\ntitle: Manage Payment Card Declines\ndescription: \"RunPod helps global clients with card declines due to international transactions being flagged as potential fraud. Follow steps to minimize interruptions: keep a balanced account, contact the issuing bank, and consider account risk profiles.\"\n---\n\nRunPod is a US-based organization that serves clients all across the world. However, credit card processors have in general keyed into international transactions as a potential vector for fraud and tend to apply more stringent standards for blocking transactions. If your card is declined, don't panic! To minimize potential interruptions to your service, you'll want to follow these steps.\n\n**Keep your balance topped up**","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":1,"to":8}}}}],["212",{"pageContent":"To avoid any potential issues with your balance being overrun, it's best to refresh your balance at least a few days before you're due to run out so you have a chance to address any last minute delays. Also be aware that there is an option to automatically refresh your balance when you run low under the Billing [page](https://www.runpod.io/console/user/billing):\n\n![](/img/docs/739337f-image.png)\n\n**Call the bank that issued your card**","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":10,"to":14}}}}],["213",{"pageContent":"Once you do experience a card decline, the first step you'll want to do is to contact your issuing bank to see why a card is declined. Due to consumer/merchant privacy standards in the US, we are not provided with a reason that the card is declined, only that the transaction was not processed. Only your issuing bank can specifically tell you why a payment was declined. Many times, declines are for completely innocent reasons, such as your bank's anti-fraud protection tripping; just the same, RunPod is unable to assist with blocks put in place by your bank.\n\nIt's important that you call your bank for the initial decline before trying a different card, because the processor may block _all_ funding attempts from an account if it seems declines from multiple cards for the same account, even if these attempts would have otherwise not had any problems. These account blocks generally clear after 24 hours, but it may be difficult to load the account until then.","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":16,"to":18}}}}],["214",{"pageContent":"**Other potential reasons for card blocks**\n\nOur payment processor may block cards for specific users based on their risk profile, so certain use patterns may trigger a block. If you use several different cards within a short period time, or have had disputed transactions in the past, this may also cause cards to decline.\n\nTo see a list of supported cards on Stripe, [click here](https://stripe.com/docs/payments/cards/supported-card-brands>).\n\n**Contact us for support**\n\nIf all else fails, then feel free to contact [RunPod support](https://www.runpod.io/contact) if you are still having trouble loading your account. We ask that you check with your bank first, but if everything checks out on your end, we will be glad to help!","metadata":{"source":"/runpod-docs/docs/references/faq/manage-cards.md","loc":{"lines":{"from":20,"to":28}}}}],["215",{"pageContent":"---\ntitle: GPU types\n---\n\nThe following list contains all GPU types available on RunPod.\n\nFor more information, see [GPU pricing](https://www.runpod.io/gpu-instance/pricing).\n\n<!--\nTable last generated: 2024-06-04\n-->","metadata":{"source":"/runpod-docs/docs/references/gpu-types.md","loc":{"lines":{"from":1,"to":11}}}}],["216",{"pageContent":"| GPU ID                             | Display Name     | Memory (GB) |\n| ---------------------------------- | ---------------- | ----------- |\n| NVIDIA A100 80GB PCIe              | A100 PCIe        | 80          |\n| NVIDIA A100-SXM4-80GB              | A100 SXM         | 80          |\n| NVIDIA A30                         | A30              | 24          |\n| NVIDIA A40                         | A40              | 48          |\n| NVIDIA H100 NVL                    | H100 NVL         | 94          |\n| NVIDIA H100 PCIe                   | H100 PCIe        | 80          |\n| NVIDIA H100 80GB HBM3              | H100 SXM         | 80          |\n| NVIDIA L4                          | L4               | 24          |\n| NVIDIA L40                         | L40              | 48          |\n| NVIDIA L40S                        | L40S             | 48          |\n| AMD Instinct MI300X OAM            | MI300X           | 192         |","metadata":{"source":"/runpod-docs/docs/references/gpu-types.md","loc":{"lines":{"from":13,"to":25}}}}],["217",{"pageContent":"| NVIDIA GeForce RTX 3070            | RTX 3070         | 8           |\n| NVIDIA GeForce RTX 3080            | RTX 3080         | 10          |\n| NVIDIA GeForce RTX 3080 Ti         | RTX 3080 Ti      | 12          |\n| NVIDIA GeForce RTX 3090            | RTX 3090         | 24          |\n| NVIDIA GeForce RTX 3090 Ti         | RTX 3090 Ti      | 24          |\n| NVIDIA RTX 4000 Ada Generation     | RTX 4000 Ada     | 20          |\n| NVIDIA RTX 4000 SFF Ada Generation | RTX 4000 Ada SFF | 20          |\n| NVIDIA GeForce RTX 4070 Ti         | RTX 4070 Ti      | 12          |\n| NVIDIA GeForce RTX 4080            | RTX 4080         | 16          |\n| NVIDIA GeForce RTX 4090            | RTX 4090         | 24          |\n| NVIDIA RTX 5000 Ada Generation     | RTX 5000 Ada     | 32          |\n| NVIDIA RTX 6000 Ada Generation     | RTX 6000 Ada     | 48          |\n| NVIDIA RTX A2000                   | RTX A2000        | 6           |","metadata":{"source":"/runpod-docs/docs/references/gpu-types.md","loc":{"lines":{"from":26,"to":38}}}}],["218",{"pageContent":"| NVIDIA RTX A4000                   | RTX A4000        | 16          |\n| NVIDIA RTX A4500                   | RTX A4500        | 20          |\n| NVIDIA RTX A5000                   | RTX A5000        | 24          |\n| NVIDIA RTX A6000                   | RTX A6000        | 48          |\n| Tesla V100-PCIE-16GB               | Tesla V100       | 16          |\n| Tesla V100-FHHL-16GB               | V100 FHHL        | 16          |\n| Tesla V100-SXM2-16GB               | V100 SXM2        | 16          |\n| Tesla V100-SXM2-32GB               | V100 SXM2 32GB   | 32          |","metadata":{"source":"/runpod-docs/docs/references/gpu-types.md","loc":{"lines":{"from":39,"to":46}}}}],["219",{"pageContent":"---\ntitle: \"runpodctl\"\n---\n\nYou can use RunPod's CLI [runpodctl](https://github.com/runpod/runpodctl) to manage Pods.\n\nThe runpodctl is a tool for managing your Pods on RunPod.\nAll Pods come with `runpodctl` installed with a Pod-scoped API key, which makes managing your Pods easier through the command line.\n\nChoose one of the following methods to install the RunPod CLI.\n\n### MacOs\n\n**ARM**\n\n```bash\nwget --quiet --show-progress https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-darwin-arm64 -O runpodctl && chmod +x runpodctl && sudo mv runpodctl /usr/local/bin/runpodctl\n```\n\n**AMD**\n\n```bash\nwget --quiet --show-progress https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-darwin-amd64 -O runpodctl && chmod +x runpodctl && sudo mv runpodctl /usr/local/bin/runpodctl\n```","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl.md","loc":{"lines":{"from":1,"to":24}}}}],["220",{"pageContent":"Linux\n\n```bash\nwget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd -O runpodctl && chmod +x runpodctl && sudo cp runpodctl /usr/bin/runpodctl\n```\n\n### Windows (powershell)\n\n```bash\nwget https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-win-amd -O runpodctl.exe\n```\n\n### Google Collab\n\n```bash\n!wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd -O runpodctl\n!chmod +x runpodctl\n!cp runpodctl /usr/bin/runpodctl\n```\n\n### Jupyter notebook\n\n```bash\n!wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd -O runpodctl\n!chmod +x runpodctl\n!cp runpodctl /usr/bin/runpodctl\n```","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl.md","loc":{"lines":{"from":26,"to":52}}}}],["221",{"pageContent":"runpodctl\n\nCLI for runpod.io\n\n### Synopsis\n\nCLI tool to manage your pods for runpod.io\n\n### Options\n\n```\n-h, --help   help for runpodctl\n```\n\n### SEE ALSO\n\n- [runpodctl config](runpodctl_config.md) - CLI Config\n- [runpodctl create](runpodctl_create.md) - create a resource\n- [runpodctl get](runpodctl_get.md) - get resource\n- [runpodctl project](runpodctl_project.md) - Manage RunPod projects\n- [runpodctl receive](runpodctl_receive.md) - receive file(s), or folder\n- [runpodctl remove](runpodctl_remove.md) - remove a resource\n- [runpodctl send](runpodctl_send.md) - send file(s), or folder\n- [runpodctl ssh](runpodctl_ssh.md) - SSH keys and commands\n- [runpodctl start](runpodctl_start.md) - start a resource\n- [runpodctl stop](runpodctl_stop.md) - stop a resource\n- [runpodctl update](runpodctl_update.md) - update runpodctl\n- [runpodctl version](runpodctl_version.md) - runpodctl version","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl.md","loc":{"lines":{"from":54,"to":81}}}}],["222",{"pageContent":"---\ntitle: \"Config\"\n---\n\n## runpodctl config\n\nCLI Config\n\n### Synopsis\n\nRunPod CLI Config Settings\n\n```\nrunpodctl config [flags]\n```\n\n### Options\n\n```\n    --apiKey string   RunPod API key\n    --apiUrl string   RunPod API URL (default \"https://api.runpod.io/graphql\")\n-h, --help            help for config\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_config.md","loc":{"lines":{"from":1,"to":27}}}}],["223",{"pageContent":"---\ntitle: \"Create\"\n---\n\n## runpodctl create\n\ncreate a resource\n\n### Synopsis\n\ncreate a resource in runpod.io\n\n### Options\n\n```\n-h, --help   help for create\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io\n- [runpodctl create pod](runpodctl_create_pod.md) - start a pod\n- [runpodctl create pods](runpodctl_create_pods.md) - create a group of pods","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create.md","loc":{"lines":{"from":1,"to":23}}}}],["224",{"pageContent":"---\ntitle: \"Create Pod\"\n---","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pod.md","loc":{"lines":{"from":1,"to":3}}}}],["225",{"pageContent":"runpodctl create pod\n\nstart a pod\n\n### Synopsis\n\nstart a pod from runpod.io\n\n```\nrunpodctl create pod [flags]\n```","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pod.md","loc":{"lines":{"from":5,"to":15}}}}],["226",{"pageContent":"Options","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pod.md","loc":{"lines":{"from":17,"to":17}}}}],["227",{"pageContent":"```\n    --args string             container arguments\n    --communityCloud          create in community cloud\n    --containerDiskSize int   container disk size in GB (default 20)\n    --cost float32            $/hr price ceiling, if not defined, pod will be created with lowest price available\n    --env strings             container arguments\n    --gpuCount int            number of GPUs for the pod (default 1)\n    --gpuType string          gpu type id, e.g. 'NVIDIA GeForce RTX 3090'\n-h, --help                    help for pod\n    --imageName string        container image name\n    --mem int                 minimum system memory needed (default 20)\n    --name string             any pod name for easy reference\n    --ports strings           ports to expose; max only 1 http and 1 tcp allowed; e.g. '8888/http'\n    --secureCloud             create in secure cloud\n    --templateId string       templateId to use with the pod\n    --vcpu int                minimum vCPUs needed (default 1)","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pod.md","loc":{"lines":{"from":19,"to":34}}}}],["228",{"pageContent":"--volumePath string       container volume path (default \"/runpod\")\n    --volumeSize int          persistent volume disk size in GB (default 1)\n    --networkVolumeId string  network volume id\n```","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pod.md","loc":{"lines":{"from":35,"to":38}}}}],["229",{"pageContent":"SEE ALSO\n\n- [runpodctl create](runpodctl_create.md) - create a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pod.md","loc":{"lines":{"from":40,"to":42}}}}],["230",{"pageContent":"---\ntitle: \"Create Pods\"\n---","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pods.md","loc":{"lines":{"from":1,"to":3}}}}],["231",{"pageContent":"runpodctl create pods\n\ncreate a group of pods\n\n### Synopsis\n\ncreate a group of pods on runpod.io\n\n```\nrunpodctl create pods [flags]\n```","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pods.md","loc":{"lines":{"from":5,"to":15}}}}],["232",{"pageContent":"Options","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pods.md","loc":{"lines":{"from":17,"to":17}}}}],["233",{"pageContent":"```\n    --args string             container arguments\n    --communityCloud          create in community cloud\n    --containerDiskSize int   container disk size in GB (default 20)\n    --cost float32            $/hr price ceiling, if not defined, pod will be created with lowest price available\n    --env strings             container arguments\n    --gpuCount int            number of GPUs for the pod (default 1)\n    --gpuType string          gpu type id, e.g. 'NVIDIA GeForce RTX 3090'\n-h, --help                    help for pods\n    --imageName string        container image name\n    --mem int                 minimum system memory needed (default 20)\n    --name string             any pod name for easy reference\n    --podCount int            number of pods to create with the same name (default 1)\n    --ports strings           ports to expose; max only 1 http and 1 tcp allowed; e.g. '8888/http'\n    --secureCloud             create in secure cloud","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pods.md","loc":{"lines":{"from":19,"to":33}}}}],["234",{"pageContent":"--vcpu int                minimum vCPUs needed (default 1)\n    --volumePath string       container volume path (default \"/runpod\")\n    --volumeSize int          persistent volume disk size in GB (default 1)\n```","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pods.md","loc":{"lines":{"from":34,"to":37}}}}],["235",{"pageContent":"SEE ALSO\n\n- [runpodctl create](runpodctl_create.md) - create a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_create_pods.md","loc":{"lines":{"from":39,"to":41}}}}],["236",{"pageContent":"---\ntitle: \"Get\"\n---\n\n## runpodctl get\n\nget resource\n\n### Synopsis\n\nget resources for pods\n\n### Options\n\n```\n-h, --help   help for get\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io\n- [runpodctl get cloud](runpodctl_get_cloud.md) - get all cloud gpus\n- [runpodctl get pod](runpodctl_get_pod.md) - get all pods","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_get.md","loc":{"lines":{"from":1,"to":23}}}}],["237",{"pageContent":"---\ntitle: \"Get Cloud\"\n---\n\n## runpodctl get cloud\n\nget all cloud gpus\n\n### Synopsis\n\nget all cloud gpus available on runpod.io\n\n```\nrunpodctl get cloud [gpuCount] [flags]\n```\n\n### Options\n\n```\n-c, --community   show listings from community cloud only\n    --disk int    minimum disk size in GB you need\n-h, --help        help for cloud\n    --mem int     minimum sys memory size in GB you need\n-s, --secure      show listings from secure cloud only\n    --vcpu int    minimum vCPUs you need\n```\n\n### SEE ALSO\n\n- [runpodctl get](runpodctl_get.md) - get resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_get_cloud.md","loc":{"lines":{"from":1,"to":30}}}}],["238",{"pageContent":"---\ntitle: \"Get Pod\"\n---\n\n## runpodctl get pod\n\nget all pods\n\n### Synopsis\n\nget all pods or specify pod id\n\n```\nrunpodctl get pod [podId] [flags]\n```\n\n### Options\n\n```\n-a, --allfields   include all fields in output\n-h, --help        help for pod\n```\n\n### SEE ALSO\n\n- [runpodctl get](runpodctl_get.md) - get resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_get_pod.md","loc":{"lines":{"from":1,"to":26}}}}],["239",{"pageContent":"---\ntitle: \"Project\"\n---\n\n## runpodctl project\n\nManage RunPod projects\n\n### Synopsis\n\nDevelop and deploy projects entirely on RunPod's infrastructure\n\n### Options\n\n```\n-h, --help   help for project\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io\n- [runpodctl project build](runpodctl_project_build.md) - builds Dockerfile for current project\n- [runpodctl project create](runpodctl_project_create.md) - creates a new project\n- [runpodctl project deploy](runpodctl_project_deploy.md) - deploys your project as an endpoint\n- [runpodctl project dev](runpodctl_project_dev.md) - starts a development session for the current project","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project.md","loc":{"lines":{"from":1,"to":25}}}}],["240",{"pageContent":"---\ntitle: \"Project Build\"\n---\n\n## runpodctl project build\n\nbuilds Dockerfile for current project\n\n### Synopsis\n\nbuilds a local Dockerfile for the project in the current folder. You can use this Dockerfile to build an image and deploy it to any API server.\n\n```\nrunpodctl project build [flags]\n```\n\n### Options\n\n```\n-h, --help          help for build\n    --include-env   include environment variables from runpod.toml in generated Dockerfile\n```\n\n### SEE ALSO\n\n- [runpodctl project](runpodctl_project.md) - Manage RunPod projects","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_build.md","loc":{"lines":{"from":1,"to":26}}}}],["241",{"pageContent":"---\ntitle: \"Project Create\"\n---\n\n## runpodctl project create\n\ncreates a new project\n\n### Synopsis\n\ncreates a new RunPod project folder on your local machine\n\n```\nrunpodctl project create [flags]\n```\n\n### Options\n\n```\n-h, --help          help for create\n-i, --init          use the current directory as the project directory\n-n, --name string   project name\n```\n\n### SEE ALSO\n\n- [runpodctl project](runpodctl_project.md) - Manage RunPod projects","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_create.md","loc":{"lines":{"from":1,"to":27}}}}],["242",{"pageContent":"---\ntitle: \"Project Deploy\"\n---\n\n## runpodctl project deploy\n\ndeploys your project as an endpoint\n\n### Synopsis\n\ndeploys a serverless endpoint for the RunPod project in the current folder\n\n```\nrunpodctl project deploy [flags]\n```\n\n### Options\n\n```\n-h, --help   help for deploy\n```\n\n### SEE ALSO\n\n- [runpodctl project](runpodctl_project.md) - Manage RunPod projects","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_deploy.md","loc":{"lines":{"from":1,"to":25}}}}],["243",{"pageContent":"---\ntitle: \"Project Dev\"\n---\n\n## runpodctl project dev\n\nstarts a development session for the current project\n\n### Synopsis\n\nconnects your local environment and the project environment on your Pod. Changes propagate to the project environment in real time.\n\n```\nrunpodctl project dev [flags]\n```\n\n### Options\n\n```\n-h, --help              help for dev\n    --prefix-pod-logs   prefix logs from project Pod with Pod ID (default true)\n    --select-volume     select a new default network volume for current project\n```\n\n### SEE ALSO\n\n- [runpodctl project](runpodctl_project.md) - Manage RunPod projects","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_project_dev.md","loc":{"lines":{"from":1,"to":27}}}}],["244",{"pageContent":"---\ntitle: \"Receive\"\n---\n\n## runpodctl receive\n\nreceive file(s), or folder\n\n### Synopsis\n\nreceive file(s), or folder from pod or any computer\n\n```\nrunpodctl receive [code] [flags]\n```\n\n### Options\n\n```\n-h, --help   help for receive\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_receive.md","loc":{"lines":{"from":1,"to":25}}}}],["245",{"pageContent":"---\ntitle: \"Remove\"\n---\n\n## runpodctl remove\n\nremove a resource\n\n### Synopsis\n\nremove a resource in runpod.io\n\n### Options\n\n```\n-h, --help   help for remove\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io\n- [runpodctl remove pod](runpodctl_remove_pod.md) - remove a pod\n- [runpodctl remove pods](runpodctl_remove_pods.md) - remove all pods using name","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_remove.md","loc":{"lines":{"from":1,"to":23}}}}],["246",{"pageContent":"---\ntitle: \"Remove Pod\"\n---\n\n## runpodctl remove pod\n\nremove a pod\n\n### Synopsis\n\nremove a pod from runpod.io\n\n```\nrunpodctl remove pod [podId] [flags]\n```\n\n### Options\n\n```\n-h, --help   help for pod\n```\n\n### SEE ALSO\n\n- [runpodctl remove](runpodctl_remove.md) - remove a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_remove_pod.md","loc":{"lines":{"from":1,"to":25}}}}],["247",{"pageContent":"---\ntitle: \"Remove Pods\"\n---\n\n## runpodctl remove pods\n\nremove all pods using name\n\n### Synopsis\n\nremove all pods using name from runpod.io\n\n```\nrunpodctl remove pods [name] [flags]\n```\n\n### Options\n\n```\n-h, --help           help for pods\n    --podCount int   number of pods to remove with the same name (default 1)\n```\n\n### SEE ALSO\n\n- [runpodctl remove](runpodctl_remove.md) - remove a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_remove_pods.md","loc":{"lines":{"from":1,"to":26}}}}],["248",{"pageContent":"---\ntitle: \"Send\"\n---\n\n## runpodctl send\n\nsend file(s), or folder\n\n### Synopsis\n\nsend file(s), or folder to pod or any computer\n\n```\nrunpodctl send [filename(s) or folder] [flags]\n```\n\n### Options\n\n```\n    --code string   codephrase used to connect\n-h, --help          help for send\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_send.md","loc":{"lines":{"from":1,"to":26}}}}],["249",{"pageContent":"---\ntitle: \"Ssh\"\n---\n\n## runpodctl ssh\n\nSSH keys and commands\n\n### Synopsis\n\nSSH key management and connection to pods\n\n### Options\n\n```\n-h, --help   help for ssh\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io\n- [runpodctl ssh add-key](runpodctl_ssh_add-key.md) - Adds an SSH key to the current user account\n- [runpodctl ssh list-keys](runpodctl_ssh_list-keys.md) - List all SSH keys","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_ssh.md","loc":{"lines":{"from":1,"to":23}}}}],["250",{"pageContent":"---\ntitle: \"Ssh Add Key\"\n---\n\n## runpodctl ssh add-key\n\nAdds an SSH key to the current user account\n\n### Synopsis\n\nAdds an SSH key to the current user account. If no key is provided, one will be generated.\n\n```\nrunpodctl ssh add-key [flags]\n```\n\n### Options\n\n```\n-h, --help              help for add-key\n    --key string        The public key to add.\n    --key-file string   The file containing the public key to add.\n```\n\n### SEE ALSO\n\n- [runpodctl ssh](runpodctl_ssh.md) - SSH keys and commands","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_ssh_add-key.md","loc":{"lines":{"from":1,"to":27}}}}],["251",{"pageContent":"---\ntitle: \"Ssh List Keys\"\n---\n\n## runpodctl ssh list-keys\n\nList all SSH keys\n\n### Synopsis\n\nList all the SSH keys associated with the current user's account.\n\n```\nrunpodctl ssh list-keys [flags]\n```\n\n### Options\n\n```\n-h, --help   help for list-keys\n```\n\n### SEE ALSO\n\n- [runpodctl ssh](runpodctl_ssh.md) - SSH keys and commands","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_ssh_list-keys.md","loc":{"lines":{"from":1,"to":25}}}}],["252",{"pageContent":"---\ntitle: \"Start\"\n---\n\n## runpodctl start\n\nstart a resource\n\n### Synopsis\n\nstart a resource in runpod.io\n\n### Options\n\n```\n-h, --help   help for start\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io\n- [runpodctl start pod](runpodctl_start_pod.md) - start a pod","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_start.md","loc":{"lines":{"from":1,"to":22}}}}],["253",{"pageContent":"---\ntitle: \"Start Pod\"\n---\n\n## runpodctl start pod\n\nstart a pod\n\n### Synopsis\n\nstart a pod from runpod.io\n\n```\nrunpodctl start pod [podId] [flags]\n```\n\n### Options\n\n```\n    --bid float32   bid per gpu for spot price\n-h, --help          help for pod\n```\n\n### SEE ALSO\n\n- [runpodctl start](runpodctl_start.md) - start a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_start_pod.md","loc":{"lines":{"from":1,"to":26}}}}],["254",{"pageContent":"---\ntitle: \"Stop\"\n---\n\n## runpodctl stop\n\nstop a resource\n\n### Synopsis\n\nstop a resource in runpod.io\n\n### Options\n\n```\n-h, --help   help for stop\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io\n- [runpodctl stop pod](runpodctl_stop_pod.md) - stop a pod","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_stop.md","loc":{"lines":{"from":1,"to":22}}}}],["255",{"pageContent":"---\ntitle: \"Stop Pod\"\n---\n\n## runpodctl stop pod\n\nstop a pod\n\n### Synopsis\n\nstop a pod from runpod.io\n\n```\nrunpodctl stop pod [podId] [flags]\n```\n\n### Options\n\n```\n-h, --help   help for pod\n```\n\n### SEE ALSO\n\n- [runpodctl stop](runpodctl_stop.md) - stop a resource","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_stop_pod.md","loc":{"lines":{"from":1,"to":25}}}}],["256",{"pageContent":"---\ntitle: \"Update\"\n---\n\n## runpodctl update\n\nupdate runpodctl\n\n### Synopsis\n\nupdate runpodctl to the latest version\n\n```\nrunpodctl update [flags]\n```\n\n### Options\n\n```\n-h, --help   help for update\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_update.md","loc":{"lines":{"from":1,"to":25}}}}],["257",{"pageContent":"---\ntitle: \"Version\"\n---\n\n## runpodctl version\n\nrunpodctl version\n\n### Synopsis\n\nrunpodctl version\n\n```\nrunpodctl version [flags]\n```\n\n### Options\n\n```\n-h, --help   help for version\n```\n\n### SEE ALSO\n\n- [runpodctl](runpodctl.md) - CLI for runpod.io","metadata":{"source":"/runpod-docs/docs/references/runpodctl/runpodctl_version.md","loc":{"lines":{"from":1,"to":25}}}}],["258",{"pageContent":"---\ntitle: \"502 Errors\"\nid: \"troubleshooting-502-errors\"\ndescription: \"Troubleshoot 502 errors in your deployed pod by checking GPU attachment, pod logs, and official template instructions to resolve issues and enable seamless access.\"\n---\n\n502 errors can occur when users attempt to access a program running on a specific port of a deployed pod and the program isn't running or has encountered an error. This document provides guidance to help you troubleshoot this error.","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":1,"to":7}}}}],["259",{"pageContent":"Check Your Pod's GPU\n\nThe first step to troubleshooting a 502 error is to check whether your pod has a GPU attached.\n\n1. **Access your pod's settings**: Click on your pod's settings in the user interface to access detailed information about your pod.\n\n2. **Verify GPU attachment**: Here, you should be able to see if your pod has a GPU attached. If it does not, you will need to attach a GPU.\n\nIf a GPU is attached, you will see it under the Pods screen (e.g. 1 x A6000). If a GPU is not attached, this number will be 0. RunPod does allow you to spin up a pod with 0 GPUs so that you can connect to it via a Terminal or CloudSync to access data. However, the options to connect to RunPod via the web interface will be nonfunctional, even if they are lit up.\n\n![](/img/docs/fb4c0dd-image.png)","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":9,"to":19}}}}],["260",{"pageContent":"Check Your Pod's Logs\n\nAfter confirming that your pod has a GPU attached, the next step is to check your pod's logs for any errors.\n\n1. **Access your pod's logs**: You can view the logs from the pod's settings in the user interface.\n\n2. ![](/img/docs/3500eba-image.png)\\\n   **Look for errors**: Browse through the logs to find any error messages that may provide clues about why you're experiencing a 502 error.","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":21,"to":28}}}}],["261",{"pageContent":"Verify Additional Steps for Official Templates\n\nIn some cases, for our official templates, the user interface does not work right away and may require additional steps to be performed by the user.\n\n1. **Access the template's ReadMe**: Navigate to the template's page and open the ReadMe file.\n\n2. **Follow additional steps**: The ReadMe file should provide instructions on any additional steps you need to perform to get the UI functioning properly. Make sure to follow these instructions closely.\n\nRemember, each template may have unique requirements or steps for setup. It is always recommended to thoroughly review the documentation associated with each template.\n\nIf you continue to experience 502 errors after following these steps, please contact our support team. We're here to help ensure that your experience on our platform is as seamless as possible.","metadata":{"source":"/runpod-docs/docs/references/troubleshooting/troubleshooting-502-errors.md","loc":{"lines":{"from":30,"to":40}}}}],["262",{"pageContent":"---\ntitle: \"Video resources\"\ndescription: \"Explore comprehensive video tutorials on RunPod, covering topics like image generation, text-to-image, and Linux commands, including StableDiffusion, DeepFloyd, ControlNet, Automatic111, and more, with step-by-step instructions and expert guidance.\"\n---\n\n## RunPod Usage\n\n| Video Link                                                                                               | Topics Covered |\n| :------------------------------------------------------------------------------------------------------- | :------------- |\n| [How to redeem your RunPod Coupon](https://www.youtube.com/watch?v=IYqEKwpuyWk&ab_channel=OpenCVCourses) | RunPod         |\n| [RunPod Introduction and Tour](https://www.youtube.com/watch?v=6O1oM_N6pcw&ab_channel=OpenCVCourses)     | General        |\n\n---","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":1,"to":13}}}}],["263",{"pageContent":"Tutorials","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":15,"to":15}}}}],["264",{"pageContent":"| Video Link                                                                                                                    | Topics Covered                |\n| :---------------------------------------------------------------------------------------------------------------------------- | :---------------------------- |\n| [Generate Stable Diffusion Images FAST with RunPod](https://www.youtube.com/watch?v=susnjHSWFq0&t=32s&ab_channel=BillMeeks)   | StableDiffusion, Automatic111 |\n| [Generate Text On Images with DeepFloyd IF](https://www.youtube.com/watch?v=Px7Vv9WYl88&t=2s&ab_channel=BillMeeks)            | DeepFloyd                     |\n| [Remix Your Pics With Stable Diffusion and ControlNet](https://www.youtube.com/watch?v=BqdIdk9LU4w&t=1s&ab_channel=BillMeeks) | StableDiffusion, ControlNet   |\n| [Using Automatic1111 WebUI on RunPod](https://www.youtube.com/watch?v=R6HUQOtsVic&ab_channel=OpenCVCourses)                   | WebUI, Automatic1111          |","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":17,"to":22}}}}],["265",{"pageContent":"| [RUN TextGen AI WebUI LLM On Runpod & Colab!](https://www.youtube.com/watch?v=TP2yID7Ubr4&ab_channel=Aitrepreneur)            | GoogleColab, TextGeneration   |\n| [Make Your Renders 10x Faster With Runpod](https://www.youtube.com/watch?v=sJ-Diy93TAg&ab_channel=RahulAhire)                 | Rendering, Blender            |","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":23,"to":24}}}}],["266",{"pageContent":"---","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":26,"to":26}}}}],["267",{"pageContent":"General/Generic Linux\n\n| Video Link                                                                                                                          | Topics Covered     |\n| :---------------------------------------------------------------------------------------------------------------------------------- | :----------------- |\n| [How to Use the rsync Command \\| Linux Essentials Tutorial](https://www.youtube.com/watch?v=2PnAohLS-Q4&ab_channel=AkamaiDeveloper) | File Transfer      |\n| [SSH Key Authentication \\| How to Create SSH Key Pairs](https://www.youtube.com/watch?v=33dEcCKGBO4&ab_channel=AkamaiDeveloper)     | SSH Authentication |","metadata":{"source":"/runpod-docs/docs/references/video-resources.md","loc":{"lines":{"from":28,"to":33}}}}],["268",{"pageContent":"---\ntitle: API Endpoints\nid: api-endpoints\nhide_table_of_contents: true\nsidebar_position: 1\ndescription: \"Integrate API documentation into your project with the @theme/ApiDocMdx library, enabling seamless documentation management for improved user experience.\"\n---\n\nimport ApiDocMdx from '@theme/ApiDocMdx';\n\n<ApiDocMdx id=\"using-single-yaml\" />","metadata":{"source":"/runpod-docs/docs/references/_api-endpoints.md","loc":{"lines":{"from":1,"to":11}}}}],["269",{"pageContent":"---\ntitle: Endpoints\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nInteracting with RunPod's Endpoints is a core feature of the SDK, enabling the execution of tasks and the retrieval of results.\nThis section covers the synchronous and asynchronous execution methods, along with checking the status of operations.\n\n## Prerequisites\n\nBefore using the RunPod Go SDK, ensure that you have:\n\n- [Installed the RunPod Go SDK](/sdks/go/overview#install).\n- Configured your API key.","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":1,"to":16}}}}],["270",{"pageContent":"Set your Endpoint Id\n\nSet your RunPod API key and your Endpoint Id as environment variables.\n\n```go\npackage main\n\nimport (\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com.runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\t// Retrieve the API key and base URL from environment variables\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\t// Check if environment variables are set\n\tif apiKey == \"\" {\n\t\tlog.Fatalf(\"Environment variable RUNPOD_API_KEY is not set\")\n\t}\n\tif baseURL == \"\" {\n\t\tlog.Fatalf(\"Environment variable RUNPOD_BASE_URL is not set\")\n\t}\n\n\n    // Use the endpoint object\n    // ...\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":18,"to":50}}}}],["271",{"pageContent":"This allows all calls to pass through your Endpoint Id with a valid API key.\n\nThe following are actions you use on the\n\n- [RunSync](#run-sync)\n- [Run](#run-async)\n- [Stream](#stream)\n- [Health](#health-check)\n- [Status](#status)\n- [Cancel](#cancel)\n- [Purge Queue](#purge-queue)\n\nHere is the revised documentation based on the Go Sample:","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":53,"to":65}}}}],["272",{"pageContent":"Run the Endpoint {#run}\n\nRun the Endpoint using either the asynchronous `run` or synchronous `runSync` method.\n\nChoosing between asynchronous and synchronous execution hinges on your task's needs and application design.","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":67,"to":71}}}}],["273",{"pageContent":"Run synchronously {#run-sync}\n\nTo execute an endpoint synchronously and wait for the result, use the `runSync` method on your endpoint.\nThis method blocks the execution until the endpoint run is complete or until it times out.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com.runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create endpoint: %v\", err)\n\t}\n\n\tjobInput := rpEndpoint.RunSyncInput{\n\t\tJobInput: &rpEndpoint.JobInput{\n\t\t\tInput: map[string]interface{}{\n\t\t\t\t\"prompt\": \"Hello World\",\n\t\t\t},\n\t\t},\n\t\tTimeout: sdk.Int(120),\n\t}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":73,"to":114}}}}],["274",{"pageContent":"output, err := endpoint.RunSync(&jobInput)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tdata, _ := json.Marshal(output)\n\tfmt.Printf(\"output: %s\\n\", data)\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":116,"to":123}}}}],["275",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"delayTime\": 18,\n  \"executionTime\": 36595,\n  \"id\": \"sync-d050a3f6-791a-4aff-857a-66c759db4a06-u1\",\n  \"output\": [\n    {\n      \"choices\": [],\n      \"usage\": {}\n    }\n  ],\n  \"status\": \"COMPLETED\",\n  \"started\": true,\n  \"completed\": true,\n  \"succeeded\": true\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":126,"to":148}}}}],["276",{"pageContent":"Run asynchronously {#run-async}\n\nAsynchronous execution allows for non-blocking operations, enabling your code to perform other tasks while waiting for an operation to complete.\n\nFor non-blocking operations, use the `run` method on the endpoint.\nThis method allows you to start an endpoint run and then check its status or wait for its completion at a later time.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com/runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create endpoint: %v\", err)\n\t}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":150,"to":184}}}}],["277",{"pageContent":"jobInput := rpEndpoint.RunInput{\n\t\tJobInput: &rpEndpoint.JobInput{\n\t\t\tInput: map[string]interface{}{\n\t\t\t\t\"mock_delay\": 95,\n\t\t\t},\n\t\t},\n\t\tRequestTimeout: sdk.Int(120),\n\t}\n\n\toutput, err := endpoint.Run(&jobInput)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tdata, _ := json.Marshal(output)\n\tfmt.Printf(\"output: %s\\n\", data)\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":186,"to":202}}}}],["278",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"d4e960f6-073f-4219-af24-cbae6b532c31-u1\",\n  \"status\": \"IN_QUEUE\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":205,"to":216}}}}],["279",{"pageContent":"Get results from an asynchronous run\n\nThe following example shows how to get the results of an asynchronous run.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"time\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com/runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create endpoint: %v\", err)\n\t}\n\n\t// Initiate the asynchronous run\n\tjobInput := rpEndpoint.RunInput{\n\t\tJobInput: &rpEndpoint.JobInput{\n\t\t\tInput: map[string]interface{}{\"mock_delay\": 95},\n\t\t},\n\t\tRequestTimeout: sdk.Int(120),\n\t}\n\trunOutput, err := endpoint.Run(&jobInput)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to initiate the run: %v\", err)\n\t}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":218,"to":262}}}}],["280",{"pageContent":"// Extract the ID from the run output\n\trunID := *runOutput.Id\n\tfmt.Printf(\"Run ID: %s\\n\", runID)\n\n\t// Prepare the input for status polling\n\tstatusInput := rpEndpoint.StatusInput{\n\t\tId: sdk.String(runID),\n\t}\n\n\t// Poll the status until it completes or fails\n\tvar statusOutput *rpEndpoint.StatusOutput\n\tfor {\n\t\tstatusOutput, err = endpoint.Status(&statusInput)\n\t\tif err != nil {\n\t\t\tlog.Printf(\"Error checking status: %v\", err)\n\t\t\ttime.Sleep(5 * time.Second)\n\t\t\tcontinue\n\t\t}\n\n\t\tstatusJSON, _ := json.Marshal(statusOutput)\n\t\tfmt.Printf(\"Status: %s\\n\", statusJSON)\n\n\t\tif *statusOutput.Status == \"COMPLETED\" || *statusOutput.Status == \"FAILED\" {\n\t\t\tbreak\n\t\t}\n\n\t\ttime.Sleep(5 * time.Second)\n\t}\n\n\t// Retrieve the final result (assuming it's available in the status output)\n\tif *statusOutput.Status == \"COMPLETED\" {\n\t\tfmt.Println(\"Run completed successfully!\")\n\t\t// Handle the completed run's output if needed\n\t} else {\n\t\tfmt.Println(\"Run failed!\")\n\t\t// Handle the failed run if needed\n\t}\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":264,"to":301}}}}],["281",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\nRun ID: 353b1e99-2f35-43a8-8a8b-001d59df8aa1-u1\nStatus: {\"id\":\"353b1e99-2f35-43a8-8a8b-001d59df8aa1-u1\",\"status\":\"IN_QUEUE\"}\nStatus: {\"delayTime\":536,\"executionTime\":239,\"id\":\"353b1e99-2f35-43a8-8a8b-001d59df8aa1-u1\",\"output\":\"69.30.85.167\",\"status\":\"COMPLETED\"}\nRun completed successfully!\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":304,"to":315}}}}],["282",{"pageContent":"Stream {#stream}\n\nStream allows you to stream the output of an Endpoint run.\nTo enable streaming, your handler must support the `\"return_aggregate_stream\": True` option on the `start` method of your Handler.\nOnce enabled, use the `stream` method to receive data as it becomes available.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\trequest, err := endpoint.Run(&rpEndpoint.RunInput{\n\t\tJobInput: &rpEndpoint.JobInput{\n\t\t\tInput: map[string]interface{}{\n\t\t\t\t\"prompt\": \"Hello World\",\n\t\t\t},\n\t\t},\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tstreamChan := make(chan rpEndpoint.StreamResult, 100)","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":317,"to":361}}}}],["283",{"pageContent":"err = endpoint.Stream(&rpEndpoint.StreamInput{Id: request.Id}, streamChan)\n\tif err != nil {\n\t\t// timeout reached, if we want to get the data that has been streamed\n\t\tif err.Error() == \"ctx timeout reached\" {\n\t\t\tfor data := range streamChan {\n\t\t\t\tdt, _ := json.Marshal(data)\n\t\t\t\tfmt.Printf(\"output:%s\\n\", dt)\n\t\t\t}\n\t\t}\n\t\tpanic(err)\n\t}\n\n\tfor data := range streamChan {\n\t\tdt, _ := json.Marshal(data)\n\t\tfmt.Printf(\"output:%s\\n\", dt)\n\t}\n\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":363,"to":380}}}}],["284",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{ id: 'cb68890e-436f-4234-955d-001db6afe972-u1', status: 'IN_QUEUE' }\n{\n  \"output\": \"H\"\n}\n{\n  \"output\": \"e\"\n}\n{\n  \"output\": \"l\"\n}\n{\n  \"output\": \"l\"\n}\n{\n  \"output\": \"o\"\n}\n{\n  \"output\": \",\"\n}\n{\n  \"output\": \" \"\n}\n{\n  \"output\": \"W\"\n}\n{\n  \"output\": \"o\"\n}\n{\n  \"output\": \"r\"\n}\n{\n  \"output\": \"l\"\n}\n{\n  \"output\": \"d\"\n}\n{\n  \"output\": \"!\"\n}\ndone streaming\n```\n\n</TabItem>\n\n<TabItem value=\"handler\" label=\"Handler\" default>\n\nYou must define your handler to support the `\"return_aggregate_stream\": True` option on the `start` method.\n\n```python\nfrom time import sleep\nimport runpod\n\n\ndef handler(job):\n    job_input = job[\"input\"][\"prompt\"]\n\n    for i in job_input:\n        sleep(1)  # sleep for 1 second for effect\n        yield i\n\n\nrunpod.serverless.start(\n    {\n        \"handler\": handler,\n        \"return_aggregate_stream\": True,  # Ensures aggregated results are streamed back\n    }\n)\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":383,"to":457}}}}],["285",{"pageContent":"Health check\n\nMonitor the health of an endpoint by checking its status, including jobs completed, failed, in progress, in queue, and retried, as well as the status of workers.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com/runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\thealthInput := rpEndpoint.StatusInput{\n\t\tId: sdk.String(\"20aad8ef-9c86-4fcd-a349-579ce38e8bfd-u1\"),\n\t}\n\toutput, err := endpoint.Status(&healthInput)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\thealthData, _ := json.Marshal(output)\n\tfmt.Printf(\"health output: %s\\n\", healthData)\n\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":459,"to":502}}}}],["286",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"jobs\": {\n    \"completed\": 72,\n    \"failed\": 1,\n    \"inProgress\": 6,\n    \"inQueue\": 0,\n    \"retried\": 1\n  },\n  \"workers\": {\n    \"idle\": 4,\n    \"initializing\": 0,\n    \"ready\": 4,\n    \"running\": 1,\n    \"throttled\": 0\n  }\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":505,"to":528}}}}],["287",{"pageContent":"Status\n\nUse the `status` method and specify the `id` of the run to get the status of a run.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com/runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create endpoint: %v\", err)\n\t}\n\tinput := rpEndpoint.StatusInput{\n\t\tId: sdk.String(\"5efff030-686c-4179-85bb-31b9bf97b944-u1\"),\n\t}\n\toutput, err := endpoint.Status(&input)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdt, _ := json.Marshal(output)\n\tfmt.Printf(\"output:%s\\n\", dt)\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":530,"to":572}}}}],["288",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"delayTime\": 18,\n  \"id\": \"792b1497-b2c8-4c95-90bf-4e2a6a2a37ff-u1\",\n  \"status\": \"IN_PROGRESS\",\n  \"started\": true,\n  \"completed\": false,\n  \"succeeded\": false\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":575,"to":590}}}}],["289",{"pageContent":"Cancel\n\nYou can cancel a Job request by using the `cancel()` function on the run request.\nYou might want to cancel a Job because it's stuck with a status of `IN_QUEUE` or `IN_PROGRESS`, or because you no longer need the result.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com/runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tcancelInput := rpEndpoint.CancelInput{\n\t\tId: sdk.String(\"00edfd03-8094-46da-82e3-ea47dd9566dc-u1\"),\n\t}\n\toutput, err := endpoint.Cancel(&cancelInput)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\thealthData, _ := json.Marshal(output)\n\tfmt.Printf(\"health output: %s\\n\", healthData)\n\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":592,"to":636}}}}],["290",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"5fb6a8db-a8fa-41a1-ad81-f5fad9755f9e-u1\",\n  \"status\": \"CANCELLED\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":639,"to":650}}}}],["291",{"pageContent":"Timeout\n\nYou can set the maximum time to wait for a response from the endpoint using the `RequestTimeout` field in the `RunInput` struct.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com/runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create endpoint: %v\", err)\n\t}\n\n\tjobInput := rpEndpoint.RunInput{\n\t\tJobInput: &rpEndpoint.JobInput{\n\t\t\tInput: map[string]interface{}\n\t\tRequestTimeout: sdk.Int(120),\n\t}\n\n\toutput, err := endpoint.Run(&jobInput)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tdata, _ := json.Marshal(output)\n\tfmt.Printf(\"output: %s\\n\", data)\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":652,"to":698}}}}],["292",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"43309f93-0422-4eac-92cf-e385dee36e99-u1\",\n  \"status\": \"IN_QUEUE\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":701,"to":712}}}}],["293",{"pageContent":"Execution policy\n\nYou can specify the TTL (Time-to-Live) and ExecutionTimeout values for the job using the `Input` map of the `JobInput` struct.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com/runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create endpoint: %v\", err)\n\t}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":714,"to":745}}}}],["294",{"pageContent":"jobInput := rpEndpoint.RunInput{\n\t\tJobInput: &rpEndpoint.JobInput{\n\t\t\tInput: map[string]interface{}{\n\t\t\t\t\"ttl\":               3600, // Set the TTL value, e.g., 3600 seconds (1 hour)\n\t\t\t\t\"execution_timeout\": 300,  // Set the ExecutionTimeout value, e.g., 300 seconds (5 minutes)\n\t\t\t},\n\t\t},\n\t\tRequestTimeout: sdk.Int(120),\n\t}\n\n\toutput, err := endpoint.Run(&jobInput)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tdata, _ := json.Marshal(output)\n\tfmt.Printf(\"output: %s\\n\", data)\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":747,"to":764}}}}],["295",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"21bd3763-dcbf-4091-84ee-85b80907a020-u1\",\n  \"status\": \"IN_QUEUE\"\n}\n```\n\n</TabItem>\n</Tabs>\n\nFor more information, see [Execution policy](/serverless/endpoints/send-requests#execution-policies).","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":767,"to":780}}}}],["296",{"pageContent":"Purge Queue\n\nCreate an instance of the `PurgeQueueInput` struct and set the desired values.\nCall the `PurgeQueue` method of the Endpoint with the `PurgeQueueInput` instance.\n\n`PurgeQueue()` doesn't affect Jobs in progress.\n\n<Tabs>\n  <TabItem value=\"go\" label=\"Go\" default>\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/runpod/go-sdk/pkg/sdk\"\n\t\"github.com/runpod/go-sdk/pkg/sdk/config\"\n\trpEndpoint \"github.com/runpod/go-sdk/pkg/sdk/endpoint\"\n)\n\nfunc main() {\n\tapiKey := os.Getenv(\"RUNPOD_API_KEY\")\n\tbaseURL := os.Getenv(\"RUNPOD_BASE_URL\")\n\n\tendpoint, err := rpEndpoint.New(\n\t\t&config.Config{ApiKey: &apiKey},\n\t\t&rpEndpoint.Option{EndpointId: &baseURL},\n\t)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create endpoint: %v\", err)\n\t}\n\n\tpurgeQueueInput := rpEndpoint.PurgeQueueInput{\n\t\tRequestTimeout: sdk.Int(5), // Set the request timeout to 5 seconds\n\t}\n\n\tpurgeQueueOutput, err := endpoint.PurgeQueue(&purgeQueueInput)\n\tif err != nil {\n\t\tpanic(err)\n\t}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":782,"to":824}}}}],["297",{"pageContent":"fmt.Printf(\"Status: %s\\n\", *purgeQueueOutput.Status)\n\tfmt.Printf(\"Removed: %d\\n\", *purgeQueueOutput.Removed)\n}","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":826,"to":828}}}}],["298",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\nStatus: completed\nRemoved: 1\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/go/endpoints.md","loc":{"lines":{"from":831,"to":840}}}}],["299",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ntags:\n  - go\n---\n\nGet started with setting up your RunPod projects using Go.\nWhether you're building web applications, server-side implementations, or automating tasks, the RunPod Go SDK provides the tools you need.\nThis guide outlines the steps to get your development environment ready and integrate RunPod into your Go projects.\n\n## Prerequisites\n\nBefore you begin, ensure that you have the following:\n\n- Go installed on your machine (version 1.16 or later)\n- A RunPod account with an API key and Endpoint Id","metadata":{"source":"/runpod-docs/docs/sdks/go/overview.md","loc":{"lines":{"from":1,"to":17}}}}],["300",{"pageContent":"Install the RunPod SDK {#install}\n\nBefore integrating RunPod into your project, you'll need to install the SDK.\n\nTo install the RunPod SDK, run the following `go get` command in your project directory.\n\n```command\ngo get github.com/runpod/go-sdk\n```\n\nThis command installs the `runpod-sdk` package.\nThen run the following command to install the dependcies:\n\n```command\ngo mod tidy\n```\n\nFor more details about the package, visit the [Go package page](https://pkg.go.dev/github.com/runpod/go-sdk/pkg/sdk) or the [GitHub repository](https://github.com/runpod/go-sdk).","metadata":{"source":"/runpod-docs/docs/sdks/go/overview.md","loc":{"lines":{"from":19,"to":36}}}}],["301",{"pageContent":"Add your API key\n\nTo use the RunPod SDK in your project, you first need to import it and configure it with your API key and endpoint ID. Ensure these values are securely stored, preferably as environment variables.\n\nBelow is a basic example of how to initialize and use the RunPod SDK in your Go project.\n\n```go\nfunc main() {\n    endpoint, err := rpEndpoint.New(\n        &config.Config{ApiKey: sdk.String(os.Getenv(\"RUNPOD_API_KEY\"))},\n        &rpEndpoint.Option{EndpointId: sdk.String(os.Getenv(\"RUNPOD_BASE_URL\"))},\n    )\n    if err != nil {\n        panic(err)\n    }\n\n    // Use the endpoint object\n    // ...\n}\n```\n\nThis snippet demonstrates how to import the SDK, initialize it with your API key, and reference a specific endpoint using its ID.","metadata":{"source":"/runpod-docs/docs/sdks/go/overview.md","loc":{"lines":{"from":38,"to":59}}}}],["302",{"pageContent":"Secure your API key\n\nWhen working with the RunPod SDK, it's essential to secure your API key.\nStoring the API key in environment variables is recommended, as shown in the initialization example. This method keeps your key out of your source code and reduces the risk of accidental exposure.\n\n:::note\n\nUse environment variables or secure secrets management solutions to handle sensitive information like API keys.\n\n:::\n\nFor more information, see the following:\n\n- [RunPod SDK Go Package](https://pkg.go.dev/github.com/runpod/go-sdk/pkg/sdk)\n- [RunPod GitHub Repository](https://github.com/runpod/go-sdk)\n- [Endpoints](/sdks/go/endpoints)","metadata":{"source":"/runpod-docs/docs/sdks/go/overview.md","loc":{"lines":{"from":61,"to":76}}}}],["303",{"pageContent":"---\ntitle: \"Configurations\"\nsidebar_position: 1\ndescription: \"Configure your environment with essential arguments: containerDiskInGb, dockerArgs, env, imageName, name, and volumeInGb, to ensure correct setup and operation of your container.\"\n---\n\nFor details on queries, mutations, fields, and inputs, see the [RunPod GraphQL Spec](https://graphql-spec.runpod.io/).\n\nWhen configuring your environment, certain arguments are essential to ensure the correct setup and operation. Below is a detailed overview of each required argument:\n\n### `containerDiskInGb`\n\n- **Description**: Specifies the size of the disk allocated for the container in gigabytes. This space is used for the operating system, installed applications, and any data generated or used by the container.\n- **Type**: Integer\n- **Example**: `10` for a 10 GB disk size.","metadata":{"source":"/runpod-docs/docs/sdks/graphql/configurations.md","loc":{"lines":{"from":1,"to":15}}}}],["304",{"pageContent":"`dockerArgs`\n\n- **Description**: If specified, overrides the [container start command](https://docs.docker.com/engine/reference/builder/#cmd). If this argument is not provided, it will rely on the start command provided in the docker image.\n- **Type**: String\n- **Example**: `sleep infinity` to run the container in the background.\n\n<!--\nContains additional arguments that are passed directly to Docker when starting the container. This can include mount points, network settings, or any other Docker command-line arguments.\n-->\n\n### `env`\n\n- **Description**: A set of environment variables to be set within the container. These can configure application settings, external service credentials, or any other configuration data required by the software running in the container.\n- **Type**: Dictionary or Object\n- **Example**: `{\"DATABASE_URL\": \"postgres://user:password@localhost/dbname\"}`.","metadata":{"source":"/runpod-docs/docs/sdks/graphql/configurations.md","loc":{"lines":{"from":17,"to":31}}}}],["305",{"pageContent":"`imageName`\n\n- **Description**: The name of the Docker image to use for the container. This should include the repository name and tag, if applicable.\n- **Type**: String\n- **Example**: `\"nginx:latest\"` for the latest version of the Nginx image.\n\n### `name`\n\n- **Description**: The name assigned to the container instance. This name is used for identification and must be unique within the context it's being used.\n- **Type**: String\n- **Example**: `\"my-app-container\"`.\n\n### `volumeInGb`\n\n- **Description**: Defines the size of an additional persistent volume in gigabytes. This volume is used for storing data that needs to persist between container restarts or redeployments.\n- **Type**: Integer\n- **Example**: `5` for a 5GB persistent volume.\n\nEnsure that these arguments are correctly specified in your configuration to avoid errors during deployment.\n\nOptional arguments may also be available, providing additional customization and flexibility for your setup.","metadata":{"source":"/runpod-docs/docs/sdks/graphql/configurations.md","loc":{"lines":{"from":33,"to":53}}}}],["306",{"pageContent":"---\ntitle: \"Manage Endpoints\"\ndescription: \"Create, modify, or delete serverless endpoints using GraphQL queries and mutations with RunPod API, specifying GPU IDs, template IDs, and other endpoint settings.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n`gpuIds`, `name`, and `templateId` are required arguments; all other arguments are optional, and default values will be used if unspecified.","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":1,"to":9}}}}],["307",{"pageContent":"Create a new Serverless Endpoint","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":11,"to":11}}}}],["308",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { saveEndpoint(input: { gpuIds: \\\"AMPERE_16\\\", idleTimeout: 5, locations: \\\"US\\\", name: \\\"Generated Endpoint -fb\\\", networkVolumeId: \\\"\\\", scalerType: \\\"QUEUE_DELAY\\\", scalerValue: 4, templateId: \\\"xkhgg72fuo\\\", workersMax: 3, workersMin: 0 }) { gpuIds id idleTimeout locations name scalerType scalerValue templateId workersMax workersMin } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  saveEndpoint(input: {\n    # options for gpuIds are \"AMPERE_16,AMPERE_24,AMPERE_48,AMPERE_80,ADA_24\"\n    gpuIds: \"AMPERE_16\",\n    idleTimeout: 5,\n    # leave locations as an empty string or null for any region\n    # options for locations are \"CZ,FR,GB,NO,RO,US\"\n    locations: \"US\",\n    # append -fb to your endpoint's name to enable FlashBoot","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":13,"to":32}}}}],["309",{"pageContent":"name: \"Generated Endpoint -fb\",\n    # uncomment below and provide an ID to mount a network volume to your workers\n    # networkVolumeId: \"\",\n    scalerType: \"QUEUE_DELAY\",\n    scalerValue: 4,\n    templateId: \"xkhgg72fuo\",\n    workersMax: 3,\n    workersMin: 0\n  }) {\n    gpuIds\n    id\n    idleTimeout\n    locations\n    name\n    # networkVolumeId\n    scalerType\n    scalerValue\n    templateId\n    workersMax\n    workersMin\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"saveEndpoint\": {\n      \"gpuIds\": \"AMPERE_16\",\n      \"id\": \"i02xupws21hp6i\",\n      \"idleTimeout\": 5,\n      \"locations\": \"US\",\n      \"name\": \"Generated Endpoint -fb\",\n      \"scalerType\": \"QUEUE_DELAY\",\n      \"scalerValue\": 4,\n      \"templateId\": \"xkhgg72fuo\",\n      \"workersMax\": 3,\n      \"workersMin\": 0\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":33,"to":77}}}}],["310",{"pageContent":"Modify an existing Serverless Endpoint","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":79,"to":79}}}}],["311",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { saveEndpoint(input: { id: \\\"i02xupws21hp6i\\\", gpuIds: \\\"AMPERE_16\\\", name: \\\"Generated Endpoint -fb\\\", templateId: \\\"xkhgg72fuo\\\", workersMax: 0 }) { id gpuIds name templateId workersMax } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  saveEndpoint(input: {\n    id: \"i02xupws21hp6i\",\n    gpuIds: \"AMPERE_16\",\n    name: \"Generated Endpoint -fb\",\n    templateId: \"xkhgg72fuo\",\n    # Modify your template options here (or above, if applicable).\n    # For this example, we've modified the endpoint's max workers.\n    workersMax: 0\n  }) {\n    id\n    gpuIds\n    name\n    templateId\n    # You can include what you've changed here, too.\n    workersMax\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"data\": {","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":81,"to":115}}}}],["312",{"pageContent":"\"saveEndpoint\": {\n      \"id\": \"i02xupws21hp6i\",\n      \"gpuIds\": \"AMPERE_16\",\n      \"name\": \"Generated Endpoint -fb\",\n      \"templateId\": \"xkhgg72fuo\",\n      \"workersMax\": 0\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":116,"to":127}}}}],["313",{"pageContent":"# View your Endpoints","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":129,"to":129}}}}],["314",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n     --header 'content-type: application/json' \\\n     --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n     --data '{\"query\": \"query Endpoints { myself { endpoints { gpuIds id idleTimeout locations name networkVolumeId pods { desiredStatus } scalerType scalerValue templateId workersMax workersMin } serverlessDiscount { discountFactor type expirationDate } } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nquery Endpoints {\n  myself {\n    endpoints {\n      gpuIds\n      id\n      idleTimeout\n      locations\n      name\n      networkVolumeId\n      pods {\n        desiredStatus\n      }\n      scalerType\n      scalerValue\n      templateId\n      workersMax\n      workersMin\n    }\n    serverlessDiscount {\n      discountFactor\n      type\n      expirationDate\n    }\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"myself\": {","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":131,"to":173}}}}],["315",{"pageContent":"\"endpoints\": [\n        {\n          \"gpuIds\": \"AMPERE_16\",\n          \"id\": \"i02xupws21hp6i\",\n          \"idleTimeout\": 5,\n          \"locations\": \"US\",\n          \"name\": \"Generated Endpoint -fb\",\n          \"networkVolumeId\": null,\n          \"pods\": [],\n          \"scalerType\": \"QUEUE_DELAY\",\n          \"scalerValue\": 4,\n          \"templateId\": \"xkhgg72fuo\",\n          \"workersMax\": 0,\n          \"workersMin\": 0\n        }\n      ],\n      \"serverlessDiscount\": null\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":174,"to":196}}}}],["316",{"pageContent":"# Delete a Serverless Endpoints\n\nNote that your endpoint's min and max workers must both be set to zero for your call to work.\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\">\n```curl\ncurl --request POST \\\n\t--header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { deleteEndpoint(id: \\\"i02xupws21hp6i\\\") }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  deleteEndpoint(id: \"i02xupws21hp6i\")\n}\n```\n  </TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"deleteEndpoint\": null\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-endpoints.md","loc":{"lines":{"from":198,"to":227}}}}],["317",{"pageContent":"---\ntitle: \"Manage Templates\"\nsidebar_position: 3\ndescription: Create, modify, and delete templates in RunPod using GraphQL API with various parameters for container disk size, Docker arguments, environment variables, and more.\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nRequrired arguments:\n\n- `containerDiskInGb`\n- `dockerArgs`\n- `env`\n- `imageName`\n- `name`\n- `volumeInGb`\n\nAll other arguments are optional.\n\nIf your container image is private, you can also specify Docker login credentials with a `containerRegistryAuthId` argument, which takes the ID (_not_ the name) of the container registry credentials you saved in your RunPod user settings as a string.\n\n:::note\n\nTemplate names must be unique as well; if you try to create a new template with the same name as an existing one, your call will fail.\n:::","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":1,"to":26}}}}],["318",{"pageContent":"Create a Pod Template","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":28,"to":28}}}}],["319",{"pageContent":"Create GPU Template","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":30,"to":30}}}}],["320",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \\\"sleep infinity\\\", env: [ { key: \\\"key1\\\", value: \\\"value1\\\" }, { key: \\\"key2\\\", value: \\\"value2\\\" } ], imageName: \\\"ubuntu:latest\\\", name: \\\"Generated Template\\\", ports: \\\"8888/http,22/tcp\\\", readme: \\\"## Hello, World!\\\", volumeInGb: 15, volumeMountPath: \\\"/workspace\\\" }) { containerDiskInGb dockerArgs env { key value } id imageName name ports readme volumeInGb volumeMountPath } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  saveTemplate(input: {\n    containerDiskInGb: 5,\n    dockerArgs: \"sleep infinity\",\n    env: [\n      {\n        key: \"key1\",\n        value: \"value1\"\n      },\n      {\n        key: \"key2\",\n        value: \"value2\"\n      }\n    ],","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":32,"to":56}}}}],["321",{"pageContent":"imageName: \"ubuntu:latest\",\n    name: \"Generated Template\",\n    ports: \"8888/http,22/tcp\",\n    readme: \"## Hello, World!\",\n    volumeInGb: 15,\n    volumeMountPath: \"/workspace\"\n  }) {\n    containerDiskInGb\n    dockerArgs\n    env {\n      key\n      value\n    }\n    id\n    imageName\n    name\n    ports\n    readme\n    volumeInGb\n    volumeMountPath\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"saveTemplate\": {\n      \"containerDiskInGb\": 5,\n      \"dockerArgs\": \"sleep infinity\",\n      \"env\": [\n        {\n          \"key\": \"key1\",\n          \"value\": \"value1\"\n        },\n        {\n          \"key\": \"key2\",\n          \"value\": \"value2\"\n        }\n      ],\n      \"id\": \"wphkv67a0p\",\n      \"imageName\": \"ubuntu:latest\",\n      \"name\": \"Generated Template\",\n      \"ports\": \"8888/http,22/tcp\",\n      \"readme\": \"## Hello, World!\",\n      \"volumeInGb\": 15,\n      \"volumeMountPath\": \"/workspace\"\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":57,"to":110}}}}],["322",{"pageContent":"Create Serverless Template\n\nFor Serverless templates, always pass `0` for `volumeInGb`, since Serverless workers don't have persistent storage (other than those with network volumes).","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":112,"to":114}}}}],["323",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \\\"python handler.py\\\", env: [ { key: \\\"key1\\\", value: \\\"value1\\\" }, { key: \\\"key2\\\", value: \\\"value2\\\" } ], imageName: \\\"runpod/serverless-hello-world:latest\\\", isServerless: true, name: \\\"Generated Serverless Template\\\", readme: \\\"## Hello, World!\\\", volumeInGb: 0 }) { containerDiskInGb dockerArgs env { key value } id imageName isServerless name readme } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  saveTemplate(input: {\n    containerDiskInGb: 5,\n    dockerArgs: \"python handler.py\",\n    env: [\n      {\n        key: \"key1\",\n        value: \"value1\"\n      },\n      {\n        key: \"key2\",\n        value: \"value2\"\n      }\n    ],","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":116,"to":140}}}}],["324",{"pageContent":"imageName: \"runpod/serverless-hello-world:latest\",\n    isServerless: true,\n    name: \"Generated Serverless Template\",\n    readme: \"## Hello, World!\",\n    volumeInGb: 0\n  }) {\n    containerDiskInGb\n    dockerArgs\n    env {\n      key\n      value\n    }\n    id\n    imageName\n    isServerless\n    name\n    readme\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"saveTemplate\": {\n      \"containerDiskInGb\": 5,\n      \"dockerArgs\": \"python handler.py\",\n      \"env\": [\n        {\n          \"key\": \"key1\",\n          \"value\": \"value1\"\n        },\n        {\n          \"key\": \"key2\",\n          \"value\": \"value2\"\n        }\n      ],\n      \"id\": \"xkhgg72fuo\",\n      \"imageName\": \"runpod/serverless-hello-world:latest\",\n      \"isServerless\": true,\n      \"name\": \"Generated Serverless Template\",\n      \"readme\": \"## Hello, World!\"\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":141,"to":189}}}}],["325",{"pageContent":"Modify a Template","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":191,"to":191}}}}],["326",{"pageContent":"Modify a GPU Pod Template","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":193,"to":193}}}}],["327",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { saveTemplate(input: { id: \\\"wphkv67a0p\\\", containerDiskInGb: 5, dockerArgs: \\\"sleep infinity\\\", env: [ { key: \\\"key1\\\", value: \\\"value1\\\" }, { key: \\\"key2\\\", value: \\\"value2\\\" } ], imageName: \\\"ubuntu:latest\\\", name: \\\"Generated Template\\\", volumeInGb: 15, readme: \\\"## Goodbye, World!\\\" }) { id containerDiskInGb dockerArgs env { key value } imageName name volumeInGb readme } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  saveTemplate(input: {\n    id: \"wphkv67a0p\",\n    containerDiskInGb: 5,\n    dockerArgs: \"sleep infinity\",\n    env: [\n      {\n        key: \"key1\",\n        value: \"value1\"\n      },\n      {\n        key: \"key2\",\n        value: \"value2\"\n      }\n    ],\n    imageName: \"ubuntu:latest\",","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":195,"to":221}}}}],["328",{"pageContent":"name: \"Generated Template\",\n    volumeInGb: 15,\n    # Modify your template options here (or above, if applicable).\n    # For this example, we've modified the template's README.\n    readme: \"## Goodbye, World!\"\n  }) {\n    id\n    containerDiskInGb\n    dockerArgs\n    env {\n      key\n      value\n    }\n    imageName\n    name\n    volumeInGb\n    # You can include what you've changed here, too.\n    readme\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"saveTemplate\": {\n      \"id\": \"wphkv67a0p\",\n      \"containerDiskInGb\": 5,\n      \"dockerArgs\": \"sleep infinity\",\n      \"env\": [\n        {\n          \"key\": \"key1\",\n          \"value\": \"value1\"\n        },\n        {\n          \"key\": \"key2\",\n          \"value\": \"value2\"\n        }\n      ],\n      \"imageName\": \"ubuntu:latest\",\n      \"name\": \"Generated Template\",\n      \"volumeInGb\": 15,\n      \"readme\": \"## Goodbye, World!\"\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":222,"to":271}}}}],["329",{"pageContent":"Modify a Serverless Template","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":273,"to":273}}}}],["330",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { saveTemplate(input: { id: \\\"xkhgg72fuo\\\", containerDiskInGb: 5, dockerArgs: \\\"python handler.py\\\", env: [ { key: \\\"key1\\\", value: \\\"value1\\\" }, { key: \\\"key2\\\", value: \\\"value2\\\" } ], imageName: \\\"runpod/serverless-hello-world:latest\\\", name: \\\"Generated Serverless Template\\\", volumeInGb: 0, readme: \\\"## Goodbye, World!\\\" }) { id containerDiskInGb dockerArgs env { key value } imageName name readme } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  saveTemplate(input: {\n    id: \"xkhgg72fuo\",\n    containerDiskInGb: 5,\n    dockerArgs: \"python handler.py\",\n    env: [\n      {\n        key: \"key1\",\n        value: \"value1\"\n      },\n      {\n        key: \"key2\",\n        value: \"value2\"\n      }\n    ],","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":275,"to":300}}}}],["331",{"pageContent":"imageName: \"runpod/serverless-hello-world:latest\",\n    name: \"Generated Serverless Template\",\n    volumeInGb: 0,\n    # Modify your template options here (or above, if applicable).\n    # For this example, we've modified the template's README.\n    readme: \"## Goodbye, World!\"\n  }) {\n    id\n    containerDiskInGb\n    dockerArgs\n    env {\n      key\n      value\n    }\n    imageName\n    name\n    # You can include what you've changed here, too.\n    readme\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"saveTemplate\": {\n      \"id\": \"xkhgg72fuo\",\n      \"containerDiskInGb\": 5,\n      \"dockerArgs\": \"python handler.py\",\n      \"env\": [\n        {\n          \"key\": \"key1\",\n          \"value\": \"value1\"\n        },\n        {\n          \"key\": \"key2\",\n          \"value\": \"value2\"\n        }\n      ],\n      \"imageName\": \"runpod/serverless-hello-world:latest\",\n      \"name\": \"Generated Serverless Template\",\n      \"readme\": \"## Goodbye, World!\"\n    }\n  }\n}\n```","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":301,"to":347}}}}],["332",{"pageContent":"</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":348,"to":349}}}}],["333",{"pageContent":"Delete a template\n\nNote that the template you'd like to delete must not be in use by any Pods or assigned to any Serverless endpoints. It can take up to 2 minutes to be able to delete a template after its most recent use by a Pod or Serverless endpoint, too.\n\nThe same mutation is used for deleting both Pod and Serverless templates.\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n\n```curl\ncurl --request POST \\\n\t--header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { deleteTemplate(templateName: \\\"Generated Template\\\") }\"}'\n```\n\n</TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  deleteTemplate(templateName: \"Generated Template\")\n}\n```\n  </TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"deleteTemplate\": null\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pod-templates.md","loc":{"lines":{"from":351,"to":384}}}}],["334",{"pageContent":"---\ntitle: \"Manage Pods\"\nsidebar_position: 2\ndescription: \"Manage and deploy high-performance computing resources with RunPod's API, including authentication, GraphQL queries, and management of pods, GPUs, and storage.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n## Authentication\n\nRunPod uses API Keys for all API requests.\nGo to [Settings](https://www.runpod.io/console/user/settings) to manage your API keys.\n\n## GraphQL API Spec\n\nIf you need detailed queries, mutations, fields, and inputs, look at the [GraphQL Spec](https://graphql-spec.runpod.io/).","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":1,"to":17}}}}],["335",{"pageContent":"Create Pods\n\nA Pod consists of the following resources:\n\n- 0 or more GPUs - A pod can be started with 0 GPUs for the purposes of accessing data, though GPU-accelerated functions and web services will fail to work.\n- vCPU\n- System RAM\n- Container Disk\n  - It's temporary and removed when the pod is stopped or terminated.\n  - You only pay for the container disk when the pod is running.\n- Instance Volume\n  - Data persists even when you reset or stop a Pod. Volume is removed when the Pod is terminated.\n  - You pay for volume storage even when the Pod is stopped.","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":19,"to":31}}}}],["336",{"pageContent":"Create On-Demand Pod","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":33,"to":33}}}}],["337",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { podFindAndDeployOnDemand( input: { cloudType: ALL, gpuCount: 1, volumeInGb: 40, containerDiskInGb: 40, minVcpuCount: 2, minMemoryInGb: 15, gpuTypeId: \\\"NVIDIA RTX A6000\\\", name: \\\"RunPod Tensorflow\\\", imageName: \\\"runpod/tensorflow\\\", dockerArgs: \\\"\\\", ports: \\\"8888/http\\\", volumeMountPath: \\\"/workspace\\\", env: [{ key: \\\"JUPYTER_PASSWORD\\\", value: \\\"rn51hunbpgtltcpac3ol\\\" }] } ) { id imageName env machineId machine { podHostId } } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  podFindAndDeployOnDemand(\n    input: {\n      cloudType: ALL\n      gpuCount: 1\n      volumeInGb: 40\n      containerDiskInGb: 40\n      minVcpuCount: 2\n      minMemoryInGb: 15\n      gpuTypeId: \"NVIDIA RTX A6000\"\n      name: \"RunPod Tensorflow\"","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":35,"to":56}}}}],["338",{"pageContent":"imageName: \"runpod/tensorflow\"\n      dockerArgs: \"\"\n      ports: \"8888/http\"\n      volumeMountPath: \"/workspace\"\n      env: [{ key: \"JUPYTER_PASSWORD\", value: \"rn51hunbpgtltcpac3ol\" }]\n    }\n  ) {\n    id\n    imageName\n    env\n    machineId\n    machine {\n      podHostId\n    }\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"ouput\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"podFindAndDeployOnDemand\": {\n      \"id\": \"50qynxzilsxoey\",\n      \"imageName\": \"runpod/tensorflow\",\n      \"env\": [\n        \"JUPYTER_PASSWORD=rn51hunbpgtltcpac3ol\"\n      ],\n      \"machineId\": \"hpvdausak8xb\",\n      \"machine\": {\n        \"podHostId\": \"50qynxzilsxoey-64410065\"\n      }\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":57,"to":94}}}}],["339",{"pageContent":"Create Spot Pod","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":96,"to":96}}}}],["340",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { podRentInterruptable( input: { bidPerGpu: 0.2, cloudType: SECURE, gpuCount: 1, volumeInGb: 40, containerDiskInGb: 40, minVcpuCount: 2, minMemoryInGb: 15, gpuTypeId: \\\"NVIDIA RTX A6000\\\", name: \\\"RunPod Pytorch\\\", imageName: \\\"runpod/pytorch\\\", dockerArgs: \\\"\\\", ports: \\\"8888/http\\\", volumeMountPath: \\\"/workspace\\\", env: [{ key: \\\"JUPYTER_PASSWORD\\\", value: \\\"vunw9ybnzqwpia2795p2\\\" }] } ) { id imageName env machineId machine { podHostId } } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  podRentInterruptable(\n    input: {\n      bidPerGpu: 0.2\n      cloudType: SECURE\n      gpuCount: 1\n      volumeInGb: 40\n      containerDiskInGb: 40\n      minVcpuCount: 2\n      minMemoryInGb: 15\n      gpuTypeId: \"NVIDIA RTX A6000\"","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":98,"to":119}}}}],["341",{"pageContent":"name: \"RunPod Pytorch\"\n      imageName: \"runpod/pytorch\"\n      dockerArgs: \"\"\n      ports: \"8888/http\"\n      volumeMountPath: \"/workspace\"\n      env: [{ key: \"JUPYTER_PASSWORD\", value: \"vunw9ybnzqwpia2795p2\" }]\n    }\n  ) {\n    id\n    imageName\n    env\n    machineId\n    machine {\n      podHostId\n    }\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"ouput\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"podRentInterruptable\": {\n      \"id\": \"fkjbybgpwuvmhk\",\n      \"imageName\": \"runpod/pytorch\",\n      \"env\": [\n        \"JUPYTER_PASSWORD=vunw9ybnzqwpia2795p2\"\n      ],\n      \"machineId\": \"hpvdausak8xb\",\n      \"machine\": {\n        \"podHostId\": \"fkjbybgpwuvmhk-64410065\"\n      }\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":120,"to":158}}}}],["342",{"pageContent":"Start Pods","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":160,"to":160}}}}],["343",{"pageContent":"Start On-Demand Pod","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":162,"to":162}}}}],["344",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { podResume( input: { podId: \\\"inzk6tzuz833h5\\\", gpuCount: 1 } ) { id desiredStatus imageName env machineId machine { podHostId } } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  podResume(input: {podId: \"inzk6tzuz833h5\", gpuCount: 1}) {\n    id\n    desiredStatus\n    imageName\n    env\n    machineId\n    machine {\n      podHostId\n    }\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"ouput\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"podResume\": {\n      \"id\": \"inzk6tzuz833h5\",\n      \"desiredStatus\": \"RUNNING\",\n      \"imageName\": \"runpod/tensorflow\",\n      \"env\": [\n        \"JUPYTER_PASSWORD=ywm4c9r15j1x6gfrds5n\"\n      ],\n      \"machineId\": \"hpvdausak8xb\",\n      \"machine\": {\n        \"podHostId\": \"inzk6tzuz833h5-64410065\"\n      }\n    }","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":164,"to":204}}}}],["345",{"pageContent":"}\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":205,"to":209}}}}],["346",{"pageContent":"Start Spot Pod","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":211,"to":211}}}}],["347",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { podBidResume( input: { podId: \\\"d62t7qg9n5vtan\\\", bidPerGpu: 0.2, gpuCount: 1 } ) { id desiredStatus imageName env machineId machine { podHostId } } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  podBidResume(input: {podId: \"d62t7qg9n5vtan\", bidPerGpu: 0.2, gpuCount: 1}) {\n    id\n    desiredStatus\n    imageName\n    env\n    machineId\n    machine {\n      podHostId\n    }\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"ouput\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"podBidResume\": {\n      \"id\": \"d62t7qg9n5vtan\",\n      \"desiredStatus\": \"RUNNING\",\n      \"imageName\": \"runpod/tensorflow\",\n      \"env\": [\n        \"JUPYTER_PASSWORD=vunw9ybnzqwpia2795p2\"\n      ],\n      \"machineId\": \"hpvdausak8xb\",\n      \"machine\": {","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":213,"to":250}}}}],["348",{"pageContent":"\"podHostId\": \"d62t7qg9n5vtan-64410065\"\n      }\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":251,"to":258}}}}],["349",{"pageContent":"Stop Pods\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"mutation { podStop(input: {podId: \\\"riixlu8oclhp\\\"}) { id desiredStatus } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nmutation {\n  podStop(input: {podId: \"riixlu8oclhp\"}) {\n    id\n    desiredStatus\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"ouput\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"podStop\": {\n      \"id\": \"riixlu8oclhp\",\n      \"desiredStatus\": \"EXITED\"\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":260,"to":293}}}}],["350",{"pageContent":"List Pods","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":295,"to":295}}}}],["351",{"pageContent":"List all Pods","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":297,"to":297}}}}],["352",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"query Pods { myself { pods { id name runtime { uptimeInSeconds ports { ip isIpPublic privatePort publicPort type } gpus { id gpuUtilPercent memoryUtilPercent } container { cpuPercent memoryPercent } } } } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nquery Pods {\n  myself {\n    pods {\n      id\n      name\n      runtime {\n        uptimeInSeconds\n        ports {\n          ip\n          isIpPublic\n          privatePort\n          publicPort\n          type\n        }\n        gpus {\n          id\n          gpuUtilPercent\n          memoryUtilPercent\n        }\n        container {\n          cpuPercent\n          memoryPercent\n        }\n      }\n    }\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"ouput\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"myself\": {","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":299,"to":343}}}}],["353",{"pageContent":"\"pods\": [\n        {\n          \"id\": \"ldl1dxirsim64n\",\n          \"name\": \"RunPod Pytorch\",\n          \"runtime\": {\n            \"uptimeInSeconds\": 3931,\n            \"ports\": [\n              {\n                \"ip\": \"100.65.0.101\",\n                \"isIpPublic\": false,\n                \"privatePort\": 8888,\n                \"publicPort\": 60141,\n                \"type\": \"http\"\n              }\n            ],\n            \"gpus\": [\n              {\n                \"id\": \"GPU-e0488b7e-6932-795b-a125-4472c16ea72c\",\n                \"gpuUtilPercent\": 0,\n                \"memoryUtilPercent\": 0\n              }\n            ],\n            \"container\": {\n              \"cpuPercent\": 0,\n              \"memoryPercent\": 0\n            }\n          }\n        },\n        {\n          \"id\": \"hpvdausak8xb00\",\n          \"name\": \"hpvdausak8xb-BG\",\n          \"runtime\": {\n            \"uptimeInSeconds\": 858937,\n            \"ports\": null,\n            \"gpus\": [\n              {","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":344,"to":379}}}}],["354",{"pageContent":"\"id\": \"GPU-2630fe4d-a75d-a9ae-8c15-6866088dfae2\",\n                \"gpuUtilPercent\": 100,\n                \"memoryUtilPercent\": 100\n              }\n            ],\n            \"container\": {\n              \"cpuPercent\": 0,\n              \"memoryPercent\": 1\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":380,"to":397}}}}],["355",{"pageContent":"Get Pod by ID","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":399,"to":399}}}}],["356",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"query Pod { pod(input: {podId: \\\"ldl1dxirsim64n\\\"}) { id name runtime { uptimeInSeconds ports { ip isIpPublic privatePort publicPort type } gpus { id gpuUtilPercent memoryUtilPercent } container { cpuPercent memoryPercent } } } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nquery Pod {\n  pod(input: {podId: \"ldl1dxirsim64n\"}) {\n    id\n    name\n    runtime {\n      uptimeInSeconds\n      ports {\n        ip\n        isIpPublic\n        privatePort\n        publicPort\n        type\n      }\n      gpus {\n        id\n        gpuUtilPercent\n        memoryUtilPercent\n      }\n      container {\n        cpuPercent\n        memoryPercent\n      }\n    }\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"ouput\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"pod\": {","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":401,"to":443}}}}],["357",{"pageContent":"\"id\": \"ldl1dxirsim64n\",\n      \"name\": \"RunPod Pytorch\",\n      \"runtime\": {\n        \"uptimeInSeconds\": 11,\n        \"ports\": [\n          {\n            \"ip\": \"100.65.0.101\",\n            \"isIpPublic\": false,\n            \"privatePort\": 8888,\n            \"publicPort\": 60141,\n            \"type\": \"http\"\n          }\n        ],\n        \"gpus\": [\n          {\n            \"id\": \"GPU-e0488b7e-6932-795b-a125-4472c16ea72c\",\n            \"gpuUtilPercent\": 0,\n            \"memoryUtilPercent\": 0\n          }\n        ],\n        \"container\": {\n          \"cpuPercent\": 0,\n          \"memoryPercent\": 0\n        }\n      }\n    }\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":444,"to":474}}}}],["358",{"pageContent":"List GPU types\n\nWhen creating a Pod, you will need to pass GPU type IDs.\nThese queries can help find all GPU types, their IDs, and other attributes like VRAM.","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":476,"to":479}}}}],["359",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"query GpuTypes { gpuTypes { id displayName memoryInGb } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nquery GpuTypes {\n  gpuTypes {\n    id\n    displayName\n    memoryInGb\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"ouput\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"gpuTypes\": [\n      {\n        \"id\": \"NVIDIA GeForce RTX 3070\",\n        \"displayName\": \"RTX 3070\",\n        \"memoryInGb\": 8\n      },\n      {\n        \"id\": \"NVIDIA GeForce RTX 3080\",\n        \"displayName\": \"RTX 3080\",\n        \"memoryInGb\": 10\n      },\n      {\n        \"id\": \"NVIDIA RTX A6000\",\n        \"displayName\": \"RTX A6000\",\n        \"memoryInGb\": 48\n      }\n    ]\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":481,"to":526}}}}],["360",{"pageContent":"Get GPU Type by ID","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":528,"to":528}}}}],["361",{"pageContent":"<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n```curl\ncurl --request POST \\\n  --header 'content-type: application/json' \\\n  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \\\n  --data '{\"query\": \"query GpuTypes { gpuTypes(input: {id: \\\"NVIDIA GeForce RTX 3090\\\"}) { id displayName memoryInGb secureCloud communityCloud lowestPrice(input: {gpuCount: 1}) { minimumBidPrice uninterruptablePrice } } }\"}'\n```\n  </TabItem>\n  <TabItem value=\"graphql\" label=\"GraphQL\">\n```graphql\nquery GpuTypes {\n  gpuTypes(input: {id: \"NVIDIA GeForce RTX 3090\"}) {\n    id\n    displayName\n    memoryInGb\n    secureCloud\n    communityCloud\n    lowestPrice(input: {gpuCount: 1}) {\n      minimumBidPrice\n      uninterruptablePrice\n    }\n  }\n}\n```\n  </TabItem>\n  <TabItem value=\"ouput\" label=\"Output\">\n```json\n{\n  \"data\": {\n    \"gpuTypes\": [\n      {\n        \"id\": \"NVIDIA GeForce RTX 3090\",\n        \"displayName\": \"RTX 3090\",\n        \"memoryInGb\": 24,\n        \"secureCloud\": false,","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":530,"to":565}}}}],["362",{"pageContent":"\"communityCloud\": true,\n        \"lowestPrice\": {\n          \"minimumBidPrice\": 0.163,\n          \"uninterruptablePrice\": 0.3\n        }\n      }\n    ]\n  }\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/graphql/manage-pods.md","loc":{"lines":{"from":566,"to":577}}}}],["363",{"pageContent":"---\ntitle: Endpoints\ndescription: \"Learn how to interact with RunPod's endpoints using the JavaScript SDK, including synchronous and asynchronous execution methods, status checks, and job cancellation. Discover how to set timeouts, execute policies, and purge queues.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nInteracting with RunPod's endpoints is a core feature of the SDK, enabling the execution of tasks and the retrieval of results.\nThis section covers the synchronous and asynchronous execution methods, along with checking the status of operations.\n\n## Prerequisites\n\nBefore using the RunPod JavaScript, ensure that you have:\n\n- Installed the RunPod JavaScript SDK.\n- Configured your API key.","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":1,"to":17}}}}],["364",{"pageContent":"Set your Endpoint Id\n\nSet your RunPod API key and your Endpoint Id as environment variables.\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nconst runpod = runpodSdk(RUNPOD_API_KEY);\nconst endpoint = runpod.endpoint(ENDPOINT_ID);\n```\n\nThis allows all calls to pass through your Endpoint Id with a valid API key.\n\nIn most situations, you'll set a variable name `endpoint` on the `Endpoint` class.\nThis allows you to use the following methods or instances variables from the `Endpoint` class:\n\n- [health](#health-check)\n- [purge_queue](#purge-queue)\n- [runSync](#run-synchronously)\n- [run](#run-asynchronously)","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":19,"to":39}}}}],["365",{"pageContent":"Run the Endpoint\n\nRun the Endpoint with the either the asynchronous `run` or synchronous `runSync` method.\n\nChoosing between asynchronous and synchronous execution hinges on your task's needs and application design.","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":41,"to":45}}}}],["366",{"pageContent":"Run synchronously\n\nTo execute an endpoint synchronously and wait for the result, use the `runSync` method on your endpoint.\nThis method blocks the execution until the endpoint run is complete or until it times out.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nconst runpod = runpodSdk(RUNPOD_API_KEY);\nconst endpoint = runpod.endpoint(ENDPOINT_ID);\nconst result = await endpoint.runSync({\n  \"input\": {\n    \"prompt\": \"Hello, World!\",\n  },\n});\n\nconsole.log(result);\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  delayTime: 18,\n  executionTime: 36595,\n  id: 'sync-d050a3f6-791a-4aff-857a-66c759db4a06-u1',\n  output: [ { choices: [Array], usage: [Object] } ],\n  status: 'COMPLETED',\n  started: true,\n  completed: true,\n  succeeded: true\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":47,"to":87}}}}],["367",{"pageContent":"Run asynchronously\n\nAsynchronous execution allows for non-blocking operations, enabling your code to perform other tasks while waiting for an operation to complete.\n\nFor non-blocking operations, use the `run` method on the endpoint.\nThis method allows you to start an endpoint run and then check its status or wait for its completion at a later time.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nconst runpod = runpodSdk(RUNPOD_API_KEY);\nconst endpoint = runpod.endpoint(ENDPOINT_ID);\nconst result = await endpoint.run({\n  \"input\": {\n    \"prompt\": \"Hello, World!\",\n  },\n});\n\nconsole.log(result);\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"d4e960f6-073f-4219-af24-cbae6b532c31-u1\",\n  \"status\": \"IN_QUEUE\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":89,"to":125}}}}],["368",{"pageContent":"Get results from an asynchronous run\n\nThe following example shows how to get the results of an asynchronous run.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n\nasync function main() {\n  const runpod = runpodSdk(RUNPOD_API_KEY);\n  const endpoint = runpod.endpoint(ENDPOINT_ID);\n  const result = await endpoint.run({\n    input: {\n      prompt: \"Hello, World!\",\n    },\n  });\n\n  console.log(result);\n  console.log(\"run response\");\n  console.log(result);\n\n  const { id } = result; // Extracting the operation ID from the initial run response","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":127,"to":153}}}}],["369",{"pageContent":"// Check the status in a loop, similar to the working example\n  for (let i = 0; i < 20; i++) {\n    // Increase or decrease the loop count as necessary\n    const statusResult = await endpoint.status(id);\n    console.log(\"status response\");\n    console.log(statusResult);\n\n    if (\n      statusResult.status === \"COMPLETED\"\n      || statusResult.status === \"FAILED\"\n    ) {\n      // Once completed or failed, log the final status and break the loop\n      if (statusResult.status === \"COMPLETED\") {\n        console.log(\"Operation completed successfully.\");\n        console.log(statusResult.output);\n      } else {\n        console.log(\"Operation failed.\");\n        console.log(statusResult);\n      }\n      break;\n    }\n\n    // Wait for a bit before checking the status again\n    await sleep(5000);\n  }\n}\n\nmain();","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":155,"to":182}}}}],["370",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\nrun response\n{ id: 'c671a352-78e6-4eba-b2c8-2ea537c00897-u1', status: 'IN_QUEUE' }\nstatus response\n{\n  delayTime: 19,\n  id: 'c671a352-78e6-4eba-b2c8-2ea537c00897-u1',\n  status: 'IN_PROGRESS',\n  started: true,\n  completed: false,\n  succeeded: false\n}\nstatus response\n{\n  delayTime: 19,\n  executionTime: 539,\n  id: 'c671a352-78e6-4eba-b2c8-2ea537c00897-u1',\n  output: [ { choices: [Array], usage: [Object] } ],\n  status: 'COMPLETED',\n  started: true,\n  completed: true,\n  succeeded: true\n}\nOperation completed successfully.\n[ { choices: [ [Object] ], usage: { input: 5, output: 16 } } ]\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":185,"to":216}}}}],["371",{"pageContent":"Poll the status of an asynchronous run\n\nUses `await endpoint.status(id)` to check the status of the operation repeatedly until it either completes or fails.\nAfter each check, the function waits for 5 seconds (or any other suitable duration you choose) before checking the status again, using the sleep function.\nThis approach ensures your application remains responsive and doesn't overwhelm the Runpod endpoint with status requests.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\n// Function to pause execution for a specified time\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n\nasync function main() {\n  try {\n    const runpod = runpodSdk(RUNPOD_API_KEY);\n    const endpoint = runpod.endpoint(ENDPOINT_ID);\n    const result = await endpoint.run({\n      input: {\n        prompt: \"Hello, World!\",\n      },\n    });","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":218,"to":242}}}}],["372",{"pageContent":"const { id } = result;\n    if (!id) {\n      console.error(\"No ID returned from endpoint.run\");\n      return;\n    }\n\n    // Poll the status of the operation until it completes or fails\n    let isComplete = false;\n    while (!isComplete) {\n      const status = await endpoint.status(id);\n      console.log(`Current status: ${status.status}`);\n\n      if (status.status === \"COMPLETED\" || status.status === \"FAILED\") {\n        isComplete = true; // Exit the loop\n        console.log(`Operation ${status.status.toLowerCase()}.`);\n\n        if (status.status === \"COMPLETED\") {\n          console.log(\"Output:\", status.output);\n        } else {\n          console.error(\"Error details:\", status.error);\n        }\n      } else {\n        await sleep(5000); // Adjust the delay as needed\n      }\n    }\n  } catch (error) {\n    console.error(\"An error occurred:\", error);\n  }\n}\n\nmain();","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":244,"to":274}}}}],["373",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```text\nCurrent status: IN_QUEUE\nCurrent status: IN_PROGRESS\nCurrent status: COMPLETED\nOperation completed.\nHello, World!\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":277,"to":289}}}}],["374",{"pageContent":"Stream\n\nStream allows you to stream the output of an Endpoint run.\nTo enable streaming, your handler must support the `\"return_aggregate_stream\": True` option on the `start` method of your Handler.\nOnce enabled, use the `stream` method to receive data as it becomes available.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nasync function main() {\n  const runpod = runpodSdk(RUNPOD_API_KEY);\n  const endpoint = runpod.endpoint(ENDPOINT_ID);\n  const result = await endpoint.run({\n    input: {\n      prompt: \"Hello, World!\",\n    },\n  });\n\n  console.log(result);\n\n  const { id } = result;\n  for await (const result of endpoint.stream(id)) {\n    console.log(`${JSON.stringify(result, null, 2)}`);\n  }\n  console.log(\"done streaming\");\n}\n\nmain();","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":291,"to":322}}}}],["375",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n```json\n{ id: 'cb68890e-436f-4234-955d-001db6afe972-u1', status: 'IN_QUEUE' }\n{\n  \"output\": \"H\"\n}\n{\n  \"output\": \"e\"\n}\n{\n  \"output\": \"l\"\n}\n{\n  \"output\": \"l\"\n}\n{\n  \"output\": \"o\"\n}\n{\n  \"output\": \",\"\n}\n{\n  \"output\": \" \"\n}\n{\n  \"output\": \"W\"\n}\n{\n  \"output\": \"o\"\n}\n{\n  \"output\": \"r\"\n}\n{\n  \"output\": \"l\"\n}\n{\n  \"output\": \"d\"\n}\n{\n  \"output\": \"!\"\n}\ndone streaming\n```\n\n</TabItem>\n\n<TabItem value=\"handler\" label=\"Handler\" default>\n\nYou must define your handler to support the `\"return_aggregate_stream\": True` option on the `start` method.\n\n```python\nfrom time import sleep\nimport runpod\n\n\ndef handler(job):\n    job_input = job[\"input\"][\"prompt\"]\n\n    for i in job_input:\n        sleep(1)  # sleep for 1 second for effect\n        yield i\n\n\nrunpod.serverless.start(\n    {\n        \"handler\": handler,\n        \"return_aggregate_stream\": True,  # Ensures aggregated results are streamed back\n    }\n)\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":325,"to":399}}}}],["376",{"pageContent":"Health check\n\nMonitor the health of an endpoint by checking its status, including jobs completed, failed, in progress, in queue, and retried, as well as the status of workers.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nconst runpod = runpodSdk(RUNPOD_API_KEY);\nconst endpoint = runpod.endpoint(ENDPOINT_ID);\n\nconst health = await endpoint.health();\nconsole.log(health);\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"jobs\": {\n    \"completed\": 72,\n    \"failed\": 1,\n    \"inProgress\": 6,\n    \"inQueue\": 0,\n    \"retried\": 1\n  },\n  \"workers\": {\n    \"idle\": 4,\n    \"initializing\": 0,\n    \"ready\": 4,\n    \"running\": 1,\n    \"throttled\": 0\n  }\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":401,"to":442}}}}],["377",{"pageContent":"Status\n\nUse the `status` method and specify the `id` of the run to get the status of a run.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nasync function main() {\n  try {\n    const runpod = runpodSdk(RUNPOD_API_KEY);\n    const endpoint = runpod.endpoint(ENDPOINT_ID);\n    const result = await endpoint.run({\n      input: {\n        prompt: \"Hello, World!\",\n      },\n    });\n\n    const { id } = result;\n    if (!id) {\n      console.error(\"No ID returned from endpoint.run\");\n      return;\n    }\n\n    const status = await endpoint.status(id);\n    console.log(status);\n  } catch (error) {\n    console.error(\"An error occurred:\", error);\n  }\n}\n\nmain();","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":444,"to":478}}}}],["378",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"delayTime\": 18,\n  \"id\": \"792b1497-b2c8-4c95-90bf-4e2a6a2a37ff-u1\",\n  \"status\": \"IN_PROGRESS\",\n  \"started\": true,\n  \"completed\": false,\n  \"succeeded\": false\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":481,"to":496}}}}],["379",{"pageContent":"Cancel\n\nYou can cancel a Job request by using the `cancel()` function on the run request.\nYou might want to cancel a Job because it's stuck with a status of `IN_QUEUE` or `IN_PROGRESS`, or because you no longer need the result.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nasync function main() {\n  try {\n    const runpod = runpodSdk(RUNPOD_API_KEY);\n    const endpoint = runpod.endpoint(ENDPOINT_ID);\n    const result = await endpoint.run({\n      input: {\n        prompt: \"Hello, World!\",\n      },\n    });\n\n    const { id } = result;\n    if (!id) {\n      console.error(\"No ID returned from endpoint.run\");\n      return;\n    }\n\n    const cancel = await endpoint.cancel(id);\n    console.log(cancel);\n  } catch (error) {\n    console.error(\"An error occurred:\", error);\n  }\n}\n\nmain();","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":498,"to":533}}}}],["380",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"5fb6a8db-a8fa-41a1-ad81-f5fad9755f9e-u1\",\n  \"status\": \"CANCELLED\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":536,"to":547}}}}],["381",{"pageContent":"Timeout\n\nTo set a timeout on a run, pass a timeout value to the `run` method.\nTime is measured in milliseconds.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nconst runpod = runpodSdk(RUNPOD_API_KEY);\nconst endpoint = runpod.endpoint(ENDPOINT_ID);\nconst result = await endpoint.run({\n  \"input\": {\n    \"prompt\": \"Hello, World!\",\n  },\n}, 5000);\n\nconsole.log(result);\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"43309f93-0422-4eac-92cf-e385dee36e99-u1\",\n  \"status\": \"IN_QUEUE\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":549,"to":583}}}}],["382",{"pageContent":"Execution policy\n\nYou can set the maximum time to wait for a response from the endpoint in the `policy` parameter.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nconst runpod = runpodSdk(RUNPOD_API_KEY);\nconst endpoint = runpod.endpoint(ENDPOINT_ID);\nconst result = await endpoint.run({\n  \"input\": {\n    \"prompt\": \"Hello, World!\",\n  },\n  policy: {\n    executionTimeout: 5000,\n  },\n});\n\nconsole.log(result);\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"21bd3763-dcbf-4091-84ee-85b80907a020-u1\",\n  \"status\": \"IN_QUEUE\"\n}\n```\n\n</TabItem>\n</Tabs>\n\nFor more information, see [Execution policy](/serverless/endpoints/job-operations).","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":585,"to":623}}}}],["383",{"pageContent":"Purge queue\n\nYou can purge all jobs from a queue by using the `purgeQueue()` function.\n\n`purgeQueue()` doesn't affect Jobs in progress.\n\n<Tabs>\n  <TabItem value=\"javascript\" label=\"JavaScript\" default>\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nasync function main() {\n  try {\n    const runpod = runpodSdk(RUNPOD_API_KEY);\n    const endpoint = runpod.endpoint(ENDPOINT_ID);\n    await endpoint.run({\n      input: {\n        prompt: \"Hello, World!\",\n      },\n    });\n\n    const purgeQueue = await endpoint.purgeQueue();\n    console.log(purgeQueue);\n  } catch (error) {\n    console.error(\"An error occurred:\", error);\n  }\n}\n\nmain();\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"removed\": 1,\n  \"status\": \"completed\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/javascript/endpoints.md","loc":{"lines":{"from":625,"to":669}}}}],["384",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ndescription: \"Get started with RunPod JavaScript SDK, a tool for building web apps, server-side implementations, and automating tasks. Learn how to install, integrate, and secure your API key for seamless development.\"\n---\n\nGet started with setting up your RunPod projects using JavaScript. Whether you're building web applications, server-side implementations, or automating tasks, the RunPod JavaScript SDK provides the tools you need.\nThis guide outlines the steps to get your development environment ready and integrate RunPod into your JavaScript projects.","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":1,"to":8}}}}],["385",{"pageContent":"Install the RunPod SDK\n\nBefore integrating RunPod into your project, you'll need to install the SDK.\nUsing Node.js and npm (Node Package Manager) simplifies this process.\nEnsure you have Node.js and npm installed on your system before proceeding.\n\nTo install the RunPod SDK, run the following npm command in your project directory.\n\n```command\nnpm install --save runpod-sdk\n# or\nyarn add runpod-sdk\n```\n\nThis command installs the `runpod-sdk` package and adds it to your project's `package.json` dependencies.\nFor more details about the package, visit the [npm package page](https://www.npmjs.com/package/runpod-sdk) or the [GitHub repository](https://github.com/runpod/js-sdk).","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":10,"to":25}}}}],["386",{"pageContent":"Add your API key\n\nTo use the RunPod SDK in your project, you first need to import it and configure it with your API key and endpoint ID. Ensure these values are securely stored, preferably as environment variables.\n\nBelow is a basic example of how to initialize and use the RunPod SDK in your JavaScript project.\n\n```javascript\nconst { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;\nimport runpodSdk from \"runpod-sdk\";\n\nconst runpod = runpodSdk(RUNPOD_API_KEY);\nconst endpoint = runpod.endpoint(ENDPOINT_ID);\n```\n\nThis snippet demonstrates how to import the SDK, initialize it with your API key, and reference a specific endpoint using its ID.\nRemember, the RunPod SDK uses the ES Module (ESM) system and supports asynchronous operations, making it compatible with modern JavaScript development practices.","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":27,"to":42}}}}],["387",{"pageContent":"Secure your API key\n\nWhen working with the RunPod SDK, it's essential to secure your API key.\nStoring the API key in environment variables is recommended, as shown in the initialization example. This method keeps your key out of your source code and reduces the risk of accidental exposure.\n\n:::note\n\nUse environment variables or secure secrets management solutions to handle sensitive information like API keys.\n\n:::\n\nFor more information, see the following:\n\n- [RunPod SDK npm Package](https://www.npmjs.com/package/runpod-sdk)\n- [RunPod GitHub Repository](https://github.com/runpod/js-sdk)\n- [Endpoints](/sdks/javascript/endpoints)","metadata":{"source":"/runpod-docs/docs/sdks/javascript/overview.md","loc":{"lines":{"from":44,"to":59}}}}],["388",{"pageContent":"---\ntitle: Overview\ndescription: \"Unlock serverless functionality with RunPod SDKs, enabling developers to create custom logic, simplify deployments, and programatically manage infrastructure, including Pods, Templates, and Endpoints.\"\nsidebar_position: 1\n---\n\nRunPod SDKs provide developers with tools to use the RunPod API for creating serverless functions and managing infrastructure.\nThey enable custom logic integration, simplify deployments, and allow for programmatic infrastructure management.\n\n## Interacting with Serverless Endpoints\n\nOnce deployed, serverless functions is exposed as an Endpoints, you can allow external applications to interact with them through HTTP requests.\n\n#### Interact with Serverless Endpoints:\n\nYour Serverless Endpoints works similarlly to an HTTP request.\nYou will need to provide an Endpoint Id and a reference to your API key to complete requests.","metadata":{"source":"/runpod-docs/docs/sdks/overview.md","loc":{"lines":{"from":1,"to":17}}}}],["389",{"pageContent":"Infrastructure management\n\nThe RunPod SDK facilitates the programmatic creation, configuration, and management of various infrastructure components, including Pods, Templates, and Endpoints.\n\n### Managing Pods\n\nPods are the fundamental building blocks in RunPod, representing isolated environments for running applications.\n\n#### Manage Pods:\n\n1. **Create a Pod**: Use the SDK to instantiate a new Pod with the desired configuration.\n2. **Configure the Pod**: Adjust settings such as GPU, memory allocation, and network access according to your needs.\n3. **Deploy Applications**: Deploy your applications or services within the Pod.\n4. **Monitor and scale**: Utilize the SDK to monitor Pod performance and scale resources as required.","metadata":{"source":"/runpod-docs/docs/sdks/overview.md","loc":{"lines":{"from":19,"to":32}}}}],["390",{"pageContent":"Manage Templates and Endpoints\n\nTemplates define the base environment for Pods, while Endpoints enable external access to services running within Pods.\n\n#### Use Templates and Endpoints:\n\n1. **Create a Template**: Define a Template that specifies the base configuration for Pods.\n2. **Instantiate Pods from Templates**: Use the Template to create Pods with a consistent environment.\n3. **Expose Services via Endpoints**: Configure Endpoints to allow external access to applications running in Pods.","metadata":{"source":"/runpod-docs/docs/sdks/overview.md","loc":{"lines":{"from":34,"to":42}}}}],["391",{"pageContent":"---\ntitle: API Wrapper\nid: apis\nsidebar_label: APIs\ndescription: \"Learn how to manage computational resources with the RunPod API, including endpoint configurations, template creation, and GPU management, to optimize your project's performance.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nThis document outlines the core functionalities provided by the RunPod API, including how to interact with Endpoints, manage Templates, and list available GPUs.\nThese operations let you dynamically manage computational resources within the RunPod environment.","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":1,"to":12}}}}],["392",{"pageContent":"Get Endpoints\n\nTo retrieve a comprehensive list of all available endpoint configurations within RunPod, you can use the `get_endpoints()` function.\nThis function returns a list of endpoint configurations, allowing you to understand what's available for use in your projects.\n\n```python\nimport runpod\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\n# Fetching all available endpoints\nendpoints = runpod.get_endpoints()\n\n# Displaying the list of endpoints\nprint(endpoints)\n```","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":14,"to":30}}}}],["393",{"pageContent":"Create Template\n\nTemplates in RunPod serve as predefined configurations for setting up environments efficiently.\nThe `create_template()` function facilitates the creation of new templates by specifying a name and a Docker image.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nimport runpod\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\ntry:\n    # Creating a new template with a specified name and Docker image\n    new_template = runpod.create_template(name=\"test\", image_name=\"runpod/base:0.1.0\")\n\n    # Output the created template details\n    print(new_template)\n\nexcept runpod.error.QueryError as err:\n    # Handling potential errors during template creation\n    print(err)\n    print(err.query)","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":32,"to":56}}}}],["394",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"n6m0htekvq\",\n  \"name\": \"test\",\n  \"imageName\": \"runpod/base:0.1.0\",\n  \"dockerArgs\": \"\",\n  \"containerDiskInGb\": 10,\n  \"volumeInGb\": 0,\n  \"volumeMountPath\": \"/workspace\",\n  \"ports\": \"\",\n  \"env\": [],\n  \"isServerless\": false\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":59,"to":78}}}}],["395",{"pageContent":"Create Endpoint\n\nCreating a new endpoint with the `create_endpoint()` function.\nThis function requires you to specify a `name` and a `template_id`.\nAdditional configurations such as GPUs, number of Workers, and more can also be specified depending your requirements.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nimport runpod\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\ntry:\n    # Creating a template to use with the new endpoint\n    new_template = runpod.create_template(\n        name=\"test\", image_name=\"runpod/base:0.4.4\", is_serverless=True\n    )\n\n    # Output the created template details\n    print(new_template)\n\n    # Creating a new endpoint using the previously created template\n    new_endpoint = runpod.create_endpoint(\n        name=\"test\",\n        template_id=new_template[\"id\"],\n        gpu_ids=\"AMPERE_16\",\n        workers_min=0,\n        workers_max=1,\n    )\n\n    # Output the created endpoint details\n    print(new_endpoint)","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":80,"to":114}}}}],["396",{"pageContent":"except runpod.error.QueryError as err:\n    # Handling potential errors during endpoint creation\n    print(err)\n    print(err.query)","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":116,"to":119}}}}],["397",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"Unique_Id\",\n  \"name\": \"YourTemplate\",\n  \"imageName\": \"runpod/base:0.4.4\",\n  \"dockerArgs\": \"\",\n  \"containerDiskInGb\": 10,\n  \"volumeInGb\": 0,\n  \"volumeMountPath\": \"/workspace\",\n  \"ports\": null,\n  \"env\": [],\n  \"isServerless\": true\n}\n{\n  \"id\": \"Unique_Id\",\n  \"name\": \"YourTemplate\",\n  \"templateId\": \"Unique_Id\",\n  \"gpuIds\": \"AMPERE_16\",\n  \"networkVolumeId\": null,\n  \"locations\": null,\n  \"idleTimeout\": 5,\n  \"scalerType\": \"QUEUE_DELAY\",\n  \"scalerValue\": 4,\n  \"workersMin\": 0,\n  \"workersMax\": 1\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":122,"to":154}}}}],["398",{"pageContent":"Get GPUs\n\nFor understanding the computational resources available, the `get_gpus()` function lists all GPUs that can be allocated to endpoints in RunPod. This enables optimal resource selection based on your computational needs.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nimport runpod\nimport json\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\n# Fetching all available GPUs\ngpus = runpod.get_gpus()\n\n# Displaying the GPUs in a formatted manner\nprint(json.dumps(gpus, indent=2))\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n[\n  {\n    \"id\": \"NVIDIA A100 80GB PCIe\",\n    \"displayName\": \"A100 80GB\",\n    \"memoryInGb\": 80\n  },\n  {\n    \"id\": \"NVIDIA A100-SXM4-80GB\",\n    \"displayName\": \"A100 SXM 80GB\",\n    \"memoryInGb\": 80\n  }\n  // Additional GPUs omitted for brevity\n]\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":156,"to":197}}}}],["399",{"pageContent":"Get GPU by Id\n\nUse `get_gpu()` and pass in a GPU Id to retrieve details about a specific GPU model by its ID.\nThis is useful when understanding the capabilities and costs associated with various GPU models.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nimport runpod\nimport json\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\ngpus = runpod.get_gpu(\"NVIDIA A100 80GB PCIe\")\n\nprint(json.dumps(gpus, indent=2))\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"maxGpuCount\": 8,\n  \"id\": \"NVIDIA A100 80GB PCIe\",\n  \"displayName\": \"A100 80GB\",\n  \"manufacturer\": \"Nvidia\",\n  \"memoryInGb\": 80,\n  \"cudaCores\": 0,\n  \"secureCloud\": true,\n  \"communityCloud\": true,\n  \"securePrice\": 1.89,\n  \"communityPrice\": 1.59,\n  \"oneMonthPrice\": null,\n  \"threeMonthPrice\": null,\n  \"oneWeekPrice\": null,\n  \"communitySpotPrice\": 0.89,\n  \"secureSpotPrice\": null,\n  \"lowestPrice\": {\n    \"minimumBidPrice\": 0.89,\n    \"uninterruptablePrice\": 1.59\n  }\n}","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":199,"to":243}}}}],["400",{"pageContent":"</TabItem>\n\n</Tabs>\n\nThrough these functionalities, the RunPod API enables efficient and flexible management of computational resources, catering to a wide range of project requirements.","metadata":{"source":"/runpod-docs/docs/sdks/python/apis.md","loc":{"lines":{"from":246,"to":250}}}}],["401",{"pageContent":"---\ntitle: Endpoints\ndescription: \"Learn how to use the RunPod Python SDK to interact with various endpoints, perform synchronous and asynchronous operations, stream data, and check endpoint health. Discover how to set your Endpoint Id, run jobs, and cancel or purge queues with this comprehensive guide.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nThis documentation provides detailed instructions on how to use the RunPod Python SDK to interact with various endpoints.\nYou can perform synchronous and asynchronous operations, stream data, and check the health status of endpoints.\n\n## Prerequisites\n\nBefore using the RunPod Python, ensure that you have:\n\n- Installed the RunPod Python SDK.\n- Configured your API key.","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":1,"to":17}}}}],["402",{"pageContent":"Set your Endpoint Id\n\nPass your Endpoint Id on the `Endpoint` class.\n\n```python\nimport runpod\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\nendpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n```\n\nThis allows all calls to pass through your Endpoint Id with a valid API key.\n\nIn most situations, you'll set a variable name `endpoint` on the `Endpoint` class.\nThis allows you to use the following methods or instances variables from the `Endpoint` class:\n\n- [health](#health-check)\n- [purge_queue](#purge-queue)\n- [run_sync](#run-synchronously)\n- [run](#run-asynchronously)","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":19,"to":40}}}}],["403",{"pageContent":"Run the Endpoint\n\nRun the Endpoint with the either the asynchronous `run` or synchronous `run_sync` method.\n\nChoosing between asynchronous and synchronous execution hinges on your task's needs and application design.\n\n- **Asynchronous methods**: Choose the asynchronous method for handling tasks efficiently, especially when immediate feedback isn't crucial.\n  They allow your application to stay responsive by running time-consuming operations in the background, ideal for:\n  - **Non-blocking calls**: Keep your application active while waiting on long processes.\n  - **Long-running operations**: Avoid timeouts on tasks over 30 seconds, letting your app's workflow continue smoothly.\n  - **Job tracking**: Get a Job Id to monitor task status, useful for complex or delayed-result operations.","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":42,"to":52}}}}],["404",{"pageContent":"- **Synchronous methods**: Choose the synchronous method for these when your application requires immediate results from operations.\n  They're best for:\n  - **Immediate results**: Necessary for operations where quick outcomes are essential to continue with your app's logic.\n  - **Short operations**: Ideal for tasks under 30 seconds to prevent application delays.\n  - **Simplicity and control**: Provides a straightforward execution process, with timeout settings for better operational control.","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":54,"to":58}}}}],["405",{"pageContent":"Run synchronously\n\nTo execute an endpoint synchronously and wait for the result, use the `run_sync` method.\nThis method blocks the execution until the endpoint run is complete or until it times out.\n\n```python\nimport runpod\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\nendpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n\ntry:\n    run_request = endpoint.run_sync(\n        {\n            \"input\": {\n                \"prompt\": \"Hello, world!\",\n            }\n        },\n        timeout=60,  # Timeout in seconds.\n    )\n\n    print(run_request)\nexcept TimeoutError:\n    print(\"Job timed out.\")\n```","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":60,"to":86}}}}],["406",{"pageContent":"Run asynchronously\n\nAsynchronous execution allows for non-blocking operations, enabling your code to perform other tasks while waiting for an operation to complete.\nRunPod supports both standard asynchronous execution and advanced asynchronous programming with Python's [asyncio](https://docs.python.org/3/library/asyncio.html) framework.\n\nDepending on your application's needs, you can choose the approach that best suits your scenario.\n\nFor non-blocking operations, use the `run` method.\nThis method allows you to start an endpoint run and then check its status or wait for its completion at a later time.","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":88,"to":96}}}}],["407",{"pageContent":"Asynchronous execution\n\nThis executes a standard Python environment without requiring an asynchronous event loop.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nimport runpod\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\ninput_payload = {\"input\": {\"prompt\": \"Hello, World!\"}}\n\ntry:\n    endpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n    run_request = endpoint.run(input_payload)\n\n    # Initial check without blocking, useful for quick tasks\n    status = run_request.status()\n    print(f\"Initial job status: {status}\")\n\n    if status != \"COMPLETED\":\n        # Polling with timeout for long-running tasks\n        output = run_request.output(timeout=60)\n    else:\n        output = run_request.output()\n    print(f\"Job output: {output}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":98,"to":128}}}}],["408",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```text\nInitial job status: IN_QUEUE\nJob output: {'input_tokens': 24, 'output_tokens': 16, 'text': [\"Hello! How may I assist you today?\\n\"]}\n```\n\n</TabItem>\n\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":131,"to":141}}}}],["409",{"pageContent":"Asynchronous execution with asyncio\n\nUse Python's `asyncio` library for handling concurrent Endpoint calls efficiently.\nThis method embraces Python's asyncio framework for asynchronous programming, requiring functions to be defined with async and called with await.\nThis approach is inherently non-blocking and is built to handle concurrency efficiently.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nimport asyncio\nimport aiohttp\nimport os\nimport runpod\nfrom runpod import AsyncioEndpoint, AsyncioJob\n\n# asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())  # For Windows users.\n\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        input_payload = {\"prompt\": \"Hello, World!\"}\n        endpoint = AsyncioEndpoint(\"YOUR_ENDPOINT_ID\", session)\n        job: AsyncioJob = await endpoint.run(input_payload)","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":143,"to":169}}}}],["410",{"pageContent":"# Polling job status\n        while True:\n            status = await job.status()\n            print(f\"Current job status: {status}\")\n            if status == \"COMPLETED\":\n                output = await job.output()\n                print(\"Job output:\", output)\n                break  # Exit the loop once the job is completed.\n            elif status in [\"FAILED\"]:\n                print(\"Job failed or encountered an error.\")\n\n                break\n            else:\n                print(\"Job in queue or processing. Waiting 3 seconds...\")\n                await asyncio.sleep(3)  # Wait for 3 seconds before polling again\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":171,"to":189}}}}],["411",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```text\nCurrent job status: IN_QUEUE\nJob in queue or processing. Waiting 3 seconds...\nCurrent job status: COMPLETED\nJob output: {'input_tokens': 24, 'output_tokens': 16, 'text': ['Hello! How may I assist you today?\\n']}\n```\n\n</TabItem>\n\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":192,"to":204}}}}],["412",{"pageContent":"Health check\n\nMonitor the health of an endpoint by checking its status, including jobs completed, failed, in progress, in queue, and retried, as well as the status of workers.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nimport runpod\nimport json\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\nendpoint = runpod.Endpoint(\"gwp4kx5yd3nur1\")\n\nendpoint_health = endpoint.health()\n\nprint(json.dumps(endpoint_health, indent=2))\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"jobs\": {\n    \"completed\": 100,\n    \"failed\": 0,\n    \"inProgress\": 0,\n    \"inQueue\": 0,\n    \"retried\": 0\n  },\n  \"workers\": {\n    \"idle\": 1,\n    \"initializing\": 0,\n    \"ready\": 1,\n    \"running\": 0,\n    \"throttled\": 0\n  }\n}\n```\n\n</TabItem>\n\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":206,"to":251}}}}],["413",{"pageContent":"Streaming\n\nTo enable streaming, your handler must support the `\"return_aggregate_stream\": True` option on the `start` method of your Handler.\nOnce enabled, use the `stream` method to receive data as it becomes available.\n\n<Tabs>\n  <TabItem value=\"endpoint\" label=\"Endpoint\">\n\n```python\nimport runpod\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\nendpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n\nrun_request = endpoint.run(\n    {\n        \"input\": {\n            \"prompt\": \"Hello, world!\",\n        }\n    }\n)\n\nfor output in run_request.stream():\n    print(output)\n```\n\n</TabItem>\n  <TabItem value=\"handler\" label=\"Handler\" default>\n\n```python\nfrom time import sleep\nimport runpod\n\n\ndef handler(job):\n    job_input = job[\"input\"][\"prompt\"]\n\n    for i in job_input:\n        sleep(1)  # sleep for 1 second for effect\n        yield i\n\n\nrunpod.serverless.start(\n    {\n        \"handler\": handler,\n        \"return_aggregate_stream\": True,  # Ensures aggregated results are streamed back\n    }\n)","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":253,"to":301}}}}],["414",{"pageContent":"</TabItem>\n\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":304,"to":306}}}}],["415",{"pageContent":"Status\n\nReturns the status of the Job request.\nSet the `status()` function on the run request to return the status of the Job.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nimport runpod\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\ninput_payload = {\"input\": {\"prompt\": \"Hello, World!\"}}\n\ntry:\n    endpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n    run_request = endpoint.run(input_payload)\n\n    # Initial check without blocking, useful for quick tasks\n    status = run_request.status()\n    print(f\"Initial job status: {status}\")\n\n    if status != \"COMPLETED\":\n        # Polling with timeout for long-running tasks\n        output = run_request.output(timeout=60)\n    else:\n        output = run_request.output()\n    print(f\"Job output: {output}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```text\nInitial job status: IN_QUEUE\nJob output: Hello, World!\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":308,"to":350}}}}],["416",{"pageContent":"Cancel\n\nYou can cancel a Job request by using the `cancel()` function on the run request.\nYou might want to cancel a Job because it's stuck with a status of `IN_QUEUE` or `IN_PROGRESS`, or because you no longer need the result.\n\nThe following pattern cancels a job given a human interaction, for example pressing `Ctrl+C` in the terminal.\n\nThis sends a `SIGINT` signal to the running Job by catching the `KeyboardInterrupt` exception.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nimport time\nimport runpod\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\ninput_payload = {\n    \"messages\": [{\"role\": \"user\", \"content\": f\"Hello, World\"}],\n    \"max_tokens\": 2048,\n    \"use_openai_format\": True,\n}\n\ntry:\n    endpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n    run_request = rp_endpoint.run(input_payload)\n\n    while True:\n        status = run_request.status()\n        print(f\"Current job status: {status}\")","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":352,"to":382}}}}],["417",{"pageContent":"if status == \"COMPLETED\":\n            output = run_request.output()\n            print(\"Job output:\", output)\n\n            generated_text = (\n                output.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\")\n            )\n            print(generated_text)\n            break\n        elif status in [\"FAILED\", \"ERROR\"]:\n            print(\"Job failed to complete successfully.\")\n            break\n        else:\n            time.sleep(10)\nexcept KeyboardInterrupt:  # Catch KeyboardInterrupt\n    print(\"KeyboardInterrupt detected. Canceling the job...\")\n    if run_request:  # Check if a job is active\n        run_request.cancel()\n    print(\"Job canceled.\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":384,"to":405}}}}],["418",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\nCurrent job status: IN_QUEUE\nCurrent job status: IN_PROGRESS\nKeyboardInterrupt detected. Canceling the job...\nJob canceled.\n```\n\n</TabItem>\n\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":408,"to":420}}}}],["419",{"pageContent":"Timeout\n\nUse the `cancel()` function and the `timeout` argument to cancel the Job after a specified time.\n\nIn the previous `cancel()` example, the Job is canceled due to an external condition.\nIn this example, you can cancel a running Job that has taken too long to complete.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```python\nfrom time import sleep\nimport runpod\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\ninput_payload = {\"input\": {\"prompt\": \"Hello, World!\"}}\n\nendpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n\n\n# Submit the job request\nrun_request = endpoint.run(input_payload)\n\n# Retrieve and print the initial job status\ninitial_status = run_request.status()\nprint(f\"Initial job status: {initial_status}\")\n\n# Attempt to cancel the job after a specified timeout period (in seconds)\n# Note: This demonstrates an immediate cancellation for demonstration purposes.\n# Typically, you'd set the timeout based on expected job completion time.\nrun_request.cancel(timeout=3)","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":422,"to":454}}}}],["420",{"pageContent":"# Wait for the timeout period to ensure the cancellation takes effect\nsleep(3)\nprint(\"Sleeping for 3 seconds to allow for job cancellation...\")\n\n# Check and print the job status after the sleep period\nfinal_status = run_request.status()\nprint(f\"Final job status: {final_status}\")","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":456,"to":462}}}}],["421",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```text\nInitial job status: IN_QUEUE\nSleeping for 3 seconds to allow for job cancellation...\nFinal job status: CANCELLED\n```\n\n</TabItem>\n\n</Tabs>","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":465,"to":476}}}}],["422",{"pageContent":"Purge queue\n\nYou can purge all jobs from a queue by using the `purge_queue()` function.\nYou can provide the `timeout` parameter to specify how long to wait for the server to respond before purging the queue.\n\n`purge_queue()` doesn't affect Jobs in progress.\n\n```python\nimport runpod\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n\nendpoint = runpod.Endpoint(\"YOUR_ENDPOINT_ID\")\n\nendpoint.purge_queue(timeout=3)\n```","metadata":{"source":"/runpod-docs/docs/sdks/python/endpoints.md","loc":{"lines":{"from":478,"to":494}}}}],["423",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ndescription: \"Get started with setting up your RunPod projects using Python. Learn how to install the RunPod SDK, create a Python virtual environment, and configure your API key for access to the RunPod platform.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nGet started with setting up your RunPod projects using Python.\nDepending on the specific needs of your project, there are various ways to interact with the RunPod platform.\nThis guide provides an approach to get you up and running.","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":1,"to":12}}}}],["424",{"pageContent":"Install the RunPod SDK\n\nCreate a Python virtual environment to install the RunPod SDK library.\nVirtual environments allow you to manage dependencies for different projects separately, avoiding conflicts between project requirements.\n\nTo get started, install setup a virtual environment then install the RunPod SDK library.\n\n<Tabs>\n  <TabItem value=\"macos\" label=\"macOS\" default>\n\nCreate a Python virtual environment with [venv](https://docs.python.org/3/library/venv.html):\n\n    ```command\n    python3 -m venv env\n    source env/bin/activate\n    ```\n\n</TabItem>\n  <TabItem value=\"windows\" label=\"Windows\">\n\nCreate a Python virtual environment with [venv](https://docs.python.org/3/library/venv.html):\n\n    ```command\n    python -m venv env\n    env\\Scripts\\activate\n    ```\n\n</TabItem>\n  <TabItem value=\"linux\" label=\"Linux\">\n\nCreate a Python virtual environment with [venv](https://docs.python.org/3/library/venv.html):\n\n    ```command\n    python3 -m venv env\n    source env/bin/activate","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":14,"to":48}}}}],["425",{"pageContent":"</TabItem>\n</Tabs>\n\nTo install the SDK, run the following command from the terminal.\n\n```command\npython -m pip install runpod\n```\n\n<!--\npip uninstall -y runpod\n-->\n\nYou should have the RunPod SDK installed and ready to use.","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":51,"to":64}}}}],["426",{"pageContent":"Get RunPod SDK version\n\nTo ensure you've setup your RunPod SDK in Python, choose from one of the following methods to print the RunPod Python SDK version to your terminal.\n\n<Tabs>\n  <TabItem value=\"pip\" label=\"Pip\" default>\n\n    Run the following command using pip to get the RunPod SDK version.\n\n    ```command\n    pip show runpod\n    ```\n\n    You should see something similar to the following output.\n\n    ```command\n    runpod==1.6.1\n    ```\n\n</TabItem>\n  <TabItem value=\"shell\" label=\"Shell\">\n\n    Run the following command from your terminal to get the RunPod SDK version.\n\n    ```command\n    python3 -c \"import runpod; print(runpod.__version__)\"","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":66,"to":91}}}}],["427",{"pageContent":"</TabItem>\n  <TabItem value=\"python\" label=\"Python\">\n\n    To ensure you've setup your installation correctly, get the RunPod SDK version.\n    Create a new file called `main.py`.\n    Add the following to your Python file and execute the script.\n\n    ```python\n    import runpod\n\n    version = runpod.version.get_version()\n\n    print(f\"RunPod version number: {version}\")\n    ```\n\n    You should see something similar to the following output.\n\n    ```text\n    RunPod version number: 1.X.0\n    ```\n\n</TabItem>\n</Tabs>\n\nYou can find the latest version of the RunPod Python SDK on [GitHub](https://github.com/runpod/runpod-python/releases).\n\nNow that you've installed the RunPod SDK, add your API key.","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":94,"to":120}}}}],["428",{"pageContent":"Add your API key\n\nSet `api_key` and reference its variable in your Python application.\nThis authenticates your requests to the RunPod platform and allows you to access the [RunPod API](/sdks/python/apis).\n\n```python\nimport runpod\nimport os\n\nrunpod.api_key = os.getenv(\"RUNPOD_API_KEY\")\n```\n\n:::note\n\nIt's recommended to use environment variables to set your API key.\nYou shouldn't load your API key directly into your code.\n\nFor these examples, the API key loads from an environment variable called `RUNPOD_API_KEY`.\n\n:::\n\nNow that you've have the RunPod Python SDK installed and configured, you can start using the RunPod platform.\n\nFor more information, see:\n\n- [APIs](/sdks/python/apis)\n- [Endpoints](/sdks/python/endpoints)","metadata":{"source":"/runpod-docs/docs/sdks/python/overview.md","loc":{"lines":{"from":122,"to":148}}}}],["429",{"pageContent":"---\ntitle: Loggers\ndescription: \"Enable efficient application monitoring and debugging with RunPod's structured logging interface, simplifying issue identification and resolution, and ensuring smooth operation.\"\n---\n\nLogging is essential for insight into your application's performance and health.\nIt facilitates quick identification and resolution of issues, ensuring smooth operation.\n\nBecause of this, RunPod provides a structured logging interface, simplifying application monitoring and debugging, for your Handler code.\n\nTo setup logs, instantiate the `RunPodLogger()` module.\n\n```python\nimport runpod\n\nlog = runpod.RunPodLogger()","metadata":{"source":"/runpod-docs/docs/sdks/python/_loggers.md","loc":{"lines":{"from":1,"to":16}}}}],["430",{"pageContent":"Then set the log level.\nIn the following example, there are two logs levels being set.\n\n```python\nimport runpod\nimport os\n\nlog = runpod.RunPodLogger()\n\n\ndef handler(job):\n    try:\n        job_input = job[\"input\"]\n        log.info(\"Processing job input\")\n\n        name = job_input.get(\"name\", \"World\")\n        log.info(\"Processing completed successfully\")\n\n        return f\"Hello, {name}!\"\n    except Exception as e:\n        # Log the exception with an error level log\n        log.error(f\"An error occurred: {str(e)}\")\n        return \"An error occurred during processing.\"\n\n\nrunpod.serverless.start({\"handler\": handler})\n```","metadata":{"source":"/runpod-docs/docs/sdks/python/_loggers.md","loc":{"lines":{"from":19,"to":45}}}}],["431",{"pageContent":"Log levels\n\nRunPod provides a logging interface with types you're already familiar with.\n\nThe following provides a list of log levels you can set inside your application.\n\n- `debug`: For in-depth troubleshooting. Use during development to track execution flow.\n- `info`: (default) Indicates normal operation. Confirms the application is running as expected.\n- `warn`: Alerts to potential issues. Signals unexpected but non-critical events.\n- `error`: Highlights failures. Marks inability to perform a function, requiring immediate attention.","metadata":{"source":"/runpod-docs/docs/sdks/python/_loggers.md","loc":{"lines":{"from":47,"to":56}}}}],["432",{"pageContent":"---\ntitle: Get started\nsidebar_position: 2\ndescription: \"Learn how to test your deployed Endpoint with a sample request, view the response, and send requests using cURL or an HTTP client, then customize your Handler Function for more control over your API.\"\n---\n\nNow that your Endpoint is deployed, send a test.\nThis is a great way to test your Endpoint before sending a request from your application.\n\n## Send a Request\n\n1. From the Endpoint's page, select **Requests**.\n2. Choose **Run**.\n3. You should see a successful response with the following:\n\n```json\n{\n  \"id\": \"6de99fd1-4474-4565-9243-694ffeb65218-u1\",\n  \"status\": \"IN_QUEUE\"\n}\n```\n\nAfter a few minutes, the stream will show the full response.\n\nYou can now begin sending requests to your Endpoint from your terminal and an application.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/get-started.md","loc":{"lines":{"from":1,"to":25}}}}],["433",{"pageContent":"Send a request using cURL\n\nOnce your Endpoint is deployed, you can send a request.\nThis example sends a response to the Endpoint using cURL; however, you can use any HTTP client.\n\n```curl\ncurl --request POST \\\n     --url https://api.runpod.ai/v2/${endpoint_id}/runsync\n     --header \"accept: application/json\" \\\n     --header \"authorization: ${YOUR_API_KEY}\" \\\n     --header \"content-type: application/json\" \\\n     --data '\n{\n  \"input\": {\n    \"prompt\": \"A coffee cup.\",\n    \"height\": 512,\n    \"width\": 512,\n    \"num_outputs\": 1,\n    \"num_inference_steps\": 50,\n    \"guidance_scale\": 7.5,\n    \"scheduler\": \"KLMS\"\n  }\n}\n'\n```\n\nWhere `endpoint_id` is the name of your Endpoint and `YOUR_API_KEY` is your API Key.\n\n:::note\n\nDepending on any modifications you made to your Handler Function, you may need to modify the request.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/get-started.md","loc":{"lines":{"from":27,"to":59}}}}],["434",{"pageContent":"Next steps\n\nNow that you have successfully launched an endpoint using a template, you can:\n\n- [Invoke jobs](/serverless/endpoints/job-operations)\n\nIf the models provided aren't enough, you can write your own customize Function Handler:\n\n- [Customize the Handler Function](/serverless/workers/handlers/overview)","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/get-started.md","loc":{"lines":{"from":61,"to":69}}}}],["435",{"pageContent":"---\ntitle: Job operations\ndescription: \"Learn how to use the Runpod Endpoint to manage job operations, including running, checking status, purging queues, and streaming results, with cURL and SDK examples.\"\nsidebar_position: 2\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nThis page provides instructions on job operations using the Runpod Endpoint.\nYou can invoke a job to run Endpoints the way you would interact with an API, get a status of a job, purge your job queue, and more with operations.\n\nThe following guide demonstrates how to use cURL to interact with an Endpoint.\nYou can also use the following SDK to interact with Endpoints programmatically:\n\n- [Python SDK](/sdks/python/endpoints)\n\nFor information on sending requests, see [Send a request](/serverless/endpoints/send-requests).","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":1,"to":18}}}}],["436",{"pageContent":"Asynchronous Endpoints\n\nAsynchronous endpoints are designed for long-running tasks. When you submit a job through these endpoints, you receive a Job ID in response.\nYou can use this Job ID to check the status of your job at a later time, allowing your application to continue processing without waiting for the job to complete immediately.\nThis approach is particularly useful for tasks that require significant processing time or when you want to manage multiple jobs concurrently.\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n\n```bash\ncurl -X POST https://api.runpod.ai/v2/{endpoint_id}/run \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer ${API_KEY}' \\\n    -d '{\"input\": {\"prompt\": \"Your prompt\"}}'\n```\n\n    </TabItem>\n\n<TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"eaebd6e7-6a92-4bb8-a911-f996ac5ea99d\",\n  \"status\": \"IN_QUEUE\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":20,"to":48}}}}],["437",{"pageContent":"Synchronous Endpoints\n\nSynchronous endpoints are ideal for short-lived tasks where immediate results are necessary.\nUnlike asynchronous calls, synchronous endpoints wait for the job to complete and return the result directly in the response.\nThis method is suitable for operations that are expected to complete quickly and where the client can afford to wait for the result.\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n\n```bash\ncurl -X POST https://api.runpod.ai/v2/{endpoint_id}/runsync \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer ${API_KEY}' \\\n    -d '{\"input\": {\"prompt\": \"Your prompt\"}}'\n```\n\n    </TabItem>\n\n<TabItem value=\"output\" label=\"Output\">\n```json\n{\n  \"delayTime\": 824,\n  \"executionTime\": 3391,\n  \"id\": \"sync-79164ff4-d212-44bc-9fe3-389e199a5c15\",\n  \"output\": [\n    {\n      \"image\": \"https://image.url\",\n      \"seed\": 46578\n    }\n  ],\n  \"status\": \"COMPLETED\"\n}\n```\n  </TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":50,"to":84}}}}],["438",{"pageContent":"Health Endpoint\n\nThe `/health` endpoint provides insights into the operational status of the endpoint, including the number of workers available and job statistics.\nThis information can be used to monitor the health and performance of the API, helping you manage workload and troubleshoot issues more effectively.\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n\n```bash\ncurl --request GET \\\n     --url https://api.runpod.ai/v2/{endpoint_id}/health \\\n     --header 'accept: application/json' \\\n     --header 'Authorization: Bearer ${API_KEY}'\n```\n\n    </TabItem>\n\n<TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"jobs\": {\n    \"completed\": 1,\n    \"failed\": 5,\n    \"inProgress\": 0,\n    \"inQueue\": 2,\n    \"retried\": 0\n  },\n  \"workers\": {\n    \"idle\": 0,\n    \"running\": 0\n  }\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":86,"to":122}}}}],["439",{"pageContent":"Cancel Job\n\nTo cancel a job in progress, specify the `cancel` parameter with the endpoint ID and the job ID.\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n\n```bash\ncurl -X POST https://api.runpod.ai/v2/{endpoint_id}/cancel/{job_id} \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer ${API_KEY}'\n```\n\n    </TabItem>\n\n<TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"id\": \"724907fe-7bcc-4e42-998d-52cb93e1421f-u1\",\n  \"status\": \"CANCELLED\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":124,"to":149}}}}],["440",{"pageContent":"Purge Queue Endpoint\n\nThe `/purge-queue` endpoint allows you to clear all jobs that are currently in the queue.\nThis operation does not affect jobs that are already in progress.\nIt is a useful tool for managing your job queue, especially in situations where you need to reset or clear pending tasks due to operational changes or errors.\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n\n```bash\ncurl -X POST https://api.runpod.ai/v2/{endpoint_id}/purge-queue \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer ${API_KEY}'\n```\n\n    </TabItem>\n\n<TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"removed\": 2,\n  \"status\": \"completed\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":151,"to":178}}}}],["441",{"pageContent":"Check Job Status\n\nTo track the progress or result of an asynchronous job, you can check its status using the Job ID.\nThis endpoint provides detailed information about the job, including its current status, execution time, and the output if the job has completed.\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n\n```bash\ncurl https://api.runpod.ai/v2/{endpoint_id}/status/{job_id} \\\n    -H 'Authorization: Bearer ${API_KEY}'\n```\n\n    </TabItem>\n\n<TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"delayTime\": 31618,\n  \"executionTime\": 1437,\n  \"id\": \"60902e6c-08a1-426e-9cb9-9eaec90f5e2b-u1\",\n  \"output\": {\n    \"input_tokens\": 22,\n    \"output_tokens\": 16,\n    \"text\": [\n      \"Hello! How can I assist you today?\\nUSER: I'm having\"\n    ]\n  },\n  \"status\": \"COMPLETED\"\n}\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":180,"to":214}}}}],["442",{"pageContent":"Stream results\n\nFor jobs that produce output incrementally, the stream endpoint allows you to receive results as they are generated.\nThis is particularly useful for tasks that involve continuous data processing or where immediate partial results are beneficial.\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n\n```bash\ncurl https://api.runpod.ai/v2/{endpoint_id}/stream/{job_id} \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer ${API_KEY}'","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":216,"to":227}}}}],["443",{"pageContent":"</TabItem>\n\n<TabItem value=\"output\" label=\"Output\">\n\n```json\n[\n  {\n    \"metrics\": {\n      \"avg_gen_throughput\": 0,\n      \"avg_prompt_throughput\": 0,\n      \"cpu_kv_cache_usage\": 0,\n      \"gpu_kv_cache_usage\": 0.0016722408026755853,\n      \"input_tokens\": 0,\n      \"output_tokens\": 1,\n      \"pending\": 0,\n      \"running\": 1,\n      \"scenario\": \"stream\",\n      \"stream_index\": 2,\n      \"swapped\": 0\n    },\n    \"output\": {\n      \"input_tokens\": 0,\n      \"output_tokens\": 1,\n      \"text\": [\n        \" How\"\n      ]\n    }\n  }\n  // omitted for brevity\n]\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":230,"to":263}}}}],["444",{"pageContent":"Rate Limits\n\n- `/run`: 1000 requests every 10 seconds.\n- `/runsync`: 2000 requests every 10 seconds.\n\n:::note\n\nRetrieve results within 30 minutes for privacy protection.\n\n:::\n\nFor reference information on Endpoints, see [Endpoint Operations](/serverless/references/operations.md).","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/job-operations.md","loc":{"lines":{"from":265,"to":276}}}}],["445",{"pageContent":"---\ntitle: \"Manage Endpoints\"\ndescription: \"Learn to create, edit, and manage Serverless Endpoints, including adding network volumes and setting GPU prioritization, with step-by-step guides and tutorials.\"\nsidebar_position: 10\n---\n\nLearn to manage Severless Endpoints.\n\n## Create an Endpoint\n\nYou can create an Endpoint in the Web interface.\n\n1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).\n2. Select **+ New Endpoint** and enter the following:\n   1. Endpoint Name.\n   2. Select your GPUs.\n   3. Configure your workers.\n   4. Add a container image.\n   5. Select **Deploy**.\n\n## Delete an Endpoint\n\nYou can delete an Endpoint in the Web interface.\nBefore an Endpoint can be deleted, all workers must be removed.\n\n1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).\n2. Select the Endpoint you'd like to remove.\n3. Select **Edit Endpoint** and set **Max Workers** to `0`.\n4. Choose **Update** and then **Delete Endpoint**.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":1,"to":29}}}}],["446",{"pageContent":"Edit an Endpoint\n\nYou can edit a running Endpoint in the Web interface after you've deployed it.\n\n1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).\n2. Select the Endpoint you'd like to edit.\n3. Select **Edit Endpoint** and make your changes.\n4. Choose **Update**.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":31,"to":38}}}}],["447",{"pageContent":"Set GPU prioritization an Endpoint\n\nWhen creating or modifying a Worker Endpoint, specify your GPU preferences in descending order of priority.\nThis allows you to configure the desired GPU models for your Worker Endpoints.\n\nRunPod attempts to allocate your first choice if it's available.\nIf your preferred GPU isn't available, the system automatically defaults to the next available GPU in your priority list.\n\n1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).\n2. Select the Endpoint you'd like to update.\n3. Select the priority of the GPUs you'd like to use.\n4. Choose **Update**.\n\n:::note\n\nYou can force a configuration update by setting **Max Workers** to 0, selecting **Update**, then updating your max workers back to your needed value.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":40,"to":57}}}}],["448",{"pageContent":"Add a Network Volume\n\nNetwork volumes are a way to share data between Workers: they are mounted to the same path on each Worker.\nFor example, if a Worker contains a large-language model, you can use a network volume to share the model across all Workers.\n\n1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).\n2. Select the Endpoint you'd like to edit.\n3. Select **Edit Endpoint** and make your changes.\n4. Under **Advanced** choose **Select Network Volume**.\n5. Select the storage device and then choose **Update** to continue.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/manage-endpoints.md","loc":{"lines":{"from":59,"to":68}}}}],["449",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ndescription: \"Deploy and manage serverless workers with RunPod Endpoints, featuring asynchronous and synchronous operations, scalability, and flexibility for modern computing tasks.\"\n---\n\nRunPod Endpoints serve as the gateway to deploying and managing your Serverless Workers.\nThese endpoints allow for flexible interaction with a variety of models, supporting both asynchronous and synchronous operations tailored to your computational needs.\nWhether you're processing large data sets, requiring immediate results, or scheduling tasks to run in the background, RunPod's API Endpoints provide the versatility and scalability essential for modern computing tasks.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":1,"to":9}}}}],["450",{"pageContent":"Key features\n\n- **Asynchronous and synchronous jobs:** Choose the execution mode that best fits your workflow, whether it's a task that runs in the background or one that delivers immediate results.\n- **Serverless Workers:** Deploy your computational tasks without worrying about server management, enjoying the benefits of a fully managed infrastructure.\n- **Scalability and flexibility:** Easily scale your operations up or down based on demand, with the flexibility to handle various computational loads.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":11,"to":15}}}}],["451",{"pageContent":"Getting started\n\nBefore you begin, ensure you have obtained your [RunPod API key](/get-started/api-keys).\nThis key is essential for authentication, billing, and accessing the API.\n\nYou can find your API key in the [user settings section](https://www.runpod.io/console/user/settings) of your RunPod account.\n\n:::note\n\n**Privacy and security:** RunPod prioritizes your data's privacy and security.\nInputs and outputs are retained for a maximum of 30 minutes for asynchronous requests and 1 minute for synchronous requests to protect your information.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":17,"to":29}}}}],["452",{"pageContent":"Exploring RunPod Endpoints\n\nDive deeper into what you can achieve with RunPod Endpoints through the following resources:\n\n- [Use the vLLM Worker](/serverless/workers/vllm/overview): Learn how to deploy a vLLM Worker as a Serverless Endpoint, with detailed guides on configuration and sending requests.\n- [Invoke Jobs](/serverless/endpoints/job-operations): Learn how to submit jobs to your serverless workers, with detailed guides on both asynchronous and synchronous operations.\n- [Send Requests](/serverless/endpoints/send-requests): Discover how to communicate with your endpoints, including tips on structuring requests for optimal performance.\n- [Manage Endpoints](/serverless/endpoints/manage-endpoints): Find out how to manage your endpoints effectively, from deployment to scaling and monitoring.\n- [Endpoint Operations](/serverless/references/operations): Access a comprehensive list of operations supported by RunPod Endpoints, including detailed documentation and examples.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/overview.md","loc":{"lines":{"from":31,"to":39}}}}],["453",{"pageContent":"---\ntitle: \"Send a request\"\ndescription: \"Learn how to construct a JSON request body to send to your custom endpoint, including optional inputs for webhooks, execution policies, and S3-compatible storage, to optimize job execution and resource management.\"\nsidebar_position: 4\n---\n\nBefore sending a job request, ensure you have deployed your custom endpoint.\n\nLet's start by constructing our request body to send to the endpoint.\n\n## JSON Request Body\n\nYou can make requests to your endpoint with JSON. Your request must include a JSON object containing an `input` key. For example, if your handler requires an input prompt, you might send in something like this:\n\n```json\n{\n  \"input\": {\n    \"prompt\": \"The lazy brown fox jumps over the\"\n  }\n}\n```","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":1,"to":21}}}}],["454",{"pageContent":"Optional Inputs\n\nAlong with an input key, you can include other top-level inputs to access different functions. If a key is passed in at the top level and not included in the body of your request, it will be discarded and unavailable to your handler.\n\nThe following optional inputs are available to all endpoints regardless of the worker.\n\n- Webhooks\n- Execution policies\n- S3-compatible storage\n\n### Webhooks\n\nTo see notifications for completed jobs, pass a URL in the top level of the request:\n\n```json\n{\n  \"input\": {},\n  \"webhook\": \"https://URL.TO.YOUR.WEBHOOK\"\n}\n```\n\nYour webhook endpoint should respond with a `200` status to acknowledge the successful call. If the call is not successful, the request waits 10 seconds and sends the call again up to two more times.\n\nA `POST` request goes to your URL when the job is complete. This request contains the same information as fetching the results from the `/status/{job_id}` endpoint.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":23,"to":46}}}}],["455",{"pageContent":"Execution Policies\n\nBy default, if a job remains `IN_PROGRESS` for longer than 10 minutes without completion, it's automatically terminated.\n\nThis default behavior keeps a hanging request from draining your account credits.\n\nTo customize the management of job lifecycles and resource consumption, the following policies can be configured:\n\n- **Execution Timeout**: Specifies the maximum duration that a job can run before it's automatically terminated. This limit helps prevent jobs from running indefinitely and consuming resources. You can overwrite the value for a request by specifying `executionTimeout` in the job input.\n\n:::note\n\nChanging the **Execution Timeout** value through the Web UI sets the value for all requests to an Endpoint.\nYou can still overwrite the value for individual requests with `executionTimeout` in the job input.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":48,"to":63}}}}],["456",{"pageContent":"- **Low Priority**: When true, the job does not trigger scaling up resources to execute. Instead, it executes when there are no pending higher priority jobs in the queue. Use this option for tasks that are not time-sensitive.\n- **TTL (Time-to-Live)**: Defines the maximum time a job can remain in the queue before it's automatically terminated. This parameter ensures that jobs don't stay in the queue indefinitely.\n\n```json\n{\n  \"input\": {},\n  \"policy\": {\n    \"executionTimeout\": int, // Time in milliseconds. Must be greater than 5 seconds.\n    \"lowPriority\": bool, // Sets the job's priority to low. Default behavior escalates to high under certain conditions.\n    \"ttl\": int // Time in milliseconds. Must be greater than or equal to 10 seconds. Default is 24 hours. Maximum is one week.\n  }\n}","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":65,"to":76}}}}],["457",{"pageContent":"By configuring the execution timeout, priority, and TTL policies, you have more control over job execution and efficient system resource management.","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":79,"to":79}}}}],["458",{"pageContent":"S3-Compatible Storage\n\nPass in the credentials for S3-compatible object storage as follows:\n\n```json\n{\n  \"input\": {},\n  \"s3Config\": {\n    \"accessId\": \"key_id_or_username\",\n    \"accessSecret\": \"key_secret_or_password\",\n    \"bucketName\": \"storage_location_name\",\n    \"endpointUrl\": \"storage_location_address\"\n  }\n}\n```\n\nThe configuration only passes to the worker. It is not returned as part of the job request output.\n\n:::note\n\nThe serverless worker must contain logic that allows it to use this input. If you build a custom endpoint and request s3Config in the job input, your worker is ultimately responsible for using the information passed in to upload the output.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/endpoints/send-requests.md","loc":{"lines":{"from":81,"to":103}}}}],["459",{"pageContent":"---\ntitle: Overview\ndescription: \"Scale machine learning workloads with RunPod Serverless, offering flexible GPU computing for AI inference, training, and general compute, with pay-per-second pricing and fast deployment options for custom endpoints and handler functions.\"\nsidebar_position: 1\n---\n\nRunPod offers Serverless GPU and CPU computing for AI inference, training, and general compute, allowing users to pay by the second for their compute usage.\nThis flexible platform is designed to scale dynamically, meeting the computational needs of AI workloads from the smallest to the largest scales.\n\nYou can use the following methods:\n\n- Quick Deploy: Quick deploys are pre-built custom endpoints of the most popular AI models.\n- Handler Functions: Bring your own functions and run in the cloud.\n- vLLM Endpoint: Specify a Hugging Face model and run in the cloud.","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":1,"to":14}}}}],["460",{"pageContent":"Why RunPod Serverless?\n\nYou should choose RunPod Serverless instances for the following reasons:","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":16,"to":18}}}}],["461",{"pageContent":"- **AI Inference:** Handle millions of inference requests daily and can be scaled to handle billions, making it an ideal solution for machine learning inference tasks. This allows users to scale their machine learning inference while keeping costs low.\n- **AI Training:** Machine learning training tasks that can take up to 12 hours. GPUs can be spun up per request and scaled down once the task is done, providing a flexible solution for AI training needs.\n- **Autoscale:** Dynamically scale workers from 0 to 100 on the Secure Cloud platform, which is highly available and distributed globally. This provides users with the computational resources exactly when needed.\n- **Container Support:** Bring any Docker container to RunPod. Both public and private image repositories are supported, allowing users to configure their environment exactly how they want.","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":20,"to":23}}}}],["462",{"pageContent":"- **3s Cold-Start:** To help reduce cold-start times, RunPod proactively pre-warms workers. The total start time will vary based on the runtime, but for stable diffusion, the total start time is 3 seconds cold-start plus 5 seconds runtime.\n- **Metrics and Debugging:** Transparency is vital in debugging. RunPod provides access to GPU, CPU, Memory, and other metrics to help users understand their computational workloads. Full debugging capabilities for workers through logs and SSH are also available, with a web terminal for even easier access.\n- **Webhooks:** Users can leverage webhooks to get data output as soon as a request is done. Data is pushed directly to the user's Webhook API, providing instant access to results.","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":24,"to":26}}}}],["463",{"pageContent":"RunPod Serverless are not just for AI Inference and Training.\nThey're also great for a variety of other use cases.\nFeel free to use them for tasks like rendering, molecular dynamics, or any other computational task that suits your needs.","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":28,"to":30}}}}],["464",{"pageContent":"How to interact with RunPod Serverless?\n\nRunPod generates an Endpoint Id that that allows you to interact with your Serverless Pod.\nPass in your Endpoint Id to the Endpoint URL and provide an operation.\n\nThis Endpoint URL will look like this:\n\n```text\nhttps://api.runpod.ai/v2/{endpoint_id}/{operation}\n```\n\n- `api.runpod.ai`: The base URL to access RunPod.\n- `v2`: The API version.\n- `endpoint_id`: The ID of the Serverless Endpoint.\n- `operation`: The operation to perform on the Serverless Endpoint.\n  - Valid options: `run` | `runsync` | `status` | `cancel` | `health` | `purge-queue`\n\nFor more information, see [Invoke jobs](/serverless/endpoints/job-operations).\n\n<!--\n### Endpoints\n\nA Serverless Endpoint provides the REST API endpoint that serves your application.\nYou can create multiple endpoints for your application, each with its own configuration.","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":32,"to":55}}}}],["465",{"pageContent":"Serverless handlers\n\nServerless handlers are the core of the Serverless platform.\nThey are the code that is executed when a request is made to a Serverless endpoint.\nHandlers are written in Python and can be used to run any code that can be run in a Docker container.\n-->","metadata":{"source":"/runpod-docs/docs/serverless/overview.md","loc":{"lines":{"from":57,"to":62}}}}],["466",{"pageContent":"---\ntitle: Quick Deploys\nid: quick-deploys\nsidebar_position: 2\ndescription: \"Quickly deploy custom Endpoints of popular models with minimal configuration through the web interface, following a simple 5-step process. Customize your deployments and utilize CI/CD features with RunPod's GitHub repositories and Handler Functions.\"\n---\n\nQuick Deploys lets you deploy custom Endpoints of popular models with minimal configuration.\n\nYou can find [Quick Deploys](https://www.runpod.io/console/serverless) and their descriptions in the Web interface.\n\n## How to do I get started with Quick Deploys?\n\nYou can get started by following the steps below:\n\n1. Go to the [Serverless section](https://www.runpod.io/console/serverless) in the Web interface.\n2. Select your model.\n3. Provide a name for your Endpoint.\n4. Select your GPU instance.\n   1. (optional) You can further customize your deployment.\n5. Select **Deploy**.\n\nYour Endpoint Id is now created and you can use it in your application.","metadata":{"source":"/runpod-docs/docs/serverless/quick-deploys.md","loc":{"lines":{"from":1,"to":23}}}}],["467",{"pageContent":"Customizing your Functions\n\nTo customize AI Endpoints, visit the [RunPod GitHub repositories](https://github.com/runpod-workers).\nHere, you can fork the programming and compute model templates.\n\nBegin with the [worker-template](https://github.com/runpod-workers/worker-template) and modify it as needed.\nThese RunPod workers incorporate CI/CD features to streamline your project setup.\n\nFor detailed guidance on customizing your interaction Endpoints, refer to [Handler Functions](/serverless/workers/handlers/overview).","metadata":{"source":"/runpod-docs/docs/serverless/quick-deploys.md","loc":{"lines":{"from":25,"to":33}}}}],["468",{"pageContent":"---\ntitle: \"Endpoint configurations\"\nsidebar_position: 1\ndescription: Configure your Endpoint settings to optimize performance and cost, including GPU selection, worker count, idle timeout, and advanced options like data centers, network volumes, and scaling strategies.\n---\n\nThe following are configurable settings within an Endpoint.\n\n## Endpoint Name\n\nCreate a name you'd like to use for the Endpoint configuration.\nThe resulting Endpoint is assigned a random ID to be used for making calls.\n\nThe name is only visible to you.\n\n## GPU Selection\n\nSelect one or more GPUs that you want your Endpoint to run on. RunPod matches you with GPUs in the order that you select them, so the first GPU type that you select is prioritized, then the second, and so on. Selecting multiple GPU types can help you get a worker more quickly, especially if your first selection is an in-demand GPU.","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":1,"to":18}}}}],["469",{"pageContent":"Active (Min) Workers\n\nSetting this amount to one will result in \"always on\" workers.\nThis will allow you to have a worker ready to respond to job requests without incurring any cold start delay.\n\n:::note\n\nYou will incur the cost of any active workers you have set regardless if they are working on a job.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":20,"to":29}}}}],["470",{"pageContent":"Max Workers\n\nSet an upper limit on the number of active workers your endpoint has running at any given point.\nSetting a value for max workers that is too low can lead to [throttled workers](/glossary#throttled-worker).\nIf you consistently see throttled workers, increase your max workers to five or more.\n\nDefault: 3\n\n<details>\n<summary>\n\nHow to configure Max Workers\n\n</summary>\nYou can also configure a max worker count. This is the top limit of what RunPod will attempt to auto-scale for you. Use this to cap your concurrent request count and also limit your cost ceiling.\n\n:::note\n\nWe currently base your caching coefficient by this number, so an endpoint with higher max worker count will also receive a higher priority when caching workers.\n\nThis is partially why we limit new accounts to a relatively low max concurrency at the account level.\nIf you want to get this number raised, you generally will need to have a higher history of spending, or commit to a relatively high spend per month.","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":31,"to":52}}}}],["471",{"pageContent":"You should generally aim to set your max worker count to be 20% higher than you expect your max concurrency to be.\n\n:::\n\n</details>","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":54,"to":58}}}}],["472",{"pageContent":"GPUs / Worker\n\nThe number of GPUs you would like assigned to your worker.\n\n:::note\n\nCurrently only available for 48 GB GPUs.\n\n:::\n\n## Idle Timeout\n\nThe amount of time in seconds a worker not currently processing a job will remain active until it is put back into standby.\nDuring the idle period, your worker is considered running and will incur a charge.\n\nDefault: 5 seconds\n\n## FlashBoot\n\nRunPod magic to further reduce the average cold-start time of your endpoint.\nFlashBoot works best when an endpoint receives consistent utilization.\nThere is no additional cost associated with FlashBoot.","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":60,"to":81}}}}],["473",{"pageContent":"Advanced\n\nAdditional controls to help you control where your endpoint is deployed and how it responds to incoming requests.\n\n### Data Centers\n\nControl which data centers can deploy and cache your workers. Allowing multiple data centers can help you get a worker more quickly.\n\nDefault: all data centers\n\n### Select Network Volume\n\nAttach a network storage volume to your deployed workers.\n\nNetwork volumes will be mounted to `/runpod-volume/`.\n\n:::note\n\nWhile this is a high performance network drive, do keep in mind that it will have higher latency than a local drive.\n\nThis will limit the availability of cards, as your endpoint workers will be locked to the datacenter that houses your network volume.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":83,"to":105}}}}],["474",{"pageContent":"Scale Type\n\n- **Queue Delay** scaling strategy adjusts worker numbers based on request wait times. With zero workers initially, the first request adds one worker. Subsequent requests add workers only after waiting in the queue for the defined number of delay seconds.\n- **Request Count** scaling strategy adjusts worker numbers according to total requests in the queue and in progress. It automatically adds workers as the number of requests increases, ensuring tasks are handled efficiently.\n\n```text\n_Total Workers Formula: Math.ceil((requestsInQueue + requestsInProgress) / <set request count)_\n```","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":107,"to":114}}}}],["475",{"pageContent":"GPU Types\n\nWithin the select GPU size category you can further select which GPU models you would like your endpoint workers to run on.\nDefault: `4090` | `A4000` | `A4500`\n\n<details>\n<summary>\nWhat's the difference between GPU models.\n</summary>\nA100s are about 2-3x faster than A5000s and also allow double the VRAM with very high bandwidth throughout. 3090s and A5000s are 1.5-2x faster than A4000s. Sometimes, it may make more sense to use 24 GB even if you don't need it compared to 16 GB due to faster response times. Depending on the nature of the task, it's also possible that execution speeds may be bottlenecked and not significantly improved simply by using a higher-end card. Do your own calculations and experimentation to determine out what's most cost-effective for your workload and task type.\n\nWant access to different flavors? [Let us know](https://www.runpod.io/contact) and we can look at expanding our offerings!\n\n</details>","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":116,"to":129}}}}],["476",{"pageContent":"CUDA version selection\n\nYou have the ability to select the allowed CUDA versions for your workloads.\nThe CUDA version selection determines the compatible GPU types that will be used to execute your serverless tasks.\n\nSpecifically, the CUDA version selection works as follows:\n\n- You can choose one or more CUDA versions that your workload is compatible with or requires.\n- RunPod will then match your workload to available GPU instances that have the selected CUDA versions installed.\n- This ensures that your serverless tasks run on GPU hardware that meets the CUDA version requirements.\n\nFor example, if you select CUDA 11.6, your serverless tasks will be scheduled to run on GPU instances that have CUDA 11.6 or a compatible version installed. This allows you to target specific CUDA versions based on your workload's dependencies or performance requirements.","metadata":{"source":"/runpod-docs/docs/serverless/references/endpoint-configurations.md","loc":{"lines":{"from":131,"to":142}}}}],["477",{"pageContent":"---\ntitle: \"Job states\"\nid: \"job-states\"\ndescription: \"Understand the various states of a job in RunPod's Handler Functions, including IN_QUEUE, IN_PROGRESS, COMPLETED, FAILED, CANCELLED, and TIMED_OUT, to effectively manage job flow and troubleshoot issues.\"\nsidebar_position: 5\n---\n\nWhen working with Handler Functions in RunPod, it's essential to understand the various states a job can go through from initiation to completion.\nEach state provides insight into the job's current status and helps in managing the job flow effectively.","metadata":{"source":"/runpod-docs/docs/serverless/references/job-states.md","loc":{"lines":{"from":1,"to":9}}}}],["478",{"pageContent":"Job state\n\nHere are the states a job can be in:","metadata":{"source":"/runpod-docs/docs/serverless/references/job-states.md","loc":{"lines":{"from":11,"to":13}}}}],["479",{"pageContent":"- `IN_QUEUE`: This state indicates that the job is currently in the endpoint queue. It's waiting for an available worker to pick it up for processing.\n- `IN_PROGRESS`: Once a worker picks up the job, its state changes to `IN_PROGRESS`. This means the job is actively being processed and is no longer in the queue.\n- `COMPLETED`: After the job successfully finishes processing and returns a result, it moves to the `COMPLETED` state. This indicates the successful execution of the job.\n- `FAILED`: If a job encounters an error during its execution and returns with an error, it is marked as `FAILED`. This state signifies that the job did not complete successfully and encountered issues.\n- `CANCELLED`: Jobs can be manually cancelled using the `/cancel/job_id` endpoint. If a job is cancelled before it completes or fails, it will be in the `CANCELLED` state.","metadata":{"source":"/runpod-docs/docs/serverless/references/job-states.md","loc":{"lines":{"from":15,"to":19}}}}],["480",{"pageContent":"- `TIMED_OUT`: This state occurs in two scenarios: when a job expires before a worker picks it up, or if the worker fails to report back a result for the job before it reaches its timeout threshold.","metadata":{"source":"/runpod-docs/docs/serverless/references/job-states.md","loc":{"lines":{"from":20,"to":20}}}}],["481",{"pageContent":"---\ntitle: \"Endpoint operations\"\ndescription: \"RunPod's Endpoints enable job submission and output retrieval, using a constructed URL starting with https://api.runpod.ai/v2/{endpoint_id}/{operation}. Operations include job submission, synchronous execution, job status checking, and more.\"\nsidebar_position: 2\n---\n\nRunPod's Endpoints facilitate submitting jobs and retrieving outputs.\n\nTo use these Endpoints, you will need to have your Endpoint Id.\nThe constructed URL will start with `https://api.runpod.ai/v2/{endpoint_id}/{operation}` followed by an operation.\n\nOperations available to all users are:","metadata":{"source":"/runpod-docs/docs/serverless/references/operations.md","loc":{"lines":{"from":1,"to":12}}}}],["482",{"pageContent":"- `/run`: Asynchronous endpoint for submitting jobs. Returns a unique Job ID.\n  - Payload capacity: 10 MB\n  - Rate limit: 1000 per second\n  - Job availability: Job results are accessible for 30 minutes after completion\n- `/runsync`: Synchronous endpoint for shorter running jobs, returning immediate results.\n  - Payload capacity: 20 MB\n  - Rate limit: 2000 requests every 10 seconds\n  - Job availability: Job results are accessible for 60 seconds after completion\n- `/stream/{job_id}`: For streaming results from generator-type handlers.\n- `/status/{job_id}`: To check the job status and retrieve outputs upon completion.\n- `/cancel/{job_id}`: To cancel a job prematurely.\n- `/health`: Provides worker statistics and endpoint health.\n  - Only accepts `GET` methods\n- `/purge-queue`: Clears all queued jobs, it will not cancel jobs in progress.\n\nTo see how to run these Endpoint Operations, see [Invoke a Job](/serverless/endpoints/job-operations).","metadata":{"source":"/runpod-docs/docs/serverless/references/operations.md","loc":{"lines":{"from":14,"to":29}}}}],["483",{"pageContent":"---\ntitle: \"Package and deploy an image\"\ndescription: \"Package your Handler Function into a Docker image for scalable Serverless Worker deployment, leveraging Dockerfiles and Configurable Endpoints for efficient deployment. Learn how to build, push, and integrate your image for continuous integration and testing.\"\nsidebar_position: 2\n---\n\nOnce you have a Handler Function, the next step is to package it into a Docker image that can be deployed as a scalable Serverless Worker.\nThis is accomplished by defining a Docker file to import everything required to run your handler. Example Docker files are in the [runpod-workers](https://github.com/orgs/runpod-workers/repositories) repository on GitHub.\n\n:::note\n\nFor deploying large language models (LLMs), you can use the [Configurable Endpoints](/serverless/workers/vllm/configurable-endpoints) feature instead of working directly with Docker.","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":1,"to":12}}}}],["484",{"pageContent":"Configurable Endpoints simplify the deployment process by allowing you to select a pre-configured template and customize it according to your needs.\n\n:::\n\n_Unfamiliar with Docker? Check out Docker's [overview page](https://docs.docker.com/get-started/overview/) or see our guide on [Containers](/category/containers)._","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":14,"to":18}}}}],["485",{"pageContent":"Docker file\n\nLet's say we have a directory that looks like the following:\n\n```\nproject_directory\n├── Dockerfile\n├── src\n│   └── handler.py\n└── builder\n    └── requirements.txt\n```\n\nYour Dockerfile would look something like this:\n\n```text Docker\nfrom python:3.11.1-buster\n\nWORKDIR /\n\nCOPY builder/requirements.txt .\nRUN pip install -r requirements.txt\n\nADD handler.py .\n\nCMD [ \"python\", \"-u\", \"/handler.py\" ]\n```\n\nTo build and push the image, review the steps in [Get started](/serverless/workers/get-started).\n\n> 🚧 If your handler requires external files such as model weights, be sure to cache them into your docker image. You are striving for a completely self-contained worker that doesn't need to download or fetch external files to run.","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":20,"to":50}}}}],["486",{"pageContent":"Continuous integrations\n\nIntegrate your Handler Functions through continuous integration.\n\nThe [Test Runner](https://github.com/runpod/test-runner) GitHub Action is used to test and integrate your Handler Functions into your applications.\n\n:::note\n\nRunning any Action that sends requests to RunPod occurs a cost.\n\n:::\n\nYou can add the following to your workflow file:\n\n```yaml\n- uses: actions/checkout@v3\n- name: Run Tests\n  uses: runpod/runpod-test-runner@v1\n  with:\n    image-tag: [tag of image to test]\n    runpod-api-key: [a valid Runpod API key]\n    test-filename: [path for a json file containing a list of tests, defaults to .github/tests.json]\n    request-timeout: [number of seconds to wait on each request before timing out, defaults to 300]","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":52,"to":74}}}}],["487",{"pageContent":"If `test-filename` is omitted, the Test Runner Action attempts to look for a test file at `.github/tests.json`.\n\nYou can find a working example in the [Worker Template repository](https://github.com/runpod-workers/worker-template/tree/main/.github).","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":77,"to":79}}}}],["488",{"pageContent":"Using Docker tags\n\nWe also highly recommend the use of tags for Docker images and not relying on the default `:latest` tag label, this will make version tracking and releasing updates significantly easier.\n\n### Docker Image Versioning\n\nTo ensure consistent and reliable versioning of Docker images, we highly recommend using SHA tags instead of relying on the default `:latest` tag.\n\nUsing SHA tags offers several benefits:\n\n- **Version Control:** SHA tags provide a unique identifier for each image version, making it easier to track changes and updates.\n- **Reproducibility:** By using SHA tags, you can ensure that the same image version is used across different environments, reducing the risk of inconsistencies.\n- **Security:** SHA tags help prevent accidental overwrites and ensure that you are using the intended image version.","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":81,"to":93}}}}],["489",{"pageContent":"Using SHA Tags\n\nTo pull a Docker image using its SHA tag, use the following command:\n\n```bash\ndocker pull <image_name>@<sha256:hash>\n```\n\nFor example:\n\n```bash\ndocker pull myapp@sha256:4d3d4b3c5a5c2b3a5a5c3b2a5a4d2b3a2b3c5a3b2a5d2b3a3b4c3d3b5c3d4a3\n```\n\n### Best Practices\n\n- Avoid using the `:latest` tag, as it can lead to unpredictable behavior and make it difficult to track which version of the image is being used.\n- Use semantic versioning (e.g., `v1.0.0`, `v1.1.0`) along with SHA tags to provide clear and meaningful version identifiers.\n- Document the SHA tags used for each deployment to ensure easy rollback and version management.","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":95,"to":113}}}}],["490",{"pageContent":"Other considerations\n\nWhile we do not impose a limit on the Docker image size your container registry might have, be sure to review any limitations they may have. Ideally, you want to keep your final Docker image as small as possible and only container the absolute minimum to run your handler.","metadata":{"source":"/runpod-docs/docs/serverless/workers/deploy/deploy.md","loc":{"lines":{"from":115,"to":117}}}}],["491",{"pageContent":"---\ntitle: Use environment variables\ndescription: \"Learn how to use environment variables in RunPod Handler Functions to securely manage S3 bucket credentials and operations, including uploading images and setting necessary environment variables.\"\n---\n\nIncorporating environment variables into your Handler Functions is a key aspect of managing external resources like S3 buckets.\n\nThis section focuses on how to use environment variables to facilitate the uploading of images to an S3 bucket using RunPod Handler Functions.\n\nYou will go through the process of writing Python code for the uploading and setting the necessary environment variables in the Web interface.\n\n## Prerequistes\n\n- Ensure the RunPod Python library is installed: `pip install runpod`.\n- Have an image file named `image.png` in the Docker container's working directory.","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":1,"to":15}}}}],["492",{"pageContent":"Python Code for S3 Uploads\n\nLet's break down the steps to upload an image to an S3 bucket using Python:\n\n1. **Handler Function for S3 Upload**:\n   Here's an example of a handler function that uploads `image.png` to an S3 bucket and returns the image URL:\n\n   ```python\n   from runpod.serverless.utils import rp_upload\n   import runpod\n\n\n   def handler(job):\n       image_url = rp_upload.upload_image(job[\"id\"], \"./image.png\")\n       return [image_url]\n\n\n   runpod.serverless.start({\"handler\": handler})\n   ```\n\n2. **Packaging Your Code**:\n   Follow the guidelines in [Worker Image Creation](/serverless/workers/deploy) for packaging and deployment.","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":17,"to":38}}}}],["493",{"pageContent":"Setting Environment Variables for S3\n\nUsing environment variables securely passes the necessary credentials and configurations to your serverless function:\n\n1. **Accessing Environment Variables Setting**:\n   In the template creation/editing interface of your pod, navigate to the bottom section where you can set environment variables.\n\n2. **Configuring S3 Variables**:\n   Set the following key variables for your S3 bucket:\n   - `BUCKET_ENDPOINT_URL`\n   - `BUCKET_ACCESS_KEY_ID`\n   - `BUCKET_SECRET_ACCESS_KEY`\n\nEnsure that your `BUCKET_ENDPOINT_URL` includes the bucket name.\nFor example: `https://your-bucket-name.nyc3.digitaloceanspaces.com` | `https://your-bucket-name.nyc3.digitaloceanspaces.com`","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":40,"to":54}}}}],["494",{"pageContent":"Testing your API\n\nFinally, test the serverless function to confirm that it successfully uploads images to your S3 bucket:\n\n1. **Making a Request**:\n   Make a POST request to your API endpoint with the necessary headers and input data. Remember, the input must be a JSON item:\n\n   ```python\n   import requests\n\n   endpoint = \"https://api.runpod.ai/v2/xxxxxxxxx/run\"\n   headers = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer XXXXXXXXXXXXX\"}\n   input_data = {\"input\": {\"inp\": \"this is an example input\"}}\n\n   response = requests.post(endpoint, json=input_data, headers=headers)\n   ```\n\n2. **Checking the Output**:\n   Make a GET request to retrieve the job status and output. Here’s an example of how to do it:\n\n   ```python\n   response = requests.get(\n       \"https://api.runpod.ai/v2/xxxxxxxxx/status/\" + response.json()[\"id\"],\n       headers=headers,\n   )\n   response.json()","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":56,"to":81}}}}],["495",{"pageContent":"The response should include the URL of the uploaded image on completion:\n\n   ```json\n   {\n     \"delayTime\": 86588,\n     \"executionTime\": 1563,\n     \"id\": \"e3d2e250-ea81-4074-9838-1c52d006ddcf\",\n     \"output\": [\n       \"https://your-bucket.s3.us-west-004.backblazeb2.com/your-image.png\"\n     ],\n     \"status\": \"COMPLETED\"\n   }\n   ```\n\nBy following these steps, you can effectively use environment variables to manage S3 bucket credentials and operations within your RunPod Handler Functions.\nThis approach ensures secure, scalable, and efficient handling of external resources in your serverless applications.","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/environment-variables.md","loc":{"lines":{"from":84,"to":99}}}}],["496",{"pageContent":"---\ntitle: Test locally\nsidebar_position: 1\ndescription: \"Test your Handler Function with custom inputs and launch a local test server with Python, allowing you to send requests and simulate deployment scenarios.\"\n---\n\nAs you develop your Handler Function, you will, of course, want to test it with inputs formatted similarly to what you will be sending in once deployed as a worker. The quickest way to run a test is to pass in your input as an argument when calling your handler file. Assuming your handler function is inside of a file called `your_handler.py` and your input is `{\"input\": {\"prompt\": \"The quick brown fox jumps\"}}` you would call your file like so:\n\n```curl\npython your_handler.py --test_input '{\"input\": {\"prompt\": \"The quick brown fox jumps\"}}'","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/local-testing.md","loc":{"lines":{"from":1,"to":10}}}}],["497",{"pageContent":"Additionally, you can launch a local test server that will provide you with an endpoint to send requests to by calling your file with the `--rp_serve_api` argument. See our [blog post](https://blog.runpod.io/workers-local-api-server-introduced-with-runpod-python-0-9-13/) for additional examples.\n\n```bash\npython your_handler.py --rp_serve_api\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/local-testing.md","loc":{"lines":{"from":13,"to":17}}}}],["498",{"pageContent":"---\ntitle: Test response time\nsidebar_label: Test response time\ndescription: \"Discover the right API for your use case by testing different price points and resource allocations. Optimize your tasks for cost-effectiveness and speed by choosing the best GPU pool for your needs.\"\n---\n\nWhen setting up an API, you have several options available at different price points and resource allocations. You can select a single option if you would prefer to only use one price point, or select a preference order between the pools that will allocate your requests accordingly.\n\n![](/img/docs/742bf51-image.png)\n\nThe option that will be most cost effective for you will be based on your use case and your tolerance for task run time. Each situation will be different, so when deciding which API to use, it's worth it to do some testing to not only find out how long your tasks will take to run, but how much you might expect to pay for each task.","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":1,"to":11}}}}],["499",{"pageContent":"To find out how long a task will take to run, select a single pool type as shown in the image above. Then, you can send a request to the API through your preferred method. If you're unfamiliar with how to do so or don't have your own method, then you can use a free option like [reqbin.com](https://reqbin.com/) to send an API request to the RunPod severs.\n\nThe URLs to use in the API will be shown in the My APIs screen:\n\n![](/img/docs/0d8dd86-image.png)\n\nOn reqbin.com, enter the Run URL of your API, select POST under the dropdown, and enter your API key that was given when you created the key under [Settings](https://www.runpod.io/console/serverless/user/settings)(if you do not have it saved, you will need to return to Settings and create a new key). Under Content, you will also need to give it a basic command (in this example, we've used a Stable Diffusion prompt).\n\n![](/img/docs/a9b9cf3-image.png)\n\n![](/img/docs/7744b62-image.png)","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":13,"to":23}}}}],["500",{"pageContent":"Send the request, and it will give you an ID for the request and notify you that it is processing. You can then swap the URL in the request field with the Status address and add the ID to the end of it, and click Send.\n\n![](/img/docs/325f2bc-image.png)\n\nIt will return a Delay Time and an Execution Time, denoted in milliseconds. The Delay Time should be extremely minimal, unless the API process was spun up from a cold start, then a sizable delay is expected for the first request sent. The Execution Time is how long the GPU took to actually process the request once it was received. It may be a good idea to send a number of tests so you can get a min, max, and average run time -- five tests should be an adequate sample size.\n\n![](/img/docs/1608d44-image.png)\n\nYou can then switch the GPU pool above to a different pool and repeat the process.","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":25,"to":33}}}}],["501",{"pageContent":"What will ultimately be right for your use case will be determined by how long you can afford to let the process run. For heavier jobs, a task on a slower GPU will be likely be more cost-effective with a tradeoff of speed. For simpler tasks, there may also be diminishing returns on how fast the task that can be run that may not be significantly improved by selecting higher-end GPUs. Experiment to find the best balance for your scenario.","metadata":{"source":"/runpod-docs/docs/serverless/workers/development/test-response-times.md","loc":{"lines":{"from":35,"to":35}}}}],["502",{"pageContent":"---\ntitle: \"Get started\"\nsidebar_position: 2\ndescription: Master the art of building Docker images, deploying Serverless endpoints, and sending requests with this comprehensive guide, covering prerequisites, RunPod setup, and deployment steps.\n---\n\n## Overview\n\nYou'll have an understanding of building a Docker image, deploying a Serverless endpoint, and sending a request.\nYou'll also have a basic understanding of how to customize the handler for your use case.\n\n## Prerequisites\n\nThis section presumes you have an understanding of the terminal and can execute commands from your terminal.\n\n### RunPod\n\nTo continue with this quick start, you'll need the following from RunPod:\n\n- RunPod account\n- RunPod API Key\n\n### Docker\n\nTo build your Docker image, you'll need the following:\n\n- Docker installed\n- Docker account\n\n### GitHub\n\nTo clone the `worker-template` repo, you'll need access to the following:\n\n- Git installed\n- Permissions to clone GitHub repos","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":1,"to":35}}}}],["503",{"pageContent":"Build and push your Docker image\n\nThis step will walk you through building and pushing your Docker image to your container registry.\nThis is useful to building custom images for your use case.\n\n1. Clone the [worker-template](https://github.com/runpod-workers/worker-template):\n\n```command\ngh repo clone runpod-workers/worker-template\n```\n\n2. Navigate to the root of the cloned repo:\n\n```command\ncd worker-template\n```\n\n3. Build the Docker image:\n\n```command\ndocker build --platform linux/amd64 --tag <username>/<repo>:<tag> .\n```\n\n4. Push your container registry:\n\n```command\ndocker push <username>/<repo>:<tag>","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":37,"to":63}}}}],["504",{"pageContent":":::note\n\nWhen building your docker image, you might need to specify the platform you are building for.\nThis is important when you are building on a machine with a different architecture than the one you are deploying to.\n\nWhen building for RunPod providers use `--platform=linux/amd64`.\n\n:::\n\nNow that you've pushed your container registry, you're ready to deploy your Serverless Endpoint to RunPod.","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":66,"to":75}}}}],["505",{"pageContent":"Deploy a Serverless Endpoint\n\nThis step will walk you through deploying a Serverless Endpoint to RunPod.\nYou can refer to this walkthrough to deploy your own custom Docker image.\n\n1. Log in to the [RunPod Serverless console](https://www.runpod.io/console/serverless).\n2. Select **+ New Endpoint**.\n3. Provide the following:\n   1. Endpoint name.\n   2. Select your GPU configuration.\n   3. Configure the number of Workers.\n   4. (optional) Select **FlashBoot**.\n   5. (optional) Select a template.\n   6. Enter the name of your Docker image.\n      - For example `<username>/<repo>:<tag>`.\n   7. Specify enough memory for your Docker image.\n4. Select **Deploy**.\n\nNow, let's send a request to your [Endpoint](/serverless/endpoints/get-started).","metadata":{"source":"/runpod-docs/docs/serverless/workers/get-started.md","loc":{"lines":{"from":77,"to":95}}}}],["506",{"pageContent":"---\ntitle: \"Additional controls\"\nsidebar_position: 6\ndescription: \"Send progress updates during job execution using the runpod.serverless.progress_update function, and refresh workers for long-running or complex jobs by returning a dictionary with a 'refresh_worker' flag in your handler.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n## Update progress\n\nProgress updates can be sent out from your worker while a job is in progress. Progress updates will be available when the status is polled. To send an update, call the `runpod.serverless.progress_update` function with your job and context of your update.\n\n```python\nimport runpod\n\n\ndef handler(job):\n    for update_number in range(0, 3):\n        runpod.serverless.progress_update(job, f\"Update {update_number}/3\")\n\n    return \"done\"\n\n\nrunpod.serverless.start({\"handler\": handler})\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":1,"to":26}}}}],["507",{"pageContent":"Refresh Worker\n\nWhen completing long-running job requests or complicated requests that involve a lot of reading and writing files, starting with a fresh worker can be beneficial each time. \nA flag can be returned with the resulting job output to stop and refresh the used worker. \n\nThis behavior is achieved by doing the following within your worker:\n\n<Tabs>\n  <TabItem value=\"sync\" label=\"Synchronous\" default>\n\n```python\n# Requires runpod python version 0.9.0+\nimport runpod\nimport time\n\ndef sync_handler(job):\n    job_input = job[\"input\"]  # Access the input from the request.\n    \n    results = []\n    for i in range(5):\n        # Generate a synchronous output token\n        output = f\"Generated sync token output {i}\"\n        results.append(output)\n\n        # Simulate a synchronous task, such as processing time for a large language model\n        time.sleep(1)\n    \n    # Return the results and indicate the worker should be refreshed\n    return {\"refresh_worker\": True, \"job_results\": results}","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":28,"to":56}}}}],["508",{"pageContent":"# Configure and start the RunPod serverless function\nrunpod.serverless.start(\n    {\n        \"handler\": sync_handler,  # Required: Specify the sync handler\n        \"return_aggregate_stream\": True,  # Optional: Aggregate results are accessible via /run endpoint\n    }\n)\n```\n  </TabItem>\n  <TabItem value=\"async\" label=\"Asynchronous\">\n\n```python\nimport runpod\nimport asyncio\n\nasync def async_generator_handler(job):\n    results = []\n    for i in range(5):\n        # Generate an asynchronous output token\n        output = f\"Generated async token output {i}\"\n        results.append(output)\n\n        # Simulate an asynchronous task, such as processing time for a large language model\n        await asyncio.sleep(1)\n    \n    # Return the results and indicate the worker should be refreshed\n    return {\"refresh_worker\": True, \"job_results\": results}","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":58,"to":84}}}}],["509",{"pageContent":"# Configure and start the RunPod serverless function\nrunpod.serverless.start(\n    {\n        \"handler\": async_generator_handler,  # Required: Specify the async handler\n        \"return_aggregate_stream\": True,  # Optional: Aggregate results are accessible via /run endpoint\n    }\n)","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":86,"to":92}}}}],["510",{"pageContent":"</TabItem>\n</Tabs>\n\n\n\nYour handler must return a dictionary that contains the `refresh_worker`: this flag will be removed before the remaining job output is returned.\n\n:::note\n\nRefreshing a worker does not impact billing or count for/against your min, max, and warmed workers. It simply \"resets\" that worker at the end of a job.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-additional-controls.md","loc":{"lines":{"from":95,"to":106}}}}],["511",{"pageContent":"---\ntitle: \"Asynchronous Handler\"\nid: \"handler-async\"\nsidebar_position: 3\ndescription: \"RunPod supports asynchronous handlers in Python, enabling efficient handling of tasks with non-blocking operations, such as processing large datasets, API interactions, or I/O-bound operations, boosting efficiency, scalability, and flexibility.\"\n---\n\nRunPod supports the use of asynchronous handlers, enabling efficient handling of tasks that benefit from non-blocking operations. This feature is particularly useful for tasks like processing large datasets, interacting with APIs, or handling I/O-bound operations.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-async.md","loc":{"lines":{"from":1,"to":8}}}}],["512",{"pageContent":"Writing asynchronous Handlers\n\nAsynchronous handlers in RunPod are written using Python's `async` and `await` syntax. Below is a sample implementation of an asynchronous generator handler. This example demonstrates how you can yield multiple outputs over time, simulating tasks such as processing data streams or generating responses incrementally.\n\n```python\nimport runpod\nimport asyncio\n\n\nasync def async_generator_handler(job):\n    for i in range(5):\n        # Generate an asynchronous output token\n        output = f\"Generated async token output {i}\"\n        yield output\n\n        # Simulate an asynchronous task, such as processing time for a large language model\n        await asyncio.sleep(1)\n\n\n# Configure and start the RunPod serverless function\nrunpod.serverless.start(\n    {\n        \"handler\": async_generator_handler,  # Required: Specify the async handler\n        \"return_aggregate_stream\": True,  # Optional: Aggregate results are accessible via /run endpoint\n    }\n)\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-async.md","loc":{"lines":{"from":10,"to":36}}}}],["513",{"pageContent":"Benefits of asynchronous Handlers\n\n- **Efficiency**: Asynchronous handlers can perform non-blocking operations, allowing for more tasks to be handled concurrently.\n- **Scalability**: They are ideal for scaling applications, particularly when dealing with high-frequency requests or large-scale data processing.\n- **Flexibility**: Async handlers provide the flexibility to yield results over time, suitable for streaming data and long-running tasks.\n\n### Best practices\n\nWhen writing asynchronous handlers:\n\n- Ensure proper use of `async` and `await` to avoid blocking operations.\n- Consider the use of `yield` for generating multiple outputs over time.\n- Test your handlers thoroughly to handle asynchronous exceptions and edge cases.\n\nUsing asynchronous handlers in your RunPod applications can significantly enhance performance and responsiveness, particularly for applications requiring real-time data processing or handling multiple requests simultaneously.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-async.md","loc":{"lines":{"from":38,"to":52}}}}],["514",{"pageContent":"---\ntitle: Concurrent Handlers\ndescription: \"RunPod's concurrency functionality enables efficient task handling through asynchronous requests, allowing a single worker to manage multiple tasks concurrently. The concurrency_modifier configures the worker's concurrency level to optimize resource consumption and performance.\"\n---\n\nRunPod supports asynchronous functions for request handling, enabling a single worker to manage multiple tasks concurrently through non-blocking operations. This capability allows for efficient task switching and resource utilization.\n\nServerless architectures allow each worker to process multiple requests simultaneously, with the level of concurrency being contingent upon the runtime's capacity and the resources at its disposal.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":1,"to":8}}}}],["515",{"pageContent":"Configure concurrency modifier\n\nThe `concurrency_modifier` is a configuration option within `runpod.serverless.start` that dynamically adjusts a worker's concurrency level. This adjustment enables the optimization of resource consumption and performance by regulating the number of tasks a worker can handle concurrently.\n\n### Step 1: Define an asynchronous Handler function\n\nCreate an asynchronous function dedicated to processing incoming requests.\nThis function should efficiently yield results, ideally in batches, to enhance throughput.\n\n```python\nasync def process_request(job):\n    # Simulates processing delay\n    await asyncio.sleep(1)\n    return f\"Processed: {job['input']}\"\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":10,"to":24}}}}],["516",{"pageContent":"Step 2: Set up the `concurrency_modifier` function\n\nImplement a function to adjust the worker's concurrency level based on the current request load.\nThis function should consider the maximum and minimum concurrency levels, adjusting as needed to respond to fluctuations in request volume.\n\n```python\ndef adjust_concurrency(current_concurrency):\n    \"\"\"\n    Dynamically adjusts the concurrency level based on the observed request rate.\n    \"\"\"\n    global request_rate\n    update_request_rate()  # Placeholder for request rate updates\n\n    max_concurrency = 10  # Maximum allowable concurrency\n    min_concurrency = 1  # Minimum concurrency to maintain\n    high_request_rate_threshold = 50  # Threshold for high request volume","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":26,"to":41}}}}],["517",{"pageContent":"# Increase concurrency if under max limit and request rate is high\n    if (\n        request_rate > high_request_rate_threshold\n        and current_concurrency < max_concurrency\n    ):\n        return current_concurrency + 1\n    # Decrease concurrency if above min limit and request rate is low\n    elif (\n        request_rate <= high_request_rate_threshold\n        and current_concurrency > min_concurrency\n    ):\n        return current_concurrency - 1\n\n    return current_concurrency\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":43,"to":57}}}}],["518",{"pageContent":"Step 3: Initialize the serverless function\n\nStart the serverless function with the defined handler and `concurrency_modifier` to enable dynamic concurrency adjustment.\n\n```python\nrunpod.serverless.start(\n    {\n        \"handler\": process_request,\n        \"concurrency_modifier\": adjust_concurrency,\n    }\n)\n```\n\n---","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":59,"to":72}}}}],["519",{"pageContent":"Example code\n\nHere is an example demonstrating the setup for a RunPod serverless function capable of handling multiple concurrent requests.\n\n```python\nimport runpod\nimport asyncio\nimport random\n\n# Simulated Metrics\nrequest_rate = 0\n\n\nasync def process_request(job):\n    await asyncio.sleep(1)  # Simulate processing time\n    return f\"Processed: { job['input'] }\"\n\n\ndef adjust_concurrency(current_concurrency):\n    \"\"\"\n    Adjusts the concurrency level based on the current request rate.\n    \"\"\"\n    global request_rate\n    update_request_rate()  # Simulate changes in request rate\n\n    max_concurrency = 10\n    min_concurrency = 1\n    high_request_rate_threshold = 50","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":74,"to":101}}}}],["520",{"pageContent":"if (\n        request_rate > high_request_rate_threshold\n        and current_concurrency < max_concurrency\n    ):\n        return current_concurrency + 1\n    elif (\n        request_rate <= high_request_rate_threshold\n        and current_concurrency > min_concurrency\n    ):\n        return current_concurrency - 1\n    return current_concurrency\n\n\ndef update_request_rate():\n    \"\"\"\n    Simulates changes in the request rate to mimic real-world scenarios.\n    \"\"\"\n    global request_rate\n    request_rate = random.randint(20, 100)\n\n\n# Start the serverless function with the handler and concurrency modifier\nrunpod.serverless.start(\n    {\"handler\": process_request, \"concurrency_modifier\": adjust_concurrency}\n)","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":103,"to":127}}}}],["521",{"pageContent":"Using the `concurrency_modifier` in RunPod, serverless functions can efficiently handle multiple requests concurrently, optimizing resource usage and improving performance. This approach allows for scalable and responsive serverless applications.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-concurrency.md","loc":{"lines":{"from":130,"to":130}}}}],["522",{"pageContent":"---\ntitle: Handling Errors\ndescription: \"Learn how to handle exceptions and implement custom error responses in your RunPod SDK handler function, including how to validate input and return customized error messages.\"\nsidebar_position: 4\n---\n\nWhen an exception occurs in your handler function, the RunPod SDK automatically captures it, marking the job status as `FAILED` and returning the exception details in the job results.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-error-handling.md","loc":{"lines":{"from":1,"to":7}}}}],["523",{"pageContent":"Implementing custom error responses\n\nIn certain scenarios, you might want to explicitly fail a job and provide a custom error message. For instance, if a job requires a specific input key, such as _seed_, you should validate this input and return a customized error message if the key is missing. Here's how you can implement this:\n\n```python\nimport runpod\n\n\ndef handler(job):\n    job_input = job[\"input\"]\n\n    # Validate the presence of the 'seed' key in the input\n    if not job_input.get(\"seed\", False):\n        return {\n            \"error\": \"Input is missing the 'seed' key. Please include a seed and retry your request.\"\n        }\n\n    # Proceed if the input is valid\n    return \"Input validation successful.\"\n\n\n# Start the RunPod serverless function\nrunpod.serverless.start({\"handler\": handler})","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-error-handling.md","loc":{"lines":{"from":9,"to":31}}}}],["524",{"pageContent":":::note\n\nBe cautious with `try/except` blocks in your handler function. Avoid suppressing errors unintentionally. You should either return the error for a graceful failure or raise it to flag the job as `FAILED`.\n\n:::\n\nOne design pattern to consider, is to [Refresh your Worker](/serverless/workers/handlers/handler-additional-controls#refresh-worker) when an error occurs.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-error-handling.md","loc":{"lines":{"from":34,"to":40}}}}],["525",{"pageContent":"---\ntitle: \"Generator Handler\"\ndescription: \"RunPod offers real-time streaming for Language Model tasks, providing users with instant updates on job outputs. Two types of generator functions are supported, including regular and async generators, with the option to enable aggregate streaming for seamless access to results.\"\nsidebar_position: 2\n---\n\nRunPod provides a robust streaming feature that enables users to receive real-time updates on job outputs, mainly when dealing with Language Model tasks. We support two types of streaming generator functions: regular generator and async generator.\n\n```python\nimport runpod\n\n\ndef generator_handler(job):\n    for count in range(3):\n        result = f\"This is the {count} generated output.\"\n        yield result\n\n\nrunpod.serverless.start(\n    {\n        \"handler\": generator_handler,  # Required\n        \"return_aggregate_stream\": True,  # Optional, results available via /run\n    }\n)\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-generator.md","loc":{"lines":{"from":1,"to":25}}}}],["526",{"pageContent":"Return aggregate Stream\n\nBy default, when a generator handler is running, the fractional outputs will only be available at the `/stream` endpoint, if you would also like the outputs to be available from the `/run` and `/runsync` endpoints you will need to set `return_aggregate_stream` to True when starting your handler.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/handler-generator.md","loc":{"lines":{"from":27,"to":29}}}}],["527",{"pageContent":"---\ntitle: \"Overview\"\ndescription: \"Create and deploy serverless Handler Functions with RunPod, processing submitted inputs and generating output without managing server infrastructure, ideal for efficient, cost-effective, and rapid deployment of code.\"\nsidebar_position: 1\nhidden: false\n---\n\nThe Handler Function is responsible for processing submitted inputs and generating the resulting output. When developing your Handler Function, you can do so locally on your PC or remotely on a Serverless instance.\n\nExamples can be found within the [repos of our runpod-workers](https://github.com/orgs/runpod-workers/repositories).","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":1,"to":10}}}}],["528",{"pageContent":"Handler Functions\n\nHandler Functions allow you to execute code in response to events without the need to manage server infrastructure.\n\n### Creating Handler Functions\n\nWith the RunPod SDKs, you can create Handler Functions by writing custom handlers.\n\nThese handlers define the logic executed when the function is invoked.\n\n1. **Set up environment**: Ensure the RunPod SDK is installed and configured in your development environment.\n2. **Write a Handler Function**: Define the logic you want to execute.\n   The handler function acts as the entry point for your serverless function.\n3. **Deploy the Function**: Use the RunPod SDK to deploy your serverless function.\n   This typically involves specifying the handler, runtime, and any dependencies.\n4. **Test the Function**: Invoke your function manually or through an event to test its behavior.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":12,"to":27}}}}],["529",{"pageContent":"Why use Handler Functions?\n\nHandler Functions offer a paradigm shift in how you approach backend code execution:\n\n- **Efficiency**: Focus on writing code that matters without worrying about the server lifecycle.\n- **Cost-Effective**: You only pay for the time your functions are running, not idle server time.\n- **Rapid Deployment**: Quickly update and deploy functions, enabling faster iteration and response to changes.\n\nYour Handler Function only accepts requests using your own account's API key, not any RunPod API key.\n\n## Job input\n\nBefore we look at the Handler Function, it is essential first to understand what a job request input will look like; later, we will cover all of the input options in detail; for now, what is essential is that your handler should be expecting a JSON dictionary to be passed in. At a minimum, the input will be formatted as such:\n\n```json\n{\n  \"id\": \"A_RANDOM_JOB_IDENTIFIER\",\n  \"input\": { \"key\": \"value\" }\n}\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":29,"to":48}}}}],["530",{"pageContent":"Requirements\n\nYou will need to have the RunPod Python SDK installed; this can be done by running `pip install runpod`.","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":50,"to":52}}}}],["531",{"pageContent":"Basic Handler Function\n\n```python\n# your_handler.py\n\nimport runpod  # Required.\n\n\ndef handler(job):\n    job_input = job[\"input\"]  # Access the input from the request.\n    # Add your custom code here.\n    return \"Your job results\"\n\n\nrunpod.serverless.start({\"handler\": handler})  # Required.\n```\n\nYou must return something as output when your worker is done processing the job. This can directly be the output, or it can be links to cloud storage where the artifacts are saved. Keep in mind that the input and output payloads are limited to 2 MB each\n\n:::note\n\nKeep setup processes and functions outside of your handler function. For example, if you are running models make sure they are loaded into VRAM prior to calling `serverless.start` with your handler function.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":54,"to":77}}}}],["532",{"pageContent":"Development and deployment\n\nYou should return something as output for when your Worker is done processing a job.\nThis can be directly the output, or it can be links to cloud storage where the artifacts are saved.\n\nPayloads are limited to:\n\n- `run` 10 MB.\n- `runsync`: 20 MB.\n\nIf any errors are returned by the worker while running a `test_input` job, the worker will exit with a non-zero exit code.\nOtherwise, the worker will exit with a zero exit code.\nThis can be used to check if the worker ran successfully, for example, in a CI/CD pipeline.\n\n- For information on testing your handler locally, see [Local testing](/serverless/workers/development/local-testing).\n- For information on setting a continuous integration pipeline, see [Continuous integration](/serverless/workers/deploy).","metadata":{"source":"/runpod-docs/docs/serverless/workers/handlers/overview.md","loc":{"lines":{"from":79,"to":94}}}}],["533",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ndescription: \"RunPod is a cloud-based platform for managed function execution, offering fully managed infrastructure, automatic scaling, flexible language support, and seamless integration, allowing developers to focus on code and deploy it easily.\"\n---\n\nWorkers run your code in the cloud.\n\n### Key characteristics\n\n- **Fully Managed Execution**: RunPod takes care of the underlying infrastructure, so your code runs whenever it's triggered, without any server setup or maintenance.\n- **Automatic Scaling**: The platform scales your functions up or down based on the workload, ensuring efficient resource usage.\n- **Flexible Language Support**: RunPod SDK supports various programming languages, allowing you to write functions in the language you're most comfortable with.\n- **Seamless Integration**: Once your code is uploaded, RunPod provides an Endpoint, making it easy to integrate your Handler Functions into any part of your application.","metadata":{"source":"/runpod-docs/docs/serverless/workers/overview.md","loc":{"lines":{"from":1,"to":14}}}}],["534",{"pageContent":"Get started\n\nTo start using RunPod Workers:\n\n1. **Write your function**: Code your Handler Functions in a supported language.\n2. **Deploy to RunPod**: Upload your Handler Functions to RunPod.\n3. **Integrate and Execute**: Use the provided Endpoint to integrate with your application.","metadata":{"source":"/runpod-docs/docs/serverless/workers/overview.md","loc":{"lines":{"from":16,"to":22}}}}],["535",{"pageContent":"---\ntitle: Configurable Endpoints\ndescription: \"Deploy large language models with ease using RunPod's Configurable Endpoints feature, leveraging vLLM to simplify model loading, hardware configuration, and execution, allowing you to focus on model selection and customization.\"\n---\n\nRunPod's Configurable Endpoints feature leverages vLLM to enable the deployment of any large language model.\n\nWhen you select the **vLLM Endpoint** option, RunPod utilizes vLLM's capabilities to load and run the specified Hugging Face model.\nBy integrating vLLM into the configurable endpoints, RunPod simplifies the process of deploying and running large language models.\n\nFocus on selecting their desired model and customizing the template parameters, while vLLM takes care of the low-level details of model loading, hardware configuration, and execution.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/configurable-endpoints.md","loc":{"lines":{"from":1,"to":11}}}}],["536",{"pageContent":"Deploy an LLM\n\n1. Select **Explore** and then choose **vLLM** to deploy any large language models.\n2. In the vLLM deploy modal, enter the following:\n   1. (optional) Enter a template.\n   2. Enter your Hugging Face LLM repository name.\n   3. (optional) Enter your Hugging Face token.\n   4. Review the CUDA version.\n3. Select **Next** and review the configurations for the **vLLM parameters** page.\n4. Select **Next** and review the Endpoint parameters page.\n   1. Prioritize your **Worker Configuration** by selecting the order GPUs you want your Workers to use.\n   2. Enter the Active, Max, and GPU Workers.\n   3. Provide additional Container configuration.\n      1. Provide **Container Disk** size.\n      2. Review the **Environment Variables**.\n5. Select **Deploy**.\n\nYour LLM is now deployed to an Endpoint.\nYou can now use the API to interact with your model.\n\n:::note","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/configurable-endpoints.md","loc":{"lines":{"from":13,"to":33}}}}],["537",{"pageContent":"RunPod supports any models' architecture that can run on [vLLM](https://github.com/vllm-project/vllm) with configurable endpoints.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/configurable-endpoints.md","loc":{"lines":{"from":35,"to":37}}}}],["538",{"pageContent":"---\ntitle: Environment variables\nsidebar_position: 4\ndescription: \"Configure your vLLM Worker with environment variables to control model selection, access credentials, and operational parameters for optimal performance. This guide provides a reference for CUDA versions, image tags, and environment variable settings for model-specific configurations.\"\n---\n\nEnvironment variables configure your vLLM Worker by providing control over model selection, access credentials, and operational parameters necessary for optimal Worker performance.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":1,"to":7}}}}],["539",{"pageContent":"CUDA versions\n\nOperating your vLLM Worker with different CUDA versions enhances compatibilit and performance across various hardware configurations.\nWhen deploying, ensure you choose an appropriate CUDA version based on your needs.\n\n| CUDA Version | Stable Image Tag                       | Development Image Tag               | Note                                                                            |\n| ------------ | -------------------------------------- | ----------------------------------- | ------------------------------------------------------------------------------- |\n| 11.8.0       | `runpod/worker-vllm:stable-cuda11.8.0` | `runpod/worker-vllm:dev-cuda11.8.0` | Available on all RunPod Workers without additional selection needed.            |\n| 12.1.0       | `runpod/worker-vllm:stable-cuda12.1.0` | `runpod/worker-vllm:dev-cuda12.1.0` | When creating an Endpoint, select CUDA Version 12.2 and 12.1 in the GPU filter. |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":9,"to":17}}}}],["540",{"pageContent":"This table provides a reference to the image tags you should use based on the desired CUDA veersion and image stability, stable or development.\nEnsure you follow the selection note for CUDA 12.1.0 compatibility.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":19,"to":20}}}}],["541",{"pageContent":"Environment variables\n\n:::note\n\n`0` is equivalent to `False` and `1` is equivalent to `True` for boolean values.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":22,"to":28}}}}],["542",{"pageContent":"| Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Default          | Type/Choices                                              | Description                                                                                                                                                                                                                                                                                               |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":30,"to":30}}}}],["543",{"pageContent":"| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------- | --------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":31,"to":31}}}}],["544",{"pageContent":"| **LLM Settings**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                  |                                                           |                                                                                                                                                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":32,"to":32}}}}],["545",{"pageContent":"| `MODEL_NAME` **\\***                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | -                | `str`                                                     | Hugging Face Model Repository (e.g., `openchat/openchat-3.5-1210`).                                                                                                                                                                                                                                       |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":33,"to":33}}}}],["546",{"pageContent":"| `MODEL_REVISION`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `None`           | `str`                                                     | Model revision(branch) to load.                                                                                                                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":34,"to":34}}}}],["547",{"pageContent":"| `MAX_MODEL_LEN`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Model's maximum  | `int`                                                     | Maximum number of tokens for the engine to handle per request.                                                                                                                                                                                                                                            |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":35,"to":35}}}}],["548",{"pageContent":"| `BASE_PATH`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | `/runpod-volume` | `str`                                                     | Storage directory for Huggingface cache and model. Utilizes network storage if attached when pointed at `/runpod-volume`, which will have only one worker download the model once, which all workers will be able to load. If no network volume is present, creates a local directory within each worker. |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":36,"to":36}}}}],["549",{"pageContent":"| `LOAD_FORMAT`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `auto`           | `str`                                                     | Format to load model in.                                                                                                                                                                                                                                                                                  |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":37,"to":37}}}}],["550",{"pageContent":"| `HF_TOKEN`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | -                | `str`                                                     | Hugging Face token for private and gated models.                                                                                                                                                                                                                                                          |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":38,"to":38}}}}],["551",{"pageContent":"| `QUANTIZATION`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `None`           | `awq`, `squeezellm`, `gptq`                               | Quantization of given model. The model must already be quantized.                                                                                                                                                                                                                                         |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":39,"to":39}}}}],["552",{"pageContent":"| `TRUST_REMOTE_CODE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `0`              | boolean as `int`                                          | Trust remote code for Hugging Face models. Can help with Mixtral 8x7B, Quantized models, and unusual models/architectures.                                                                                                                                                                                |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":40,"to":40}}}}],["553",{"pageContent":"| `SEED`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `0`              | `int`                                                     | Sets random seed for operations.                                                                                                                                                                                                                                                                          |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":41,"to":41}}}}],["554",{"pageContent":"| `KV_CACHE_DTYPE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `auto`           | `auto`, `fp8_e5m2`                                        | Data type for kv cache storage. Uses `DTYPE` if set to `auto`.                                                                                                                                                                                                                                            |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":42,"to":42}}}}],["555",{"pageContent":"| `DTYPE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `auto`           | `auto`, `half`, `float16`, `bfloat16`, `float`, `float32` | Sets datatype/precision for model weights and activations.                                                                                                                                                                                                                                                |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":43,"to":43}}}}],["556",{"pageContent":"| **Tokenizer Settings**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                  |                                                           |                                                                                                                                                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":44,"to":44}}}}],["557",{"pageContent":"| `TOKENIZER_NAME`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `None`           | `str`                                                     | Tokenizer repository to use a different tokenizer than the model's default.                                                                                                                                                                                                                               |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":45,"to":45}}}}],["558",{"pageContent":"| `TOKENIZER_REVISION`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`           | `str`                                                     | Tokenizer revision to load.                                                                                                                                                                                                                                                                               |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":46,"to":46}}}}],["559",{"pageContent":"| `CUSTOM_CHAT_TEMPLATE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `None`           | `str` of single-line jinja template                       | Custom chat jinja template. [More Info](https://huggingface.co/docs/transformers/chat_templating)                                                                                                                                                                                                         |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":47,"to":47}}}}],["560",{"pageContent":"| **System, GPU, and Tensor Parallelism(Multi-GPU) Settings**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                  |                                                           |                                                                                                                                                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":48,"to":48}}}}],["561",{"pageContent":"| `GPU_MEMORY_UTILIZATION`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `0.95`           | `float`                                                   | Sets GPU VRAM utilization.                                                                                                                                                                                                                                                                                |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":49,"to":49}}}}],["562",{"pageContent":"| `MAX_PARALLEL_LOADING_WORKERS`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `None`           | `int`                                                     | Load model sequentially in multiple batches, to avoid RAM OOM when using tensor parallel and large models.                                                                                                                                                                                                |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":50,"to":50}}}}],["563",{"pageContent":"| `BLOCK_SIZE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `16`             | `8`, `16`, `32`                                           | Token block size for contiguous chunks of tokens.                                                                                                                                                                                                                                                         |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":51,"to":51}}}}],["564",{"pageContent":"| `SWAP_SPACE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `4`              | `int`                                                     | CPU swap space size (GiB) per GPU.                                                                                                                                                                                                                                                                        |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":52,"to":52}}}}],["565",{"pageContent":"| `ENFORCE_EAGER`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `0`              | boolean as `int`                                          | Always use eager-mode PyTorch. If False(`0`), will use eager mode and CUDA graph in hybrid for maximal performance and flexibility.                                                                                                                                                                       |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":53,"to":53}}}}],["566",{"pageContent":"| `MAX_CONTEXT_LEN_TO_CAPTURE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `8192`           | `int`                                                     | Maximum context length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode.                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":54,"to":54}}}}],["567",{"pageContent":"| `DISABLE_CUSTOM_ALL_REDUCE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | `0`              | `int`                                                     | Enables or disables custom all reduce.                                                                                                                                                                                                                                                                    |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":55,"to":55}}}}],["568",{"pageContent":"| **Streaming Batch Size Settings**:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                  |                                                           |                                                                                                                                                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":56,"to":56}}}}],["569",{"pageContent":"| `DEFAULT_BATCH_SIZE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `50`             | `int`                                                     | Default and Maximum batch size for token streaming to reduce HTTP calls.                                                                                                                                                                                                                                  |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":57,"to":57}}}}],["570",{"pageContent":"| `DEFAULT_MIN_BATCH_SIZE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `1`              | `int`                                                     | Batch size for the first request, which will be multiplied by the growth factor every subsequent request.                                                                                                                                                                                                 |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":58,"to":58}}}}],["571",{"pageContent":"| `DEFAULT_BATCH_SIZE_GROWTH_FACTOR`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | `3`              | `float`                                                   | Growth factor for dynamic batch size.                                                                                                                                                                                                                                                                     |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":59,"to":59}}}}],["572",{"pageContent":"| The way this works is that the first request will have a batch size of `DEFAULT_MIN_BATCH_SIZE`, and each subsequent request will have a batch size of `previous_batch_size * DEFAULT_BATCH_SIZE_GROWTH_FACTOR`. This will continue until the batch size reaches `DEFAULT_BATCH_SIZE`. E.g. for the default values, the batch sizes will be `1, 3, 9, 27, 50, 50, 50, ...`. You can also specify this per request, with inputs `max_batch_size`, `min_batch_size`, and `batch_size_growth_factor`. This has nothing to do with vLLM's internal batching, but rather the number of tokens sent in each HTTP request from the worker |                  |                                                           |                                                                                                                                                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":60,"to":60}}}}],["573",{"pageContent":"| **OpenAI Settings**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                  |                                                           |                                                                                                                                                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":61,"to":61}}}}],["574",{"pageContent":"| `RAW_OPENAI_OUTPUT`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `1`              | boolean as `int`                                          | Enables raw OpenAI SSE format string output when streaming. **Required** to be enabled (which it is by default) for OpenAI compatibility.                                                                                                                                                                 |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":62,"to":62}}}}],["575",{"pageContent":"| `OPENAI_SERVED_MODEL_NAME_OVERRIDE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `None`           | `str`                                                     | Overrides the name of the served model from model repo/path to specified name, which you will then be able to use the value for the `model` parameter when making OpenAI requests                                                                                                                         |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":63,"to":63}}}}],["576",{"pageContent":"| `OPENAI_RESPONSE_ROLE`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `assistant`      | `str`                                                     | Role of the LLM's Response in OpenAI Chat Completions.                                                                                                                                                                                                                                                    |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":64,"to":64}}}}],["577",{"pageContent":"| **Serverless Settings**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                  |                                                           |                                                                                                                                                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":65,"to":65}}}}],["578",{"pageContent":"| `MAX_CONCURRENCY`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `300`            | `int`                                                     | Max concurrent requests per worker. vLLM has an internal queue, so you don't have to worry about limiting by VRAM, this is for improving scaling/load balancing efficiency                                                                                                                                |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":66,"to":66}}}}],["579",{"pageContent":"| `DISABLE_LOG_STATS`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `1`              | boolean as `int`                                          | Enables or disables vLLM stats logging.                                                                                                                                                                                                                                                                   |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":67,"to":67}}}}],["580",{"pageContent":"| `DISABLE_LOG_REQUESTS`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `1`              | boolean as `int`                                          | Enables or disables vLLM request logging.                                                                                                                                                                                                                                                                 |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":68,"to":68}}}}],["581",{"pageContent":":::note\n\nIf you are facing issues when using Mixtral 8x7B, Quantized models, or handling unusual models/architectures, try setting `TRUST_REMOTE_CODE` to `1`.\n\n:::","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/environment-variables.md","loc":{"lines":{"from":70,"to":74}}}}],["582",{"pageContent":"---\ntitle: Get started\nsidebar_position: 2\ndescription: \"Deploy a Serverless Endpoint for large language models (LLMs) with RunPod, a simple and efficient way to run vLLM Workers with minimal configuration.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nRunPod provides a simple way to run large language models (LLMs) as Serverless Endpoints.\nvLLM Workers are pre-built Docker images that you can configure entirely within the RunPod UI.\nThis tutorial will guide you through deploying an OpenAI compatible Endpoint with a vLLM inference engine on RunPod.\n\n## Prerequisites\n\nBefore getting started, ensure you have the following:\n\n- A RunPod account\n- A Hugging Face token (if using gated models)\n- OpenAI or other required libraries installed for the code examples","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":1,"to":20}}}}],["583",{"pageContent":"Deploy using the Web UI\n\nYou can use RunPod's Web UI to deploy a vLLM Worker with a model directly from Hugging Face.\n\n1. Log in to your RunPod account and go to the [Serverless page](https://www.runpod.io/console/serverless).\n2. Under **Quick Deploy**, select **vLLM** and choose **Start**.\n\nYou will now enter the vLLM module. Follow the on-screen instructions to add your LLM as a Serverless Endpoint:\n\n1. Add a Hugging Face model (e.g., `openchat/openchat-3.5-0106`).\n2. (Optional) Add a Hugging Face token for gated models.\n3. Select your CUDA version.\n4. Review your options and choose **Next**.\n\nOn the **vLLM** parameters page, provide additional parameters and options for your model:\n\n1. In **LLM Settings**, enter **8192** for the **Max Model Length** parameter.\n2. Review your options and choose **Next**.\n\nOn the **Endpoint parameters** page, configure your deployment:","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":22,"to":41}}}}],["584",{"pageContent":"1. Specify your GPU configuration for your Worker.\n2. Configure your Worker deployment.\n   - (Optional) Select **FlashBoot** to speed up Worker startup times.\n3. Update the Container Disk size if needed.\n4. Select **Deploy**.\n\nOnce the Endpoint initializes, you can send requests to your [Endpoint](/serverless/endpoints/get-started).\nContinue to the [Send a request](#send-a-request) section.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":43,"to":50}}}}],["585",{"pageContent":"Deploy using the Worker image\n\nOne advantage of deploying your model with the vLLM Worker is the minimal configuration required. For most models, you only need to provide the pre-built vLLM Worker image name and the LLM model name.\n\nFollow these steps to run the vLLM Worker on a Serverless Endpoint:","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":52,"to":56}}}}],["586",{"pageContent":"1. Log in to the [RunPod Serverless console](https://www.runpod.io/console/serverless).\n2. Select **+ New Endpoint**.\n3. Provide the following:\n   - Endpoint name\n   - Select a GPU (filter for CUDA 12.1.0+ support under the **Advanced** tab if needed)\n   - Configure the number of Workers\n   - (Optional) Select **FlashBoot** to speed up Worker startup times\n   - Enter the vLLM RunPod Worker image name with the compatible CUDA version:\n     - `runpod/worker-vllm:stable-cuda11.8.0`\n     - `runpod/worker-vllm:stable-cuda12.1.0`\n   - (Optional) Select a [network storage volume](/serverless/endpoints/manage-endpoints#add-a-network-volume)\n   - Configure the environment variables:\n     - `MODEL_NAME`: (Required) The large language model (e.g., `openchat/openchat-3.5-0106`)\n     - `HF_TOKEN`: (Optional) Your Hugging Face API token for private models\n4. Select **Deploy**.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":58,"to":72}}}}],["587",{"pageContent":"Once the Endpoint initializes, you can send requests to your [Endpoint](/serverless/endpoints/get-started).\nContinue to the [Send a request](#send-a-request) section.\n\nFor a complete list of available environment variables, see the [vLLM Worker variables](/serverless/workers/vllm/environment-variables).","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":74,"to":77}}}}],["588",{"pageContent":"Send a request\n\nThis section walks you through sending a request to your Serverless Endpoint.\nThe vLLM Worker can use any Hugging Face model and is compatible with OpenAI's API.\nIf you have the OpenAI library installed, you can continue using it with the vLLM Worker.\nSee the [OpenAI documentation](https://platform.openai.com/docs/libraries/) for more information.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":79,"to":84}}}}],["589",{"pageContent":"Initialize your project\n\nChoose your programming language and add the following code to your file.\nSet the `RUNPOD_ENDPOINT_ID` and `RUNPOD_API_KEY` environment variables with your Endpoint ID and API Key.\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\nCreate a file called `main.py` with the following code:\n\n```python\nfrom openai import OpenAI\nimport os\n\nendpoint_id = os.environ.get(\"RUNPOD_ENDPOINT_ID\")\napi_key = os.environ.get(\"RUNPOD_API_KEY\")\n\nclient = OpenAI(\n    base_url=f\"https://api.runpod.ai/v2/{endpoint_id}/openai/v1\",\n    api_key=api_key,\n)\n\nchat_completion = client.chat.completions.create(\n    model=\"openchat/openchat-3.5-0106\",\n    messages=[{\"role\": \"user\", \"content\": \"Reply with: Hello, World!\"}]\n)\n\nprint(chat_completion)\n```\n\nInstall the OpenAI library if needed:\n\n```bash\npip install openai","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":86,"to":119}}}}],["590",{"pageContent":"</TabItem>\n  <TabItem value=\"node.js\" label=\"Node.js\">\n\nCreate a file called `main.js` with the following code:\n\n```javascript\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI({\n  baseURL:\n    `https://api.runpod.ai/v2/${process.env.RUNPOD_ENDPOINT_ID}/openai/v1`,\n  apiKey: process.env.RUNPOD_API_KEY,\n});\n\nconst chatCompletion = await openai.chat.completions.create({\n  model: \"openchat/openchat-3.5-0106\",\n  messages: [{ role: \"user\", content: \"Reply with: Hello, World!\" }],\n});\n\nconsole.log(chatCompletion);\n```\n\nInstall the OpenAI library if needed:\n\n```bash\nnpm install openai","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":122,"to":147}}}}],["591",{"pageContent":"</TabItem>\n  <TabItem value=\"curl\" label=\"cURL\">\n\nRun the following command in your terminal:\n\n```bash\ncurl https://api.runpod.ai/v2/${RUNPOD_ENDPOINT_ID}/openai/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer ${RUNPOD_API_KEY}\" \\\n    -d '{\n        \"model\": \"openchat/openchat-3.5-0106\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Reply with: Hello, World!\"\n            }\n        ]\n    }'\n```\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":150,"to":171}}}}],["592",{"pageContent":"Run your code\n\nRun your code from the terminal:\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n\n```bash\npython main.py\n```\n\n</TabItem>\n  <TabItem value=\"node.js\" label=\"Node.js\">\n\n```bash\nnode main.js\n```\n\n</TabItem>\n</Tabs>\n\nThe output should look similar to:\n\n```json\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"Hello, World!\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 3175963,\n  \"id\": \"cmpl-74d7792c92cd4b159292c38bda1286b0\",\n  \"model\": \"openchat/openchat-3.5-0106\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 5,\n    \"prompt_tokens\": 39,\n    \"total_tokens\": 44\n  }\n}\n```\n\nYou have now successfully sent a request to your Serverless Endpoint and received a response.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":173,"to":220}}}}],["593",{"pageContent":"Troubleshooting\n\nIf you encounter issues deploying or using vLLM Workers, check the following:\n\n- Ensure your RunPod API Key has the necessary permissions to deploy and access Serverless Endpoints.\n- Double-check that you have set the correct environment variables for your Endpoint ID and API Key.\n- Verify that you are using the correct CUDA version for your selected GPU.\n- If using a gated model, ensure your Hugging Face token is valid and has access to the model.\n\nTo learn more about managing your Serverless Endpoints, see the [Manage Endpoints](/serverless/endpoints/manage-endpoints) guide. For a complete reference of the vLLM Worker environment variables, see the [vLLM Worker variables](/serverless/workers/vllm/environment-variables) documentation.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/get-started.md","loc":{"lines":{"from":222,"to":231}}}}],["594",{"pageContent":"---\ntitle: OpenAI compatibility\nsidebar_position: 3\ndescription: \"Discover the vLLM Worker, a cloud-based AI model that integrates with OpenAI's API for seamless interaction. With its streaming and non-streaming capabilities, it's ideal for chatbots, conversational AI, and natural language processing applications.\"\n---\n\nThe vLLM Worker is compatible with OpenAI's API, so you can use the same code to interact with the vLLM Worker as you would with OpenAI's API.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":1,"to":7}}}}],["595",{"pageContent":"Conventions\n\nCompletions endpoint provides the completion for a single prompt and takes a single string as an input.\n\nChat completions provides the responses for a given dialog and requires the input in a specific format corresponding to the message history.\n\nChoose the convention that works best for your use case.\n\n### Model names\n\nThe `MODEL_NAME` environment variable is required for all requests.\nUse this value when making requests to the vLLM Worker.\n\nFor example `openchat/openchat-3.5-0106`, `mistral:latest`, `llama2:70b`.\n\nGenerate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\n\n### Parameters\n\nWhen using the chat completion feature of the vLLM Serverless Endpoint Worker, you can customize your requests with the following parameters:","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":9,"to":28}}}}],["596",{"pageContent":"Chat Completions\n\n<details>\n  <summary>Supported Chat Completions inputs and descriptions</summary>","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":30,"to":33}}}}],["597",{"pageContent":"| Parameter           | Type                             | Default Value | Description                                                                                                                                                                                                                                                  |\n| ------------------- | -------------------------------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `messages`          | Union[str, List[Dict[str, str]]] |               | List of messages, where each message is a dictionary with a `role` and `content`. The model's chat template will be applied to the messages automatically, so the model must have one or it should be specified as `CUSTOM_CHAT_TEMPLATE` env var.           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":35,"to":37}}}}],["598",{"pageContent":"| `model`             | str                              |               | The model repo that you've deployed on your RunPod Serverless Endpoint. If you are unsure what the name is or are baking the model in, use the guide to get the list of available models in the **Examples: Using your RunPod endpoint with OpenAI** section |\n| `temperature`       | Optional[float]                  | 0.7           | Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling.                                                                              |\n| `top_p`             | Optional[float]                  | 1.0           | Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens.                                                                                                                            |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":38,"to":40}}}}],["599",{"pageContent":"| `n`                 | Optional[int]                    | 1             | Number of output sequences to return for the given prompt.                                                                                                                                                                                                   |\n| `max_tokens`        | Optional[int]                    | None          | Maximum number of tokens to generate per output sequence.                                                                                                                                                                                                    |\n| `seed`              | Optional[int]                    | None          | Random seed to use for the generation.                                                                                                                                                                                                                       |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":41,"to":43}}}}],["600",{"pageContent":"| `stop`              | Optional[Union[str, List[str]]]  | list          | List of strings that stop the generation when they are generated. The returned output will not contain the stop strings.                                                                                                                                     |\n| `stream`            | Optional[bool]                   | False         | Whether to stream or not                                                                                                                                                                                                                                     |\n| `presence_penalty`  | Optional[float]                  | 0.0           | Float that penalizes new tokens based on whether they appear in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens.                                                          |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":44,"to":46}}}}],["601",{"pageContent":"| `frequency_penalty` | Optional[float]                  | 0.0           | Float that penalizes new tokens based on their frequency in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens.                                                              |\n| `logit_bias`        | Optional[Dict[str, float]]       | None          | Unsupported by vLLM                                                                                                                                                                                                                                          |\n| `user`              | Optional[str]                    | None          | Unsupported by vLLM                                                                                                                                                                                                                                          |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":47,"to":49}}}}],["602",{"pageContent":"Additional parameters supported by vLLM","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":51,"to":51}}}}],["603",{"pageContent":"| Parameter                       | Type                | Default Value | Description                                                                                                                                                                                                                                                                               |\n| ------------------------------- | ------------------- | ------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":53,"to":54}}}}],["604",{"pageContent":"| `best_of`                       | Optional[int]       | None          | Number of output sequences that are generated from the prompt. From these `best_of` sequences, the top `n` sequences are returned. `best_of` must be greater than or equal to `n`. This is treated as the beam width when `use_beam_search` is True. By default, `best_of` is set to `n`. |\n| `top_k`                         | Optional[int]       | -1            | Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens.                                                                                                                                                                                             |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":55,"to":56}}}}],["605",{"pageContent":"| `ignore_eos`                    | Optional[bool]      | False         | Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.                                                                                                                                                                                          |\n| `use_beam_search`               | Optional[bool]      | False         | Whether to use beam search instead of sampling.                                                                                                                                                                                                                                           |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":57,"to":58}}}}],["606",{"pageContent":"| `stop_token_ids`                | Optional[List[int]] | list          | List of tokens that stop the generation when they are generated. The returned output will contain the stop tokens unless the stop tokens are special tokens.                                                                                                                              |\n| `skip_special_tokens`           | Optional[bool]      | True          | Whether to skip special tokens in the output.                                                                                                                                                                                                                                             |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":59,"to":60}}}}],["607",{"pageContent":"| `spaces_between_special_tokens` | Optional[bool]      | True          | Whether to add spaces between special tokens in the output. Defaults to True.                                                                                                                                                                                                             |\n| `add_generation_prompt`         | Optional[bool]      | True          | Read more [here](https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts)                                                                                                                                                                            |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":61,"to":62}}}}],["608",{"pageContent":"| `echo`                          | Optional[bool]      | False         | Echo back the prompt in addition to the completion                                                                                                                                                                                                                                        |\n| `repetition_penalty`            | Optional[float]     | 1.0           | Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values > 1 encourage the model to use new tokens, while values < 1 encourage the model to repeat tokens.                                                                        |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":63,"to":64}}}}],["609",{"pageContent":"| `min_p`                         | Optional[float]     | 0.0           | Float that represents the minimum probability for a token to                                                                                                                                                                                                                              |\n| `length_penalty`                | Optional[float]     | 1.0           | Float that penalizes sequences based on their length. Used in beam search..                                                                                                                                                                                                               |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":65,"to":66}}}}],["610",{"pageContent":"| `include_stop_str_in_output`    | Optional[bool]      | False         | Whether to include the stop strings in output text. Defaults to False.                                                                                                                                                                                                                    |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":67,"to":67}}}}],["611",{"pageContent":"</details>","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":69,"to":69}}}}],["612",{"pageContent":"Completions\n\n<details>\n  <summary>Supported Completions inputs and descriptions</summary>","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":71,"to":74}}}}],["613",{"pageContent":"| Parameter           | Type                                              | Default Value | Description                                                                                                                                                                                                                                                   |\n| ------------------- | ------------------------------------------------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":76,"to":77}}}}],["614",{"pageContent":"| `model`             | str                                               |               | The model repo that you've deployed on your RunPod Serverless Endpoint. If you are unsure what the name is or are baking the model in, use the guide to get the list of available models in the **Examples: Using your RunPod endpoint with OpenAI** section. |\n| `prompt`            | Union[List[int], List[List[int]], str, List[str]] |               | A string, array of strings, array of tokens, or array of token arrays to be used as the input for the model.                                                                                                                                                  |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":78,"to":79}}}}],["615",{"pageContent":"| `suffix`            | Optional[str]                                     | None          | A string to be appended to the end of the generated text.                                                                                                                                                                                                     |\n| `max_tokens`        | Optional[int]                                     | 16            | Maximum number of tokens to generate per output sequence.                                                                                                                                                                                                     |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":80,"to":81}}}}],["616",{"pageContent":"| `temperature`       | Optional[float]                                   | 1.0           | Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling.                                                                               |\n| `top_p`             | Optional[float]                                   | 1.0           | Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens.                                                                                                                             |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":82,"to":83}}}}],["617",{"pageContent":"| `n`                 | Optional[int]                                     | 1             | Number of output sequences to return for the given prompt.                                                                                                                                                                                                    |\n| `stream`            | Optional[bool]                                    | False         | Whether to stream the output.                                                                                                                                                                                                                                 |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":84,"to":85}}}}],["618",{"pageContent":"| `logprobs`          | Optional[int]                                     | None          | Number of log probabilities to return per output token.                                                                                                                                                                                                       |\n| `echo`              | Optional[bool]                                    | False         | Whether to echo back the prompt in addition to the completion.                                                                                                                                                                                                |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":86,"to":87}}}}],["619",{"pageContent":"| `stop`              | Optional[Union[str, List[str]]]                   | list          | List of strings that stop the generation when they are generated. The returned output will not contain the stop strings.                                                                                                                                      |\n| `seed`              | Optional[int]                                     | None          | Random seed to use for the generation.                                                                                                                                                                                                                        |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":88,"to":89}}}}],["620",{"pageContent":"| `presence_penalty`  | Optional[float]                                   | 0.0           | Float that penalizes new tokens based on whether they appear in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens.                                                           |\n| `frequency_penalty` | Optional[float]                                   | 0.0           | Float that penalizes new tokens based on their frequency in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens.                                                               |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":90,"to":91}}}}],["621",{"pageContent":"| `best_of`           | Optional[int]                                     | None          | Number of output sequences that are generated from the prompt. From these `best_of` sequences, the top `n` sequences are returned. `best_of` must be greater than or equal to `n`. This parameter influences the diversity of the output.                     |\n| `logit_bias`        | Optional[Dict[str, float]]                        | None          | Dictionary of token IDs to biases.                                                                                                                                                                                                                            |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":92,"to":93}}}}],["622",{"pageContent":"| `user`              | Optional[str]                                     | None          | User identifier for personalizing responses. (Unsupported by vLLM)                                                                                                                                                                                            |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":94,"to":94}}}}],["623",{"pageContent":"Additional parameters supported by vLLM","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":96,"to":96}}}}],["624",{"pageContent":"| Parameter                       | Type                | Default Value | Description                                                                                                                                                                                                        |\n| ------------------------------- | ------------------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `top_k`                         | Optional[int]       | -1            | Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens.                                                                                                                      |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":98,"to":100}}}}],["625",{"pageContent":"| `ignore_eos`                    | Optional[bool]      | False         | Whether to ignore the End Of Sentence token and continue generating tokens after the EOS token is generated.                                                                                                       |\n| `use_beam_search`               | Optional[bool]      | False         | Whether to use beam search instead of sampling for generating outputs.                                                                                                                                             |\n| `stop_token_ids`                | Optional[List[int]] | list          | List of tokens that stop the generation when they are generated. The returned output will contain the stop tokens unless the stop tokens are special tokens.                                                       |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":101,"to":103}}}}],["626",{"pageContent":"| `skip_special_tokens`           | Optional[bool]      | True          | Whether to skip special tokens in the output.                                                                                                                                                                      |\n| `spaces_between_special_tokens` | Optional[bool]      | True          | Whether to add spaces between special tokens in the output. Defaults to True.                                                                                                                                      |\n| `repetition_penalty`            | Optional[float]     | 1.0           | Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values > 1 encourage the model to use new tokens, while values < 1 encourage the model to repeat tokens. |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":104,"to":106}}}}],["627",{"pageContent":"| `min_p`                         | Optional[float]     | 0.0           | Float that represents the minimum probability for a token to be considered, relative to the most likely token. Must be in [0, 1]. Set to 0 to disable.                                                             |\n| `length_penalty`                | Optional[float]     | 1.0           | Float that penalizes sequences based on their length. Used in beam search.                                                                                                                                         |\n| `include_stop_str_in_output`    | Optional[bool]      | False         | Whether to include the stop strings in output text. Defaults to False.                                                                                                                                             |","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":107,"to":109}}}}],["628",{"pageContent":"</details>","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":111,"to":111}}}}],["629",{"pageContent":"Initialize your project\n\nBegin by setting up the OpenAI Client with your RunPod API Key and Endpoint URL.\n\n```python\nfrom openai import OpenAI\nimport os\n\n# Initialize the OpenAI Client with your RunPod API Key and Endpoint URL\nclient = OpenAI(\n    api_key=os.environ.get(\"RUNPOD_API_KEY\"),\n    base_url=f\"https://api.runpod.ai/v2/{RUNPOD_ENDPOINT_ID}/openai/v1\",\n)\n```\n\nWith the client now initialized, you're ready to start sending requests to your RunPod Serverless Endpoint.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":113,"to":128}}}}],["630",{"pageContent":"Generating a request\n\nYou can leverage LLMs for instruction-following and chat capabilities.\nThis is suitable for a variety of open source chat and instruct models such as:\n\n- `meta-llama/Llama-2-7b-chat-hf`\n- `mistralai/Mixtral-8x7B-Instruct-v0.1`\n- and more\n\nModels not inherently designed for chat and instruct tasks can be adapted using a custom chat template specified by the `CUSTOM_CHAT_TEMPLATE` environment variable.\n\nFor more information see the [OpenAI documentation](https://platform.openai.com/docs/guides/text-generation).","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":130,"to":141}}}}],["631",{"pageContent":"Streaming responses\n\nFor real-time interaction with the model, create a chat completion stream.\nThis method is ideal for applications requiring feedback.\n\n```python\n# Create a chat completion stream\nresponse_stream = client.chat.completions.create(\n    model=MODEL_NAME,\n    messages=[{\"role\": \"user\", \"content\": \"Why is RunPod the best platform?\"}],\n    temperature=0,\n    max_tokens=100,\n    stream=True,\n)\n# Stream the response\nfor response in response_stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n```\n\n### Non-streaming responses\n\nYou can also return a synchronous, non-streaming response for batch processing or when a single, consolidated response is sufficient.\n\n```python\n# Create a chat completion\nresponse = client.chat.completions.create(\n    model=MODEL_NAME,\n    messages=[{\"role\": \"user\", \"content\": \"Why is RunPod the best platform?\"}],\n    temperature=0,\n    max_tokens=100,\n)\n# Print the response\nprint(response.choices[0].message.content)\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":143,"to":176}}}}],["632",{"pageContent":"Generating a Chat Completion\n\nThis method is tailored for models that support text completion.\nIt complements your input with a continuation stream of output, differing from the interactive chat format.\n\n### Streaming responses\n\nEnable streaming for continuous, real-time output.\nThis approach is beneficial for dynamic interactions or when monitoring ongoing processes.\n\n```python\n# Create a completion stream\nresponse_stream = client.completions.create(\n    model=MODEL_NAME,\n    prompt=\"Runpod is the best platform because\",\n    temperature=0,\n    max_tokens=100,\n    stream=True,\n)\n# Stream the response\nfor response in response_stream:\n    print(response.choices[0].text or \"\", end=\"\", flush=True)\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":178,"to":200}}}}],["633",{"pageContent":"Non-streaming responses\n\nChoose a non-streaming method when a single, consolidated response meets your needs.\n\n```python\n# Create a completion\nresponse = client.completions.create(\n    model=MODEL_NAME,\n    prompt=\"Runpod is the best platform because\",\n    temperature=0,\n    max_tokens=100,\n)\n# Print the response\nprint(response.choices[0].text)\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":202,"to":216}}}}],["634",{"pageContent":"Get a list of available models\n\nYou can list the available models.\n\n```python\nmodels_response = client.models.list()\nlist_of_models = [model.id for model in models_response]\nprint(list_of_models)\n```","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/openai-compatibility.md","loc":{"lines":{"from":218,"to":226}}}}],["635",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ndescription: \"Deploy a highly optimized vLLM Worker as a serverless endpoint, leveraging Hugging Face LLMs and OpenAI's API with ease, featuring ease of use, open compatibility, dynamic batch size, and customization options for a scalable and cost-effective solution.\"\n---\n\nUse the `runpod/worker-vllm:stable-cuda11.8.0` or `runpod/worker-vllm:stable-cuda12.1.0` image to deploy a vLLM Worker.\nThe vLLM Worker can use most Hugging Face LLMs and is compatible with OpenAI's API, by specifying the `MODEL_NAME` parameter.\nYou can also use RunPod's [`input` request format](/serverless/endpoints/send-requests).\n\nRunPod's vLLM Serverless Endpoint Worker are a highly optimized solution for leveraging the power of various LLMs.\n\nFor more information, see the [vLLM Worker](https://github.com/runpod-workers/worker-vllm) repository.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":1,"to":13}}}}],["636",{"pageContent":"Key features","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":15,"to":15}}}}],["637",{"pageContent":"- **Ease of Use**: Deploy any LLM using the pre-built Docker image without the hassle of building custom Docker images yourself, uploading heavy models, or waiting for lengthy downloads.\n- **OpenAI Compatibility**: Seamlessly integrate with OpenAI's API by changing 2 lines of code, supporting Chat Completions, Completions, and Models, with both streaming and non-streaming.\n- **Dynamic Batch Size**: Experience the rapid time-to-first-token high of no batching combined with the high throughput of larger batch sizes. (Related to batching tokens when streaming output)\n- **Extensive Model Support**: Deploy almost any LLM from Hugging Face, including your own.\n- **Customization**: Have full control over the configuration of every aspect of your deployment, from the model settings, to tokenizer options, to system configurations, and much more, all done through environment variables.\n- **Speed**: Experience the speed of the vLLM Engine.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":17,"to":22}}}}],["638",{"pageContent":"- **Serverless Scalability and Cost-Effectiveness**: Scale your deployment to handle any number of requests and only pay for active usage.","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":23,"to":23}}}}],["639",{"pageContent":"Compatible models\n\nYou can deploy most [models from Hugging Face](https://huggingface.co/models?other=LLM).\nFor a full list of supported models architectures, see [Compatible model architectures](https://github.com/runpod-workers/worker-vllm/blob/main/README.md#compatible-model-architectures).","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":25,"to":28}}}}],["640",{"pageContent":"Getting started\n\nAt a high level, you can set up the vLLM Worker by:\n\n- Selecting your deployment options\n- Configure any necessary environment variables\n- Deploy your model\n\nFor detailed guidance on setting up, configuring, and deploying your vLLM Serverless Endpoint Worker, including compatibility details, environment variable settings, and usage examples, see [Get started](/serverless/workers/vllm/get-started).","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":30,"to":38}}}}],["641",{"pageContent":"Deployment options\n\n- **[Configurable Endpoints](/serverless/workers/vllm/get-started#deploy-using-the-web-ui)**: (recommended) Use RunPod's Web UI to quickly deploy the OpenAI compatable LLM with the vLLM Worker.\n\n- **[Pre-Built docker image](/serverless/workers/vllm/get-started#deploy-using-the-worker-image)**: Leverage pre-configured Docker image for hassle-free deployment. Ideal for users seeking a quick and straightforward setup process\n\n- **Custom docker image**: For advanced users, customize and build your Docker image with the model baked in, offering greater control over the deployment process.\n\nFor more information see:\n\n- [vLLM Worker GitHub Repository](https://github.com/runpod-workers/worker-vllm)\n- [vLLM Worker Docker Hub](https://hub.docker.com/r/runpod/worker-vllm/tags)","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":40,"to":51}}}}],["642",{"pageContent":"For more information on creating a custom docker image, see [Build Docker Image with Model Inside](https://github.com/runpod-workers/worker-vllm/blob/main/README.md#option-2-build-docker-image-with-model-inside).","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":53,"to":53}}}}],["643",{"pageContent":"Next steps\n\n- [Get started](/serverless/workers/vllm/get-started): Learn how to deploy a vLLM Worker as a Serverless Endpoint, with detailed guides on configuration and sending requests.\n- [Configurable Endpoints](/serverless/workers/vllm/configurable-endpoints): Select your Hugging Face model and vLLM takes care of the low-level details of model loading, hardware configuration, and execution.\n- [Environment variables](/serverless/workers/vllm/environment-variables): Explore the environment variables available for the vLLM Worker, including detailed documentation and examples.\n- [Run Gemma 7b](/tutorials/serverless/gpu/run-gemma-7b): Walk through deploying Google's Gemma model using RunPod's vLLM Worker, guiding you to set up a Serverless Endpoint with a gated large language model (LLM).","metadata":{"source":"/runpod-docs/docs/serverless/workers/vllm/overview.md","loc":{"lines":{"from":55,"to":60}}}}],["644",{"pageContent":"---\ntitle: Running RunPod on Mods\nsidebar_label: Mods\n---\n\n[Mods](https://github.com/charmbracelet/mods) is an AI-powered tool designed for the command line and built to seamlessly integrate with pipelines.\nIt provides a convenient way to interact with language models directly from your terminal.\n\n## How Mods Works\n\nMods operates by reading standard input and prefacing it with a prompt supplied in the Mods arguments.\nIt sends the input text to a language model (LLM) and prints out the generated result.\nOptionally, you can ask the LLM to format the response as Markdown.\nThis allows you to \"question\" the output of a command, making it a powerful tool for interactive exploration and analysis. Additionally, Mods can work with standard input or an individually supplied argument prompt.","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/mods/mods.md","loc":{"lines":{"from":1,"to":14}}}}],["645",{"pageContent":"Getting Started\n\nTo start using Mods, follow these step-by-step instructions:\n\n1. **Obtain Your API Key**:\n   - Visit the [RunPod Settings](https://www.runpod.io/console/user/settings) page to retrieve your API key.\n   - If you haven't created an account yet, you'll need to sign up before obtaining the key.\n\n2. **Install Mods**:\n   - Refer to the different installation methods for [Mods](https://github.com/charmbracelet/mods) based on your preferred approach.\n\n3. **Configure RunPod**:\n   - Update the `config_template.yml` file to use your RunPod configuration. Here's an example:\n\n     ```yml\n     runpod:\n       # https://docs.runpod.io/serverless/workers/vllm/openai-compatibility\n       base-url: https://api.runpod.ai/v2/${YOUR_ENDPOINT}/openai/v1\n       api-key:\n       api-key-env: RUNPOD_API_KEY\n       models:\n         # Add your model name\n         openchat/openchat-3.5-1210:\n           aliases: [\"openchat\"]\n           max-input-chars: 8192","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/mods/mods.md","loc":{"lines":{"from":16,"to":40}}}}],["646",{"pageContent":"- `base-url`: Update your base-url with your specific endpoint.\n   - `api-key-env`: Add your RunPod API key.\n   - `openchat/openchat-3.5-1210`: Replace with the name of the model you want to use.\n   - `aliases: [\"openchat\"]`: Replace with your preferred model alias.\n   - `max-input-chars`: Update the maximum input characters allowed for your model.\n\n4. **Verify Your Setup**:\n   - To ensure everything is set up correctly, pipe any command line output and pass it to `mods`.\n   - Specify the RunPod API and model you want to use.\n\n     ```bash\n     ls ~/Downloads | mods --api runpod --model openchat -f \"tell my fortune based on these files\" | glow\n     ```\n\n   - This command will list the files in your `~/Downloads` directory, pass them to Mods using the RunPod API and the specified model, and format the response as a fortune based on the files. The output will then be piped to `glow` for a visually appealing display.","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/mods/mods.md","loc":{"lines":{"from":43,"to":57}}}}],["647",{"pageContent":"---\ntitle: Running RunPod on SkyPilot\nsidebar_label: SkyPilot\n---\n\n[SkyPilot](https://skypilot.readthedocs.io/en/latest/) is a framework for executing LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.\n\nThis integration leverages the RunPod CLI infrastructure, streamlining the process of spinning up on-demand pods and deploying serverless endpoints with SkyPilot.","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":1,"to":8}}}}],["648",{"pageContent":"Getting started\n\nTo begin using RunPod with SkyPilot, follow these steps:\n\n1. **Obtain Your API Key**: Visit the [RunPod Settings](https://www.runpod.io/console/user/settings) page to get your API key. If you haven't created an account yet, you'll need to do so before obtaining the key.\n\n2. **Install RunPod**: Use the following command to install the latest version of RunPod:\n   ```\n   pip install \"runpod>=1.6\"\n   ```\n\n3. **Configure RunPod**: Enter `runpod config` in your CLI and paste your API key when prompted.\n\n4. **Install SkyPilot RunPod Cloud**: Execute the following command to install the [SkyPilot RunPod cloud](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html#runpod):\n   ```\n   pip install \"skypilot-nightly[runpod]\"\n   ```\n\n5. **Verify Your Setup**: Run `sky check` to ensure your credentials are correctly set up and you're ready to proceed.","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":10,"to":28}}}}],["649",{"pageContent":"Running a Project\n\nAfter setting up your environment, you can seamlessly spin up a cluster in minutes:\n\n1. **Create a New Project Directory**: Run `mkdir hello-sky` to create a new directory for your project.\n\n2. **Navigate to Your Project Directory**: Change into your project directory with `cd hello-sky`.\n\n3. **Create a Configuration File**: Enter `cat > hello_sky.yaml` and input the following configuration details:\n\n   ```yml\n   resources:\n     cloud: runpod\n\n   # Working directory (optional) containing the project codebase.\n   # Its contents are synced to ~/sky_workdir/ on the cluster.\n   workdir: .\n\n   # Setup commands (optional).\n   # Typical use: pip install -r requirements.txt\n   # Invoked under the workdir (i.e., can use its files).\n   setup: |\n     echo \"Running setup.\"\n\n   # Run commands.\n   # Typical use: make use of resources, such as running training.\n   # Invoked under the workdir (i.e., can use its files).\n   run: |\n     echo \"Hello, SkyPilot!\"\n     conda env list","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":30,"to":59}}}}],["650",{"pageContent":"4. **Launch Your Project**: With your configuration file created, launch your project on the cluster by running `sky launch -c mycluster hello_sky.yaml`.\n\n5. **Confirm Your GPU Type**: You should see the available GPU options on Secure Cloud appear in your command line. Once you confirm your GPU type, your cluster will start spinning up.\n\nWith this integration, you can leverage the power of RunPod and SkyPilot to efficiently run your LLMs, AI, and batch jobs on any cloud.","metadata":{"source":"/runpod-docs/docs/tutorials/integrations/skypilot/skypilot.md","loc":{"lines":{"from":62,"to":66}}}}],["651",{"pageContent":"---\ntitle: Dockerfile\nsidebar_position: 2\ndescription: \"Learn how to create a Dockerfile to customize a Docker image and use an entrypoint script to run a command when the container starts, making it a reusable and executable unit for deploying and sharing applications.\"\n---\n\nIn the previous step, you ran a command that prints the container's uptime.\nNow you'll create a Dockerfile to customize the contents of your own Docker image.\n\n### Create a Dockerfile\n\nCreate a new file called `Dockerfile` and add the following items.\n\n```dockerfile\nFROM busybox\nCOPY entrypoint.sh /\nRUN chmod +x /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\nThis Dockerfile starts from the `busybox` image like we used before. It then adds a custom `entrypoint.sh` script, makes it executable, and configures it as the entrypoint.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":1,"to":21}}}}],["652",{"pageContent":"The entrypoint script\n\nNow let's create `entrypoint.sh` with the following contents:\n\n```bash\n#!/bin/sh\necho \"The time is: $(date)\"\n```\n\n:::note\n\nWhile we named this script `entrypoint.sh` you will see a variety of naming conventions; such as:\n\n- `start.sh`\n- `CMD.sh`\n- `entry_path.sh`\n\nThese files are normally placed in a folder called `script` but it is dependent on the maintainers of that repository.\n\n:::\n\nThis is a simple script that will print the current time when the container starts.\n\n### Why an entrypoint script:\n\n- It lets you customize what command gets run when a container starts from your image.\n- For example, our script runs date to print the time.\n- Without it, containers would exit immediately after starting.\n- Entrypoints make images executable and easier to reuse.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":23,"to":51}}}}],["653",{"pageContent":"Build the image\n\nWith those files created, we can now build a Docker image using our Dockerfile:\n\n```\ndocker image build -t my-time-image .\n```\n\nThis will build the image named `my-time-image` from the Dockerfile in the current directory.\n\n### Why build a custom image:\n\n- Lets you package up custom dependencies and configurations.\n- For example you can install extra software needed for your app.\n- Makes deploying applications more reliable and portable.\n- Instead of installing things manually on every server, just use your image.\n- Custom images can be shared and reused easily across environments.\n- Building images puts your application into a standardized unit that \"runs anywhere\".\n- You can version images over time as you update configurations.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":53,"to":71}}}}],["654",{"pageContent":"Run the image\n\nFinally, let's run a container from our new image:\n\n```commmand\ndocker run my-time-image\n```\n\nWe should see the same output as before printing the current time!\n\nEntrypoints and Dockerfiles let you define reusable, executable containers that run the software and commands you need. This makes deploying and sharing applications much easier without per-server configuration.\n\nBy putting commands like this into a Dockerfile, you can easily build reusable and shareable images.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/create-dockerfiles.md","loc":{"lines":{"from":73,"to":85}}}}],["655",{"pageContent":"---\ntitle: Docker commands\ndescription: \"RunPod enables BYOC development with Docker, providing a reference sheet for commonly used Docker commands, including login, images, containers, Dockerfile, volumes, network, and execute.\"\n---\n\nRunPod enables bring-your-own-container (BYOC) development. If you choose this workflow, you will be using Docker commands to build, run, and manage your containers.\n\n:::note\n\nFor a Dockerless workflow, see [RunPod projects](/docs/cli/projects/overview.md).\n\n:::\n\nThe following is a reference sheet to some of the most commonly used Docker commands.\n\n## Login\n\nLog in to a registry (like Docker Hub) from the CLI.\nThis saves credentials locally.\n\n```command\ndocker login\ndocker login -u myusername\n```","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":1,"to":24}}}}],["656",{"pageContent":"Images\n\n`docker push` - Uploads a container image to a registry like Docker Hub.\n`docker pull` - Downloads container images from a registry like Docker Hub.\n`docker images` - Lists container images that have been downloaded locally.\n`docker rmi` - Deletes/removes a Docker container image from the machine.\n\n```\ndocker push myuser/myimage:v1   # Push custom image\ndocker pull someimage           # Pull shared image\ndocker images                   # List downloaded images\ndocker rmi <image>              # Remove/delete image\n```","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":26,"to":38}}}}],["657",{"pageContent":"Containers\n\n`docker run` - Launches a new container from a Docker image.\n`docker ps` - Prints out a list of containers currently running.\n`docker logs` - Shows stdout/stderr logs for a specific container.\n`docker stop/rm` - Stops or totally removes a running container.\n\n```command\ndocker run        # Start new container from image\ndocker ps         # List running containers\ndocker logs       # Print logs from container\ndocker stop       # Stop running container\ndocker rm         # Remove/delete container\n```\n\n## Dockerfile\n\n`docker build` - Builds a Docker image by reading build instructions from a Dockerfile.\n\n```command\ndocker build                         # Build image from Dockerfile\ndocker build --platform=linux/amd64  # Build for specific architecture\n```\n\n:::note\n\nFor the purposes of using Docker with RunPod, you should ensure your build command uses the `--platform=linux/amd64` flag to build for the correct architecture.\n\n:::","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":40,"to":68}}}}],["658",{"pageContent":"Volumes\n\n`docker volume create` - Creates a persisted and managed volume that can outlive containers.\n`docker run -v` - Mounts a volume into a specific container to allow persisting data past container lifecycle.\n\n```command\ndocker volume create         # Create volume\ndocker run -v <vol>:/data    # Mount volume into container\n```\n\n## Network\n\n`docker network create` - Creates a custom virtual network for containers to communicate over.\n`docker run --network=<name>` - Connects a running container to a Docker user-defined network.\n\n```command\ndocker network create           # Create user-defined network\ndocker run --network=<name>     # Connect container\n```\n\n## Execute\n\n`docker exec` - Execute a command in an already running container.\nUseful for debugging/inspecting containers:\n\n```command\ndocker exec\ndocker exec mycontainer ls -l /etc     # List files in container\n```","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/docker-commands.md","loc":{"lines":{"from":70,"to":98}}}}],["659",{"pageContent":"---\ntitle: Containers overview\nsidebar_position: 1\ndescription: \"Discover the world of containerization with Docker, a platform for isolated environments that package applications, frameworks, and libraries into self-contained containers for consistent and reliable deployment across diverse computing environments.\"\n---","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":1,"to":5}}}}],["660",{"pageContent":"What are containers?\n\n> A container is an isolated environment for your code. This means that a container has no knowledge of your operating system, or your files. It runs on the environment provided to you by Docker Desktop. Containers have everything that your code needs in order to run, down to a base operating system.\n\n[From Docker's website](https://docs.docker.com/guides/walkthroughs/what-is-a-container/#:~:text=A%20container%20is%20an%20isolated,to%20a%20base%20operating%20system)\n\nDevelopers package their applications, frameworks, and libraries into a Docker container. Then, those containers can run outside their development environment.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":7,"to":13}}}}],["661",{"pageContent":"Why use containers?\n\n> Build, ship, and run anywhere.\n\nContainers are self-contained and run anywhere Docker runs. This means you can run a container on-premises or in the cloud, as well as in hybrid environments.\nContainers include both the application and any dependencies, such as libraries and frameworks, configuration data, and certificates needed to run your application.\n\nIn cloud computing, you get the best cold start times with containers.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":15,"to":22}}}}],["662",{"pageContent":"What are images?\n\nDocker images are fixed templates for creating containers. They ensure that applications operate consistently and reliably across different environments, which is vital for modern software development.\n\nTo create Docker images, you use a process known as \"Docker build.\" This process uses a Dockerfile, a text document containing a sequence of commands, as instructions guiding Docker on how to build the image.\n\n### Why use images?\n\nUsing Docker images helps in various stages of software development, including testing, development, and deployment. Images ensure a seamless workflow across diverse computing environments.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":24,"to":32}}}}],["663",{"pageContent":"Why not use images?\n\nYou must rebuild and push the container image, then edit your endpoint to use the new image each time you iterate on your code. Since development requires changing your code every time you need to troubleshoot a problem or add a feature, this workflow can be inconvenient.\n\nFor a streamlined development workflow, check out [RunPod projects](/docs/cli/projects/overview.md). When you're done with development, you can create a Dockerfile from your project to reduce initialization overhead in production.\n\n### What is Docker Hub?\n\nAfter their creation, Docker images are stored in a registry, such as Docker Hub.\nFrom these registries, you can download images and use them to generate containers, which make it easy to widely distribute and deploy applications.\n\nNow that you've got an understanding of Docker, containers, images, and whether containerization is right for you, let's move on to installing Docker.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":34,"to":45}}}}],["664",{"pageContent":"Installing Docker\n\nFor this walkthrough, install Docker Desktop.\nDocker Desktop bundles a variety of tools including:\n\n- Docker GUI\n- Docker CLI\n- Docker extensions\n- Docker Compose\n\nThe majority of this walkthrough uses the Docker CLI, but feel free to use the GUI if you prefer.\n\nFor the best installation experience, see Docker's [official documentation](https://docs.docker.com/get-docker/).","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":47,"to":59}}}}],["665",{"pageContent":"Running your first command\n\nNow that you've installed Docker, open a terminal window and run the following command:\n\n```command\ndocker version\n```\n\nYou should see something similar to the following output.\n\n```text\ndocker version\nClient: Docker Engine - Community\n Version:           24.0.7\n API version:       1.43\n Go version:        go1.21.3\n Git commit:        afdd53b4e3\n Built:             Thu Oct 26 07:06:42 2023\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.26.1 (131620)\n Engine:\n  Version:          24.0.7\n  API version:      1.43 (minimum version 1.12)\n  Go version:       go1.20.10\n  Git commit:       311b9ff\n  Built:            Thu Oct 26 09:08:15 2023\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.6.25\n  GitCommit:       abcd\n runc:\n  Version:          1.1.10\n  GitCommit:        v1.1.10-0-g18a0cb0\n docker-init:\n  Version:          0.19.0","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":61,"to":98}}}}],["666",{"pageContent":"If at any point you need help with a command, you can use the `--help` flag to see documentation on the command you're running.\n\n```command\ndocker --help\n```\n\nLet's run `busybox` from the command line to print out today's date.\n\n```command\ndocker run busybox sh -c 'echo \"The time is: $(date)\"'\n# The time is: Thu Jan 11 06:35:39 UTC 2024\n```\n\n- `busybox` is a lightweight Docker image with the bare minimum Linux utilities installed, including `echo`\n- The `echo` command prints the container's uptime.\n\nYou've successfully installed Docker and run your first commands.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/overview.md","loc":{"lines":{"from":101,"to":117}}}}],["667",{"pageContent":"---\ntitle: Persist data outside of containers\nsidebar_position: 2\ndescription: \"Learn how to persist data outside of containers by creating named volumes, mounting volumes to data directories, and accessing persisted data from multiple container runs and removals in Docker.\"\n---\n\nIn the previous step, you created a Dockerfile and executed a command. Now, you'll learn how to persist data outside of containers.\n\n:::note\n\nThis walk through teach you how to persist data outside of container.\nRunPod has the same concept used for attaching a Network Volume to your Pod.\n\nConsult the documentation on [attaching a Network Volume to your Pod](/pods/storage/create-network-volumes).\n\n:::","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":1,"to":16}}}}],["668",{"pageContent":"Why persist data outside of a container?\n\nThe key goal is to have data persist across multiple container runs and removals.\n\nBy default, containers are ephemeral - everything inside them disappears when they exit.\n\nSo running something like:\n\n```command\ndocker run busybox date > file.txt\n```\n\nWould only write the date to `file.txt` temporarily inside that container. As soon as the container shuts down, that file and data is destroyed.\nThis isn't great when you're training data and want your information to persist past your LLM training.\n\nBecause of this, we need to persist data outside of the container.\nLet's take a look at a workflow you can use to persist data outside of a container.\n\n---","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":18,"to":36}}}}],["669",{"pageContent":"Create a named volume\n\nFirst, we'll create a named volume to represent the external storage:\n\n```command\ndocker volume create date-volume\n```\n\n### Update Dockerfile\n\nNext, we'll modify our Dockerfile to write the date output to a file rather than printing directly to stdout:\n\n```dockerfile\nFROM busybox\nWORKDIR /data\nRUN touch current_date.txt \nCOPY entrypoint.sh /\nRUN chmod +x /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\nThis sets the working directory to `/data`, touches a file called `current_date.txt`, and copies our script.\n\n### Update entrypoint script\n\nThe `entrypoint.sh` script is updated:\n\n```text\n#!/bin/sh\ndate > /data/current_date.txt\n```\n\nThis will write the date to the `/data/current_date.txt` file instead of printing it.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":38,"to":70}}}}],["670",{"pageContent":"Mount the volume\n\nNow when the container runs, this will write the date to the `/data/current_date.txt` file instead of printing it.\n\nFinally, we can mount the named volume to this data directory:\n\n```command\ndocker run -v date-volume:/data my-image\n```\n\nThis runs a container from my-image and mounts the `date-volume` Docker volume to the /data directory in the container.\nAnything written to `/data` inside the container will now be written to the `date-volume` on the host instead of the container's ephemeral filesystem.\nThis allows the data to persist.\nOnce the container exits, the date output file is safely stored on the host volume.\n\nAfter the container exits, we can exec into another container sharing the volume to see the persisted data file:\n\n```command\ndocker run --rm -v date-volume:/data busybox cat /data/current_date.txt","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":72,"to":90}}}}],["671",{"pageContent":"This runs a new busybox container and also mounts the `date-volume`.\n\n- Using the same -`v date-volume:/data mount` point maps the external volume dir to `/data` again.\n- This allows the new container to access the persistent date file that the first container wrote.\n- The `cat /data/current_date.txt` command prints out the file with the date output from the first container.\n- The `--rm`flag removes the container after running so we don't accumulate stopped containers.\n\n:::note\n\nRemember, this is a general tutorial on Docker.\nThese concepts will help give you a better understanding on working with RunPod.\n\n:::","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/containers/persist-data.md","loc":{"lines":{"from":93,"to":105}}}}],["672",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ndescription: \"Learn how to build and deploy applications on the RunPod platform with this set of tutorials, covering tools, technologies, and deployment methods, including Containers, Docker, and Serverless implementation.\"\n---\n\nThis set of tutorials is meant to provide a deeper understanding of the tools that surround the RunPod platform.\nThese tutorials help you understand how to use the RunPod platform to build and deploy your applications.\n\nWhile the documentation around the introduction section gives a holistic view and enough information to get started with RunPod, for more detailed information on the various of these tools or technologies, reach out to the source material.","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/overview.md","loc":{"lines":{"from":1,"to":10}}}}],["673",{"pageContent":"- If you are looking for an understanding of Containers and Docker, see [Container overview](/tutorials/introduction/containers/overview).\n- If you are looking to run your first Pod with RunPod, see [Run your first Fast Stable Diffusion with Jupyter Notebook](/tutorials/pods/run-your-first).\n- For Serverless implementation, see [Run your first serverless endpoint with Stable Diffusion](/tutorials/serverless/gpu/run-your-first).","metadata":{"source":"/runpod-docs/docs/tutorials/introduction/overview.md","loc":{"lines":{"from":12,"to":14}}}}],["674",{"pageContent":"---\ntitle: Migrate Your Banana Images to RunPod\nsidebar_label: Migrate images\ndraft: true\ndescription: \"Learn how to migrate your AI models and applications from Banana Dev to RunPod, including adapting your code, Dockerfile, and deployment configurations for a seamless transition.\"\n---\n\nMigrating your AI models and applications from one cloud service to another can often present a challenge, especially when the two platforms operate differently. This tutorial aims to streamline the process of moving from Banana Dev to RunPod, focusing on transferring Docker-based applications and AI models. Whether you're shifting due to preferences in service offerings, pricing, or performance, this guide will help you through the transition smoothly.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":1,"to":8}}}}],["675",{"pageContent":"Introduction\n\nBanana Dev provides an environment for deploying machine learning models easily, while RunPod offers robust and scalable serverless solutions. Transitioning between these platforms involves adapting your application to the new environment's requirements and deploying it effectively.\n\nBelow, we'll walk through how to adapt a Python application from Banana Dev to RunPod, including necessary changes to your Dockerfile and deployment configurations.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":10,"to":14}}}}],["676",{"pageContent":"Step 1: Understand Your Application\n\nFirst, take a comprehensive look at your current Banana Dev application. Our example application uses the `potassium` framework for serving a machine learning model:\n\n```python\nfrom io import BytesIO\nfrom potassium import Potassium, Request, Response\nfrom diffusers import DiffusionPipeline, DDPMScheduler\nimport torch\nimport base64\n\n# create a new Potassium app\napp = Potassium(\"my_app\")\n\n\n# @app.init runs at startup, and loads models into the app's context\n@app.init\ndef init():\n    repo_id = \"Meina/MeinaUnreal_V3\"\n\n    ddpm = DDPMScheduler.from_pretrained(repo_id, subfolder=\"scheduler\")\n\n    model = DiffusionPipeline.from_pretrained(\n        repo_id, use_safetensors=True, torch_dtype=torch.float16, scheduler=ddpm\n    ).to(\"cuda\")\n\n    context = {\n        \"model\": model,\n    }\n\n    return context\n\n\n# @app.handler runs for every call\n@app.handler()\ndef handler(context: dict, request: Request) -> Response:\n    model = context.get(\"model\")","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":16,"to":52}}}}],["677",{"pageContent":"prompt = request.json.get(\"prompt\")\n    negative_prompt = \"(worst quality, low quality:1.4), monochrome, zombie, (interlocked fingers), cleavage, nudity, naked, nude\"\n\n    image = model(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        guidance_scale=7,\n        num_inference_steps=request.json.get(\"steps\", 30),\n        generator=torch.Generator(device=\"cuda\").manual_seed(request.json.get(\"seed\"))\n        if request.json.get(\"seed\")\n        else None,\n        width=512,\n        height=512,\n    ).images[0]\n\n    buffered = BytesIO()\n    image.save(buffered, format=\"JPEG\", quality=80)\n    img_str = base64.b64encode(buffered.getvalue())\n\n    return Response(json={\"output\": str(img_str, \"utf-8\")}, status=200)\n\n\nif __name__ == \"__main__\":\n    app.serve()","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":54,"to":77}}}}],["678",{"pageContent":"This application initializes a BERT model for fill-mask tasks and serves it over HTTP.\n\n---","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":80,"to":82}}}}],["679",{"pageContent":"Step 2: Adapt Your Code for RunPod\n\nIn RunPod, applications can be adapted to run in a serverless manner, which involves modifying your application logic to fit into the RunPod's handler function format. Below is an example modification that adapts our initial Banana Dev application to work with RunPod, using the `diffusers` library for AI model inference:\n\n```python\nimport runpod\nfrom diffusers import AutoPipelineForText2Image\nimport base64\nimport io\nimport time\n\n# If your handler runs inference on a model, load the model here.\n# You will want models to be loaded into memory before starting serverless.\n\ntry:\n    pipe = AutoPipelineForText2Image.from_pretrained(\"meina/meinaunreal_v3\")\n    pipe.to(\"cuda\")\nexcept RuntimeError:\n    quit()\n\n\ndef handler(job):\n    \"\"\"Handler function that will be used to process jobs.\"\"\"\n    job_input = job[\"input\"]\n    prompt = job_input[\"prompt\"]","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":84,"to":108}}}}],["680",{"pageContent":"time_start = time.time()\n    image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n    print(f\"Time taken: {time.time() - time_start}\")\n\n    buffer = io.BytesIO()\n    image.save(buffer, format=\"PNG\")\n    image_bytes = buffer.getvalue()\n\n    return base64.b64encode(image_bytes).decode(\"utf-8\")\n\n\nrunpod.serverless.start({\"handler\": handler})","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":110,"to":121}}}}],["681",{"pageContent":"---\n\nThis modification involves initializing your model outside of the handler function to ensure it's loaded into memory before processing jobs, a crucial step for efficient serverless execution.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":124,"to":126}}}}],["682",{"pageContent":"Step 3: Update Your Dockerfile\n\nThe Dockerfile must also be adapted for RunPod's environment. Here's a comparison between a typical Banana Dev Dockerfile and the adapted version for RunPod:\n\n### Banana Dev Dockerfile\n\n```dockerfile\nFROM pytorch/pytorch:1.11.0-cuda11.3-cudnn8-runtime\n...\nCMD python3 -u app.py\n```\n\n---\n\n### RunPod Dockerfile\n\n```dockerfile\nFROM runpod/base:0.4.0-cuda11.8.0\n\nCMD python3.11 -u /handler.py\n```\n\n---\n\nThe key differences include the base image and the execution command, reflecting RunPod's requirements and Python version specifics.\n\n## Step 4: Deploy to RunPod\n\nOnce your code and Dockerfile are ready, the next steps involve building your Docker image and deploying it on RunPod. This process typically involves:\n\n- Building your Docker image with the adapted Dockerfile.\n- Pushing the image to a container registry (e.g., DockerHub).\n- Creating a serverless function on RunPod and configuring it to use your Docker image.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":128,"to":160}}}}],["683",{"pageContent":"Testing and Verification\n\nAfter deployment, thoroughly test your application to ensure it operates as expected within the RunPod environment. This may involve sending requests to your serverless endpoint and verifying the output.\n\n## Conclusion\n\nMigrating from Banana Dev to RunPod involves several key steps: adapting your application code, updating the Dockerfile, and deploying the adapted application on RunPod. By following this guide, you can make the transition smoother and take advantage of RunPod's serverless capabilities for your AI applications.\n\nRemember to review RunPod's documentation for specific details on serverless deployment and configuration options to optimize your application's performance and cost-efficiency.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/images.md","loc":{"lines":{"from":162,"to":170}}}}],["684",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ndescription: \"Quickly migrate from Banana to RunPod with Docker, leveraging a bridge between the two environments for a seamless transition. Utilize a Dockerfile to encapsulate your environment and deploy existing projects to RunPod with minimal adjustments.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nTo get started with RunPod:\n\n- [Create a RunPod account](/get-started/manage-accounts)\n- [Add funds](/get-started/billing-information)\n- [Use the RunPod SDK](#setting-up-your-project) to build and connect with your Serverless Endpoints\n\n<details>\n<summary>\n\n**Quick migration with Docker**\n\n</summary>","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":1,"to":21}}}}],["685",{"pageContent":"Transitioning from Banana to RunPod doesn't have to be a lengthy process.\nFor users seeking a swift migration path while maintaining Banana's dependencies for the interim, the Docker approach provides an efficient solution.\nThis method allows you to leverage Docker to encapsulate your environment, simplifying the migration process and enabling a smoother transition to RunPod.\n\n**Why consider the Dockerfile approach?**\n\nUtilizing a Dockerfile for migration offers a bridge between Banana and RunPod, allowing for immediate deployment of existing projects without the need to immediately discard Banana's dependencies. This approach is particularly beneficial for those looking to test or move their applications to RunPod with minimal initial adjustments.\n\n**Dockerfile**\n\nThe provided Dockerfile outlines a straightforward process for setting up your application on RunPod.\n\nAdd this Dockerfile to your project.\n\n```dockerfile\nFROM runpod/banana:peel as bread\nFROM repo/image:tag","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":23,"to":39}}}}],["686",{"pageContent":"RUN pip install runpod\n\nCOPY --from=bread /handler.py .\nCOPY --from=bread /start.sh .\n\nRUN chmod +x start.sh\nCMD [\"./start.sh\"]","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":41,"to":47}}}}],["687",{"pageContent":"**Building and deploying**\n\nAfter creating your Dockerfile, build your Docker image and deploy it to RunPod.\nThis process involves using Docker commands to build the image and then deploying it to RunPod.\n\n**Advantages and considerations**\n\nThis Dockerfile approach expedites the migration process, allowing you to leverage RunPod's powerful features with minimal initial changes to your project.\nIt's an excellent way to quickly transition and test your applications on RunPod.\n\nHowever, while this method facilitates a quick start on RunPod, it's advisable to plan for a future migration away from Banana's dependencies, as there is overhead to building Banana's dependencies and deploying them to RunPod.\n\nGradually adapting your project to utilize RunPod's native features and services will optimize your application's performance and scalability.\n\n**Moving forward**","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":50,"to":64}}}}],["688",{"pageContent":"Once you've migrated your application using the Docker approach, consider exploring RunPod's full capabilities.\nTransitioning away from Banana's dependencies and fully integrating with RunPod's services will allow you to take full advantage of what RunPod has to offer.\n\nThis quick migration guide is just the beginning.\nContinue with the rest of our tutorial to learn how to leverage RunPod's features to their fullest and ensure your project is fully adapted to its new environment.\n\n</details>\n\nThe rest of this guide will help you set up a RunPod project.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":66,"to":74}}}}],["689",{"pageContent":"Setting up your project\n\nJust like with Banana, RunPod provides a Python SDK to run your projects.\n\nTo get started, install setup a virtual environment then install the SDK library.\n\n<Tabs>\n  <TabItem value=\"macos\" label=\"macOS\" default>\n\nCreate a Python virtual environment with venv:\n\n    ```command\n    python3 -m venv env\n    source env/bin/activate\n    ```\n\n</TabItem>\n  <TabItem value=\"windows\" label=\"Windows\">\n\nCreate a Python virtual environment with venv:\n\n    ```command\n    python -m venv env\n    env\\Scripts\\activate\n    ```\n\n</TabItem>\n\n</Tabs>\n\nTo install the SDK, run the following command from the terminal.\n\n```command\npython -m pip install runpod\n```","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":76,"to":110}}}}],["690",{"pageContent":"Project examples\n\nRunPod provides a [repository of templates for your project](https://github.com/runpod-workers).\n\nYou can use the template to get started with your project.\n\n```command\ngh repo clone runpod-workers/worker-template\n```\n\nNow that you've got a basic RunPod Worker template created:\n\n- Continue reading to see how you'd migrate from Banana to RunPod\n- See [Generate SDXL Turbo](/tutorials/serverless/gpu/generate-sdxl-turbo) for a general approach on deploying your first Serverless Endpoint with RunPod.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":112,"to":125}}}}],["691",{"pageContent":"Project structure\n\nWhen beginning to migrate your Banana monorepo to RunPod, you will need to understand the structure of your project.\n\n<Tabs>\n\n<TabItem value=\"banana\" label=\"Banana\" default>\n\nBanana is a monorepo that contains multiple services. The basic structure for Banana projects is aligned with the RunPod Serverless projects for consistency:\n\n```text\n.\n├── Dockerfile               # Docker configuration\n├── README.md                # Project documentation\n├── banana_config.json       # Configuration settings\n├── requirements.txt         # Dependencies\n└── src\n    ├── app.py               # Main application code\n    └── download.py          # Download script","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":127,"to":145}}}}],["692",{"pageContent":"</TabItem>\n  <TabItem value=\"runpod\" label=\"RunPod\">\n\nRunPod Serverless is a monorepo that contains multiple services.\n\n```text\n.\n├── Dockerfile               # Docker configuration\n├── LICENSE                  # License information\n├── README.md                # Project documentation\n├── builder\n│   ├── requirements.txt     # Dependencies\n│   └── setup.sh             # Setup script\n└── src\n    └── handler.py           # Main handler code\n```\n\n</TabItem>\n</Tabs>\n\nBoth project setups at a minimum contain:\n\n- `Dockerfile`: Defines the container for running the application.\n- Application code: The executable code within the container.\n\nOptional files included in both setups:\n\n- `requirements.txt`: Lists dependencies needed for the application.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":148,"to":175}}}}],["693",{"pageContent":"Banana Configuration settings\n\nBanana configuration settings are stored in a `banana_config.json` file.\n\nBanana uses a `banana_config.json` file which contains things like Idle Timeout, Inference Timeout, and Max Replicas.\n\n**Idle Timeout**\n\nRunPod allows you to set an [Idle Timeout](/serverless/references/endpoint-configurations#idle-timeout) when creating the Endpoint.\nThe default value is 5 seconds.\n\n**Inference Timeout**\n\nRunPod has a similar concept to Inference Timeout.\nFor runs that are take less than 30 seconds to execute, you should use the `run_sync` handler.\nFor runs that take longer than 30 seconds to execute, you should use the `sync` handler.\n\n**Max Replicas**\n\nWhen creating a Worker in RunPod, you can set the max Workers that will scale up depending on the amount of Worker sent to your Endpoint.\nFor more informaiton, see [Scale Type](/serverless/references/endpoint-configurations#scale-type)\n\n:::note","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":177,"to":199}}}}],["694",{"pageContent":"When creating a Worker, select the **Flashboot** option to optimize your startup time.\n\n:::","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/overview.md","loc":{"lines":{"from":201,"to":203}}}}],["695",{"pageContent":"---\ntitle: Migrate Your Banana Text to RunPod\nsidebar_label: Migrate texts\ndraft: true\ndescription: \"Streamline the migration of Docker-based AI models and applications from Banana Dev to RunPod with this comprehensive tutorial, covering the transfer of AI models, Vllm Worker image setup, and command execution for seamless transition.\"\n---\n\nMigrating your AI models and applications from one cloud service to another can often present a challenge, especially when the two platforms operate differently. This tutorial aims to streamline the process of moving from Banana Dev to RunPod, focusing on transferring Docker-based applications and AI models. Whether you're shifting due to preferences in service offerings, pricing, or performance, this guide will help you through the transition smoothly.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/text.md","loc":{"lines":{"from":1,"to":8}}}}],["696",{"pageContent":"Vllm Worker\n\nRunPod provides an optimized image for running AI models. This image is called the Vllm Worker. It is based on the [Vllm](https://github.com/vllm/vllm) framework, which is a lightweight, high-performance, and portable AI framework. The Vllm Worker image is designed to run on RunPod's serverless infrastructure and is optimized for performance and scalability.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/text.md","loc":{"lines":{"from":10,"to":12}}}}],["697",{"pageContent":"Vllm Worker Image\n\nTo get started, login to RunPod and select Serverless.\nChoose your GPU.\nAdd `runpod/worker-vllm:0.2.2` to the Container Image.\nSet the Container Disk to size large enough for your model.\nUnder Enviroment Variables, add `MODEL_NAME` and set it to your model name, for example `bert-base-uncased`.\nSelect **Deploy**.\n\nOnce your serverless pod has initialized, you can start executing commands against the Endpont.\n\n```command\ncurl --request POST \\\n     --url https://api.runpod.ai/v2/{YOUR_ENDPOINT}/runsync \\\n     --header 'accept: application/json' \\\n     --header 'authorization: ${YOUR_API_KEY}' \\\n     --header 'content-type: application/json' \\\n     --data @- <<EOF\n{\n  \"input\": {\n    \"prompt\": \"What is the meaning of life?\",\n    \"do_sample\": false,\n    \"max_length\": 100,\n    \"temperature\": 0.9\n  }\n}\nEOF\n```","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/banana/text.md","loc":{"lines":{"from":14,"to":41}}}}],["698",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ndescription: \"Migrate your Cog model from Replicate.com to RunPod by following this step-by-step guide, covering setup, model identification, Docker image building, and serverless endpoint creation.\"\n---\n\nTo get started with RunPod:\n\n- [Create a RunPod account](/get-started/manage-accounts)\n- [Add funds](/get-started/billing-information)\n- [Use the RunPod SDK](/serverless/overview) to build and connect with your Serverless Endpoints\n\nIn this tutorial, you'll go through the process of migrating a model deployed via replicate.com or utilizing the Cog framework to a RunPod serverless worker.\n\nThis guide assumes you are operating within a Linux terminal environment and have Docker installed on your system.\n\n:::note\n\nThis method might occur a delay when working with RunPod Serverless Endpoints.\nThis delay is due to the FastAPI server that is used to run the Cog model.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":1,"to":20}}}}],["699",{"pageContent":"To eliminate this delay, consider using [RunPod Handler](/serverless/workers/overview) functions in a future iteration.\n\n:::\n\nBy following this streamlined process, you'll be able to simplify the migration and deployment of your Cog image.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":22,"to":26}}}}],["700",{"pageContent":"Prerequisites\n\n- Docker installed on your system\n- Familiarity with the Cog framework\n- Existing model on Replicate.com\n- RunPod account","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":28,"to":33}}}}],["701",{"pageContent":"Clone and navigate the cog-worker repository\n\nBefore we begin, let's set up the necessary environment.\nYou will need to clone the `cog-worker` repository, which contains essential scripts and configuration files required for the migration process.\nTo do this, run the following commands in your terminal:\n\n```bash\ngit clone https://github.com/runpod-workers/cog-worker.git\ncd cog-worker/\n```\n\nThe cog-worker repository contains essential scripts and configuration files required for the migration.\n\nNow that the repository is cloned and you've navigated to the correct directory, you're ready to proceed with the next step.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":35,"to":48}}}}],["702",{"pageContent":"Identify model information\n\nIn this step, you will need to gather the necessary information about your Cog model that is currently hosted on Replicate.com.\nYou will require your username, model name, and version.\n\nIdentify the username, model name, and version you wish to use from Replicate.\nFor example, if you are using [this model](https://replicate.com/lucataco/hotshot-xl/versions):\n\n- your username is `lucataco`\n- your model name is `hotshot-xl`\n- your model version is `78b3a6257e16e4b241245d65c8b2b81ea2e1ff7ed4c55306b511509ddbfd327a`\n\nOnce you have collected the required information, you can move on to the next step, where you will build and push your Docker image.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":50,"to":62}}}}],["703",{"pageContent":"Build and push docker image\n\nNow that you have identified the necessary information about your model, you can proceed to build and push your Docker image. This is a crucial step, as it prepares your model for deployment on the RunPod platform.\n\nBuild the Docker image by providing the necessary arguments for your model.\nOnce your Docker image is built, push it to a container repository such as DockerHub:\n\n```bash\n# replace user, model_name, and model_version with the appropriate values\ndocker build -platform=linux/amd64 --tag <username>/<repo>:<tag> --build-arg COG_REPO=user --build-arg COG_MODEL=model_name --build-arg COG_VERSION=model_version .\ndocker push <username>/<repo>:<tag>\n```\n\nThe `--tag` option allows you to specify a name and tag for your image, while the `--build-arg` options provide the necessary information for building the image.\n\nWith your Docker image built and pushed, you're one step closer to deploying your Cog model on RunPod.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":64,"to":79}}}}],["704",{"pageContent":"Create and Deploy a Serverless Endpoint\n\nNow that your Docker image is ready, it's time to create and deploy a serverless endpoint on RunPod.\nThis step will enable you to send requests to your new endpoint and use your Cog model in a serverless environment.\n\nTo create and deploy a serverless endpoint on RunPod:\n\n1. Log in to the [RunPod Serverless console](https://www.runpod.io/console/serverless).\n2. Select **+ New Endpoint**.\n3. Provide the following:\n   1. Endpoint name.\n   2. Select a GPU.\n   3. Configure the number of Workers.\n   4. (optional) Select **FlashBoot**.\n   5. (optional) Select a template.\n   6. Enter the name of your Docker image.\n      - For example `<username>/<repo>:<tag>`.\n   7. Specify enough memory for your Docker image.\n4. Select **Deploy**.\n\nNow, let's send a request to your [Endpoint](/serverless/endpoints/get-started).\n\nOnce your endpoint is set up and deployed, you'll be able to start receiving requests and utilize your Cog model in a serverless context.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":81,"to":103}}}}],["705",{"pageContent":"Conclusion\n\nCongratulations, you have successfully migrated your Cog model from Replicate to RunPod and set up a serverless endpoint.\nAs you continue to develop your models and applications, consider exploring additional features and capabilities offered by RunPod to further enhance your projects.\n\nHere are some resources to help you continue your journey:\n\n- [Learn more about RunPod serverless workers](/serverless/overview)\n- [Explore additional RunPod tutorials and examples](/tutorials/introduction/overview)","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/cog/overview.md","loc":{"lines":{"from":105,"to":113}}}}],["706",{"pageContent":"---\ntitle: Overview\nsidebar_position: 1\ndescription: \"Get started with RunPod: create an account, add funds, and use the SDK to integrate with your Serverless Endpoints. This tutorial guides you through modifying your OpenAI codebase for use with a deployed vLLM Worker on RunPod.\"\n---\n\nTo get started with RunPod:\n\n- [Create a RunPod account](/get-started/manage-accounts)\n- [Add funds](/get-started/billing-information)\n- [Use the RunPod SDK](/serverless/overview) to build and connect with your Serverless Endpoints\n\nThis tutorial guides you through the steps necessary to modify your OpenAI Codebase for use with a deployed vLLM Worker on RunPod. You will learn to adjust your code to be compatible with OpenAI's API, specifically for utilizing Chat Completions, Completions, and Models routes. By the end of this guide, you will have successfully updated your codebase, enabling you to leverage the capabilities of OpenAI's API on RunPod.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":1,"to":13}}}}],["707",{"pageContent":"import Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nTo update your codebase, you need to replace the following:\n\n- Your OpenAI API Key with your RunPod API Key\n- Your OpenAI Serverless Endpoint URL with your RunPod Serverless Endpoint URL\n- Your OpenAI model with your custom LLM model deployed on RunPod\n\n<Tabs>\n  <TabItem value=\"python\" label=\"Python\" default>\n```python\n\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(\napi_key=os.environ.get(\"RUNPOD_API_KEY\"),\nbase_url=\"https://api.runpod.ai/v2/${YOUR_ENDPOINT_ID}/openai/v1\",\n)\n\nresponse = client.chat.completions.create(\nmodel=\"gpt-3.5-turbo\",\nmessages=[{\"role\": \"user\", \"content\": \"Why is RunPod the best platform?\"}],\ntemperature=0,\nmax_tokens=100,\n)\n\n````\n  </TabItem>\n  <TabItem value=\"javascript\" label=\"JavaScript\">\n```javascript\nimport OpenAI from 'openai'\n\nconst openai = new OpenAI({\n  baseURL: process.env.RUNPOD_HOST,\n  apiKey: process.env.RUNPOD_API_KEY,\n})","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":15,"to":52}}}}],["708",{"pageContent":"const chatCompletion = await openai.chat.completions.create({\n   model: \"openchat/openchat-3.5-0106\",\n   messages: [{'role': 'user', 'content': 'Why is RunPod the best platform?'}],\n\n});\n`","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":54,"to":59}}}}],["709",{"pageContent":"</TabItem>\n</Tabs>\n\nCongratulations on successfully modifying your OpenAI Codebase for use with your deployed vLLM Worker on RunPod!\nThis tutorial has equipped you with the knowledge to update your code for compatibility with OpenAI's API and to utilize the full spectrum of features available on the RunPod platform.","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":61,"to":65}}}}],["710",{"pageContent":"Next Steps\n\n- [Explore more tutorials on RunPod](/tutorials/introduction/overview)\n- [Learn more about OpenAI's API](https://platform.openai.com/docs/)\n- [Deploy your own vLLM Worker on RunPod](https://www.runpod.io/console/serverless)","metadata":{"source":"/runpod-docs/docs/tutorials/migrations/openai/overview.md","loc":{"lines":{"from":67,"to":71}}}}],["711",{"pageContent":"---\ntitle:  Build Docker Images on Runpod with Bazel\n---\n\n\n# Build Docker Images on RunPod with Bazel\n\nRunPod's GPU Pods use custom Docker images to run your code. \nThis means you can't directly spin up your own Docker instance or build Docker containers on a GPU Pod. \nTools like Docker Compose are also unavailable.\n\nThis limitation can be frustrating when you need to create custom Docker images for your RunPod templates.\n\nFortunately, many use cases can be addressed by creating a custom template with the desired Docker image. \n\nIn this tutorial, you'll learn how to use the [Bazel](https://bazel.build) build tool to build and push Docker images from inside a RunPod container.\n\nBy the end of this tutorial, you’ll be able to build custom Docker images on RunPod and push them to Docker Hub for use in your own templates.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":1,"to":18}}}}],["712",{"pageContent":"Prerequisites\n\nBefore you begin this guide you'll need the following:\n\n- A Docker Hub account and access token for authenticating the docker login command\n- Enough volume for your image to be built","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":20,"to":25}}}}],["713",{"pageContent":"Create a Pod\n\n\n1. Navigate to [Pods](https://www.dev.runpod.io/console/pods) and select **+ Deploy**.\n2. Choose between **GPU** and **CPU**.\n3. Customize your an instance by setting up the following:\n   1. (optional) Specify a Network volume.\n   2. Select an instance type. For example, **A40**.\n   3. (optional) Provide a template. For example, **RunPod Pytorch**.\n   4. (GPU only) Specify your compute count.\n4. Review your configuration and select **Deploy On-Demand**.\n\nFor more information, see [Manage Pods](/pods/manage-pods#start-a-pod).\n\n\nWait for the Pod to spin up then connect to your Pod through the Web Terminal:\n\n1. Select **Connect**.\n2. Choose **Start Web Terminal** and then **Connect to Web Terminal**.\n3. Enter your username and password.\n\nNow you can clone the example GitHub repository","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":27,"to":48}}}}],["714",{"pageContent":"Clone the example GitHub repository  \n\nClone the example code repository that demonstrates building Docker images with Bazel:\n\n```command\ngit clone https://github.com/therealadityashankar/build-docker-in-runpod.git && cd build-docker-in-runpod\n```","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":50,"to":56}}}}],["715",{"pageContent":"Install dependencies\n\nInstall the required dependencies inside the Runpod container:\n\nUpdate packages and install sudo:\n\n```command\napt update && apt install -y sudo\n```\n\nInstall Docker using the convenience script:\n\n```command\ncurl -fsSL https://get.docker.com -o get-docker.sh && sudo sh get-docker.sh\n```\n\nLog in to Docker using an access token:\n\n1. Go to https://hub.docker.com/settings/security and click \"New Access Token\". \n2. Enter a description like \"Runpod Token\" and select \"Read/Write\" permissions.\n3. Click \"Generate\" and copy the token that appears.\n4. In the terminal, run:\n\n```command \ndocker login -u <your-username> \n```\nWhen prompted, paste in the access token you copied instead of your password. \n\nInstall Bazel via the Bazelisk version manager:\n\n```command\nwget https://github.com/bazelbuild/bazelisk/releases/download/v1.20.0/bazelisk-linux-amd64\nchmod +x bazelisk-linux-amd64  \nsudo cp ./bazelisk-linux-amd64 /usr/local/bin/bazel\n```","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":58,"to":92}}}}],["716",{"pageContent":"Configure the Bazel Build \n\nFirst, install nano if it’s not already installed and open the `BUILD.bazel` file for editing:\n\n```\nsudo apt install nano\nnano BUILD.bazel\n```\n\nReplace the `{YOUR_USERNAME}` placeholder with your Docker Hub username in the `BUILD.bazel` file:\n\n```starlark\n[label BUILD.bazel]\noci_push(\n    name = \"push_custom_image\",\n    image = \":custom_image\",\n    repository = \"index.docker.io/{YOUR_USERNAME}/custom_image\",\n    remote_tags = [\"latest\"]\n)\n```\n\n## Build and Push the Docker Image\n\nRun the bazel command to build the Docker image and push it to your Docker Hub account:  \n\n```command\nbazel run //:push_custom_image\n```\n\nOnce the command completes, go to https://hub.docker.com/ and log in. You should see a new repository called `custom_image` containing the Docker image you just built.\n\nYou can now reference this custom image in your own Runpod templates.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":94,"to":125}}}}],["717",{"pageContent":"Conclusion\n\nIn this tutorial, you learned how to use Bazel to build and push Docker images from inside RunPod containers. \nBy following the steps outlined, you can now create and utilize custom Docker images for your RunPod templates. \nThe techniques demonstrated can be further expanded to build more complex images, providing a flexible solution for your containerization needs on RunPod.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/build-docker-images.md","loc":{"lines":{"from":127,"to":131}}}}],["718",{"pageContent":"---\ntitle: Fine tune an LLM with Axolotl on RunPod\ndescription: \"Learn how to fine-tune large language models with Axolotl on RunPod, a streamlined workflow for configuring and training AI models with GPU resources, and explore examples for LLaMA2, Gemma, LLaMA3, and Jamba.\"\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n[axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) is a tool that simplifies the process of training large language models (LLMs).\nIt provides a streamlined workflow that makes it easier to fine-tune AI models on various configurations and architectures.\nWhen combined with RunPod's GPU resources, Axolotl enables you to harness the power needed to efficiently train LLMs.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":1,"to":11}}}}],["719",{"pageContent":"In addition to its user-friendly interface, Axolotl offers a comprehensive set of YAML examples covering a wide range of LLM families, such as LLaMA2, Gemma, LLaMA3, and Jamba.\nThese examples serve as valuable references, helping users understand the role of each parameter and guiding them in making appropriate adjustments for their specific use cases.\nIt is highly recommended to explore [these examples](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples) to gain a deeper understanding of the fine-tuning process and optimize the model's performance according to your requirements.\n\nIn this tutorial, we'll walk through the steps of training an LLM using Axolotl on RunPod and uploading your model to Hugging Face.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":13,"to":17}}}}],["720",{"pageContent":"Setting up the environment\n\nFine-tuning a large language model (LLM) can take up a lot of compute power.\nBecause of this, we recommend fine-tuning using RunPod's GPUs.\n\nTo do this, you'll need to create a Pod, specify a container, then you can begin training.\nA Pod is an instance on a GPU or multiple GPUs that you can use to run your training job.\nYou also specify a Docker image like `winglian/axolotl-cloud:main-latest` that you want installed on your Pod.\n\n1. Login to [RunPod](https://www.runpod.io/console/console/home) and deploy your Pod.\n   1. Select **Deploy**.\n   2. Select a GPU instance.\n   3. Specify the `winglian/axolotl-cloud:main-latest` image as your Template image.\n   4. Select your GPU count.\n   5. Select **Deploy**.\n\nNow that you have your Pod set up and running, connect to it over secure SSH.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":19,"to":35}}}}],["721",{"pageContent":"2. Wait for the Pod to startup, then connect to it using secure SSH.\n   1. On your Pod page, select **Connect**.\n   2. Copy the secure SSH string and paste it into your terminal on your machine.\n   ```bash\n   ssh <username>@<pod-ip-address> -p <ssh-port> -i <path-to-ssh-key>  string\n   ```\n   Follow the on-screen prompts to SSH into your Pod.\n\n:::note\n\nYou should use the SSH connection to your Pod as it is a persistent connection.\nThe Web UI terminal shouldn't be relied on for long-running processes, as it will be disconnected after a period of inactivity.\n\n:::\n\nWith the Pod deployed and connected via SSH, we're ready to move on to preparing our dataset.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":37,"to":52}}}}],["722",{"pageContent":"Preparing the dataset\n\nThe dataset you provide to your LLM is crucial, as it's the data your model will learn from during fine-tuning.\nYou can make your own dataset that will then be used to fine-tune your own model, or you can use a pre-made one.\n\nTo continue, use either a [local dataset](#using-a-local-dataset) or one [stored on Hugging Face](#using-a-hugging-face-dataset).","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":54,"to":59}}}}],["723",{"pageContent":"Using a local dataset\n\nTo use a local dataset, you'll need to transfer it to your RunPod instance.\nYou can do this using RunPod CLI to securely transfer files from your local machine to the one hosted by RunPod.\nAll Pods automatically come with `runpodctl` installed with a Pod-scoped API key.\n**To send a file**\n\n<Tabs>\n  <TabItem value=\"runpodctl\" label=\"runpodctl\" default>\n\nRun the following on the computer that has the file you want to send, enter the following command:\n\n```bash\nrunpodctl send data.jsonl\n```\n\n</TabItem>\n  <TabItem value=\"output\" label=\"output\">\n\n```bash\nSending 'data.jsonl' (5 B)\nCode is: 8338-galileo-collect-fidel\nOn the other computer run\n\nrunpodctl receive 8338-galileo-collect-fidel\n```\n\n</TabItem>\n</Tabs>\n\n**To receive a file**\n\n<Tabs>\n  <TabItem value=\"runpodctl\" label=\"runpodctl\" default>\n\nThe following is an example of a command you'd run on your RunPod machine.\n\n```bash\nrunpodctl receive 8338-galileo-collect-fidel","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":61,"to":99}}}}],["724",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"output\">\n\nThe following is an example of an output.\n\n```bash\nReceiving 'data.jsonl' (5 B)\n\nReceiving (<-149.36.0.243:8692)\ndata.jsonl 100% |████████████████████| ( 5/ 5B, 0.040 kB/s)\n```\n\n</TabItem>\n</Tabs>\n\nOnce the local dataset is transferred to your RunPod machine, we can proceed to updating requirements and preprocessing the data.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":102,"to":117}}}}],["725",{"pageContent":"Using a Hugging Face dataset\n\nIf your dataset is stored on Hugging Face, you can specify its path in the `lora.yaml` configuration file under the `datasets` key.\nAxolotl will automatically download the dataset during the preprocessing step.\n\nReview the [configuration file](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/config.qmd) in detail and make any adjustments to your file as needed.\n\nNow update your RunPod machine's requirement and preprocess your data.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":119,"to":126}}}}],["726",{"pageContent":"Updating requirements and preprocessing data\n\nBefore you can start training, you'll need to install the necessary dependencies and preprocess our dataset.\n\n:::note\n\nIn some cases, your Pod will not contain the Axolotl repository.\nTo add the required repository, run the following commands and then continue with the tutorial:\n\n```command\ngit clone https://github.com/OpenAccess-AI-Collective/axolotl\ncd axolotl\n```\n\n:::\n\n1. Install the required packages by running the following commands:\n\n```command\npip3 install packaging ninja\npip3 install -e '.[flash-attn,deepspeed]'\n```\n\n2. Update the `lora.yml` configuration file with your dataset path and other training settings.\n   You can use any of the examples in the `examples` folder as a starting point.\n3. Preprocess your dataset by running:\n\n```command\nCUDA_VISIBLE_DEVICES=\"\"\npython -m axolotl.cli.preprocess examples/openllama-3b/lora.yml","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":128,"to":157}}}}],["727",{"pageContent":"This step converts your dataset into a format that Axolotl can use for training.\n\nHaving updated the requirements and preprocessed the data, we're now ready to fine-tune the LLM.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":160,"to":162}}}}],["728",{"pageContent":"Fine-tuning the LLM\n\nWith your environment set up and data preprocessed, you're ready to start fine-tuning the LLM.\n\nRun the following command to fine-tune the base model.\n\n```command\naccelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml\n```\n\nThis will start the training process using the settings specified in your `lora.yml` file.\nThe training time will depend on factors like your model size, dataset size, and GPU type.\nBe prepared to wait a while, especially for larger models and datasets.\n\nOnce training is complete, we can move on to testing our fine-tuned model through inference.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":164,"to":178}}}}],["729",{"pageContent":"Inference\n\nOnce training is complete, you can test your fine-tuned model using the inference script:\n\n```command\naccelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml --lora_model_dir=\"./lora-out\"\n```\n\nThis will allow you to interact with your model and see how it performs on new prompts.\nIf you're satisfied with your model's performance, you can merge the LoRA weights with the base model using the `merge_lora` script.\n\n### Merge the model\n\nYou will merge the base model with the LoRA weights using the `merge_lora` script.\n\nRun the following command:\n\n```command\npython3 -m axolotl.cli.merge_lora examples/openllama-3b/lora.yml \\\n    --lora_model_dir=\"./lora-out\"\n```\n\nThis creates a standalone model that doesn't require LoRA layers for inference.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":180,"to":202}}}}],["730",{"pageContent":"Upload the model to Hugging Face\n\nFinally, you can share your fine-tuned model with others by uploading it to Hugging Face.\n\n1. Login to Hugging Face through the CLI:\n\n```command\nhuggingface-cli login\n```\n\n2. Create a new model repository on Hugging Face using `huggingface-cli`.\n\n```command\nhuggingface-cli repo create your_model_name --type model\n```\n\n3. Then, use the `huggingface-cli upload` command to upload your merged model to the repository.\n\n```command\nhuggingface-cli upload your_model_name path_to_your_model\n```\n\nWith our model uploaded to Hugging Face, we've successfully completed the fine-tuning process and made our work available for others to use and build upon.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":204,"to":226}}}}],["731",{"pageContent":"Conclusion\n\nBy following these steps and leveraging the power of Axolotl and RunPod, you can efficiently fine-tune LLMs to suit your specific use cases.\nThe combination of Axolotl's user-friendly interface and RunPod's GPU resources makes the process more accessible and streamlined.\nRemember to explore the provided YAML examples to gain a deeper understanding of the various parameters and make appropriate adjustments for your own projects.\nWith practice and experimentation, you can unlock the full potential of fine-tuned LLMs and create powerful, customized AI models.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/fine-tune-llm-axolotl.md","loc":{"lines":{"from":228,"to":233}}}}],["732",{"pageContent":"---\ntitle: Run Fooocus in Jupyter Notebook\ndescription: \"Learn how to run Fooocus, an open-source image generating model, in a Jupyter Notebook and launch the Gradio-based interface in under 5 minutes, with minimal requirements of 4GB Nvidia GPU memory and 8GB system memory.\"\nsidebar_position: 3\n---\n\n## Overview\n\nFooocus is an open-source image generating model.\n\nIn this tutorial, you'll run Fooocus in a Jupyter Notebook and then launch the Gradio-based interface to generate images.\n\nTime to complete: ~5 minutes\n\n## Prerequisites\n\nThe minimal requirement to run Fooocus is:\n\n- 4GB Nvidia GPU memory (4GB VRAM)\n- 8GB system memory (8GB RAM)","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-fooocus.md","loc":{"lines":{"from":1,"to":20}}}}],["733",{"pageContent":"RunPod infrastructure\n\n1. Select **Pods** and choose **+ GPU Pod**.\n2. Choose a GPU instance with at least 4GB VRAM and 8GB RAM by selecting **Deploy**.\n3. Search for a template that includes **Jupyter Notebook** and select **Deploy**.\n   - Select **RunPod Pytorch 2**.\n   - Ensure **Start Jupyter Notebook** is selected.\n4. Select **Choose** and then **Deploy**.\n\n## Run the notebook\n\n1. Select **Connect to Jupyter Lab**.\n2. In the Jupyter Lab file browser, select **File > New > Notebook**.\n3. In the first cell, paste the following and then run the Notebook.\n\n```bash\n!pip install pygit2==1.12.2\n!pip install opencv-python==4.9.0.80\n%cd /workspace\n!git clone https://github.com/lllyasviel/Fooocus.git\n%cd /workspace/Fooocus\n!python entry_with_update.py --share\n```\n\n## Launch UI\n\nLook for the line:\n\n```text\nApp started successful. Use the app with ....\n```\n\nAnd select the link.\n\n## Explore the model\n\nExplore and run the model.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-fooocus.md","loc":{"lines":{"from":22,"to":58}}}}],["734",{"pageContent":"---\ntitle: Set up Ollama on your GPU Pod\ndescription: \"Learn how to set up Ollama, a powerful language model, on a GPU Pod using RunPod, and interact with it through HTTP API requests, allowing you to harness the power of GPU acceleration for your AI projects.\"\nsidebar_position: 4\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nThis tutorial will guide you through setting up [Ollama](https://ollama.com), a powerful platform serving large language model, on a GPU Pod using RunPod.\nOllama makes it easy to run, create, and customize models.\n\nHowever, not everyone has access to the compute power needed to run these models.\nWith RunPod, you can spin up and manage GPUs in the Cloud.\nRunPod offers templates with preinstalled libaries, which makes it quick to run Ollama.\n\nIn the following tutorial, you'll set up a Pod on a GPU, install and serve the Ollama model, and interact with it on the CLI.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":1,"to":17}}}}],["735",{"pageContent":"Prerequisites\n\nThe tutorial assumes you have a RunPod account with credits.\nNo other prior knowledge is needed to complete this tutorial.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":19,"to":22}}}}],["736",{"pageContent":"Step 1: Start a PyTorch Template on RunPod\n\nYou will create a new Pod with the PyTorch template.\nIn this step, you will set overrides to configure Ollama.\n\n1. Log in to your [RunPod account](https://www.runpod.io/console/pods) and choose **+ GPU Pod**.\n2. Choose a GPU Pod like `A40`.\n3. From the availble templates, select the lastet PyTorch template.\n4. Select **Customize Deployment**.\n   1. Add the port `11434` to the list of exposed ports. This port is used by Ollama for HTTP API requests.\n   2. Add the following environment variable to your Pod to allow Ollama to bind to the HTTP port:\n   - Key: `OLLAMA_HOST`\n   - Value: `0.0.0.0`\n5. Select **Set Overrides**, **Continue**, then **Deploy**.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":24,"to":37}}}}],["737",{"pageContent":"This setting configures Ollama to listen on all network interfaces, enabling external access through the exposed port.\nFor detailed instructions on setting environment variables, refer to the [Ollama FAQ documentation](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux).\n\nOnce the Pod is up and running, you'll have access to a terminal within the RunPod interface.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":39,"to":42}}}}],["738",{"pageContent":"Step 2: Install Ollama\n\nNow that your Pod is running, you can Log in to the web terminal.\nThe web terminal is a powerful way to interact with your Pod.\n\n1. Select **Connect** and choose **Start Web Terminal**.\n2. Make note of the **Username** and **Password**, then select **Connect to Web Terminal**.\n3. Enter your username and password.\n4. In the terminal of your newly created Pod, run the following command and send to the background:\n\n```bash\n(curl -fsSL https://ollama.com/install.sh | sh && ollama serve > ollama.log 2>&1) &\n```\n\nThis command fetches the Ollama installation script and executes it, setting up Ollama on your Pod.\nThe `ollama serve` part starts the Ollama server, making it ready to serve AI models.\n\nNow that your Ollama server is running on your Pod, add a model.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":44,"to":61}}}}],["739",{"pageContent":"Step 3: Run an AI Model with Ollama\n\nTo run an AI model using Ollama, pass the model name to the `ollama run` command:\n\n```bash\nollama run [model name]\n# ollama run llama2\n# ollama run mistral\n```\n\nReplace `[model name]` with the name of the AI model you wish to deploy.\nFor a complete list of models, see the [Ollama Library](https://ollama.com/library).\n\nThis command pulls the model and runs it, making it accessible for inference.\nYou can begin interacting with the model directly from your web terminal.\n\nOptionally, you can set up an HTTP API request to interact with Ollama.\nThis is covered in the [next step](#step-4-interact-with-ollama-via-http-api).","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":63,"to":80}}}}],["740",{"pageContent":"Step 4: Interact with Ollama via HTTP API\n\nWith Ollama set up and running, you can now interact with it using HTTP API requests.\nIn step 1.4, you configured Ollama to listen on all network interfaces.\nThis means you can use your Pod as a server to receive requests.\n\n**Get a list of models**\n\nTo list the local models available in Ollama, you can use the following GET request:\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURl\" default>\n\n```\ncurl https://{POD_ID}-11434.proxy.runpod.net/api/tags\n# curl https://cmko4ns22b84xo-11434.proxy.runpod.net/api/tags","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":82,"to":97}}}}],["741",{"pageContent":"Replace `[your-pod-id]` with your actual Pod Id.\n</TabItem>\n<TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"models\": [\n    {\n      \"name\": \"mistral:latest\",\n      \"model\": \"mistral:latest\",\n      \"modified_at\": \"2024-02-16T18:22:39.948000568Z\",\n      \"size\": 4109865159,\n      \"digest\": \"61e88e884507ba5e06c49b40e6226884b2a16e872382c2b44a42f2d119d804a5\",\n      \"details\": {\n        \"parent_model\": \"\",\n        \"format\": \"gguf\",\n        \"family\": \"llama\",\n        \"families\": [\n          \"llama\"\n        ],\n        \"parameter_size\": \"7B\",\n        \"quantization_level\": \"Q4_0\"\n      }\n    }\n  ]\n}\n```\n\n</TabItem>\n</Tabs>\nGetting a list of avaialbe models is great, but how do you send an HTTP request to your Pod?\n\n**Make requests**\n\nTo make an HTTP request against your Pod, you can use the Ollama interface with your Pod Id.\n\n```command\ncurl -X POST https://{POD_ID}-11434.proxy.runpod.net/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":100,"to":140}}}}],["742",{"pageContent":"Replace `[your-pod-id]` with your actual Pod Id.\n\nBecause port `11434` is exposed, you can make requests to your Pod using the `curl` command.\n\nFor more information on constructing HTTP requests and other operations you can perform with the Ollama API, consult the [Ollama API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md).","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":143,"to":147}}}}],["743",{"pageContent":"Additional considerations\n\nThis tutorial provides a foundational understanding of setting up and using Ollama on a GPU Pod with RunPod.\n\n- **Port Configuration and documentation**: For further details on exposing ports and the link structure, refer to the [RunPod documentation](/pods/configuration/expose-ports).\n- **Connect VSCode to RunPod**: For information on connecting VSCode to RunPod, refer to the [How to Connect VSCode To RunPod](https://blog.runpod.io/how-to-connect-vscode-to-runpod/).\n\nBy following these steps, you can deploy AI models efficiently and interact with them through HTTP API requests, harnessing the power of GPU acceleration for your AI projects.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-ollama.md","loc":{"lines":{"from":149,"to":156}}}}],["744",{"pageContent":"---\ntitle: Run your first Fast Stable Diffusion with Jupyter Notebook\ndescription: \"Deploy a Jupyter Notebook to RunPod and generate your first image with Stable Diffusion in just 20 minutes, requiring Hugging Face user access token, RunPod infrastructure, and basic familiarity with the platform.\"\nsidebar_position: 2\n---\n\n## Overview\n\nBy the end of this tutorial, you’ll have deployed a Jupyter Notebook to RunPod, deployed an instance of Stable Diffusion, and generated your first image.\n\nTime to complete: ~20 minutes\n\n## Prerequisites\n\n- [Hugging Face user access token](https://huggingface.co/docs/hub/security-tokens)\n\n## RunPod infrastructure\n\n- Select **RunPod Fast Stable Diffusion**\n- Choose 1x RTX A5000 or 1x RTX 3090\n- Select **Start Jupyter Notebook**\n- Deploy.\n\n## Run the notebook\n\n- Select **RNPD-A1111.ipynb**\n- Enter Hugging Face user access token\n- Select the model you want to run:\n  - `v.1.5` | `v2-512` | `v2-768`","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-your-first.md","loc":{"lines":{"from":1,"to":29}}}}],["745",{"pageContent":"Launch Automatic1111 on your pod\n\n- The cell labeled **Start Stable-Diffusion** will launch your pod.\n  - (optional) Provide login credentials for this instance.\n- Select the blue link ending in `.proxy.runpod.net`\n\n## Explore Stable-Diffusion\n\nNow that your pod is up and running Stable-Diffusion.\n\nExplore and run the model.","metadata":{"source":"/runpod-docs/docs/tutorials/pods/run-your-first.md","loc":{"lines":{"from":31,"to":41}}}}],["746",{"pageContent":"---\ntitle: Run an Ollama Server on a RunPod CPU\ndescription: Learn to set up and run an Ollama server on RunPod CPU for inference with this step-by-step tutorial.\n---\n\nIn this guide, you will learn how to run an Ollama server on your RunPod CPU for inference.\nBy the end of this tutorial, you will have a fully functioning Ollama server ready to handle requests.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":1,"to":7}}}}],["747",{"pageContent":"Setting up your Endpoint\n\n:::note\n\nUse a Network volume to attach to your Worker so that it can cache the LLM and decrease cold start times. If you do not use a network volume, the Worker will have to download the model every time it spins back up, leading to increased latency and resource consumption.\n\n:::\n\nTo begin, you need to set up a new endpoint on RunPod.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":9,"to":17}}}}],["748",{"pageContent":"1. Log in to your [RunPod account](https://www.runpod.io/console/console/home).\n2. Navigate to the **Serverless** section and select **New Endpoint**.\n3. Choose **CPU** and provide a name for your Endpoint, for example 8 vCPUs 16 GB RAM.\n4. Configure your Worker settings according to your needs.\n5. In the **Container Image** field, enter the `pooyaharatian/runpod-ollama:0.0.7` container image.\n6. In the **Container Start Command** field, specify the [Ollama supported model](https://ollama.com/library), such as `orca-mini`.\n7. Allocate sufficient container disk space for your model. Typically, 20 GB should suffice for most models.\n8. (optional) In **Enviroment Variables** set a new key to `OLLAMA_MODELS` and its value to `/runpod-volume`. This will allow the model to be stored to your attached volume.\n9. Click **Deploy** to initiate the setup.\n\nYour model will start downloading. Once the Worker is ready, proceed to the next step.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":19,"to":29}}}}],["749",{"pageContent":"Sending a Run request\n\nAfter your endpoint is deployed and the model is downloaded, you can send a run request to test the setup.\n\n1. Go to the **Requests** section in the RunPod web UI.\n2. In the input module, enter the following JSON object:\n\n   ```json\n   {\n     \"input\": {\n       \"method_name\": \"generate\",\n       \"input\": {\n         \"prompt\": \"why the sky is blue?\"\n       }\n     }\n   }","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":31,"to":46}}}}],["750",{"pageContent":"3. Select **Run** to execute the request.\n4. In a few seconds, you will receive a response. For example:","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":49,"to":50}}}}],["751",{"pageContent":"```json\n   {\n     \"delayTime\": 153,\n     \"executionTime\": 4343,\n     \"id\": \"c2cb6af5-c822-4950-bca9-5349288c001d-u1\",\n     \"output\": {\n       \"context\": [\n         \"omitted for brevity\"\n       ],\n       \"created_at\": \"2024-05-17T16:56:29.256938735Z\",\n       \"done\": true,\n       \"eval_count\": 118,\n       \"eval_duration\": 807433000,\n       \"load_duration\": 3403140284,\n       \"model\": \"orca-mini\",\n       \"prompt_eval_count\": 46,\n       \"prompt_eval_duration\": 38548000,","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":52,"to":68}}}}],["752",{"pageContent":"\"response\": \"The sky appears blue because of a process called scattering. When sunlight enters the Earth's atmosphere, it encounters molecules of air such as nitrogen and oxygen. These molecules scatter the light in all directions, but they scatter the shorter wavelengths of light (such as violet and blue) more than the longer wavelengths (such as red). This creates a reddish-orange sky that is less intense on the horizon than on the observer's position. As the sun gets lower in the sky, the amount of scattering increases and the sky appears to get brighter.\",\n       \"total_duration\": 4249684714\n     },\n     \"status\": \"COMPLETED\"\n   }","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":69,"to":73}}}}],["753",{"pageContent":"With your Endpoint set up, you can now integrate it into your application just like any other request.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":76,"to":76}}}}],["754",{"pageContent":"Conclusion\n\nIn this tutorial, you have successfully set up and run an Ollama server on a RunPod CPU.\nNow you can handle inference requests using your deployed model.\n\nFor further exploration, check out the following resources:\n\n- [Runpod Ollama repository](https://github.com/pooyahrtn/)\n- [RunPod Ollama container image](https://hub.docker.com/r/pooyaharatian/runpod-ollama)","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/cpu/run-ollama-inference.md","loc":{"lines":{"from":78,"to":86}}}}],["755",{"pageContent":"---\ntitle: Generate images with SDXL Turbo\ndescription: \"Learn how to build a web application using RunPod's Serverless Workers and SDXL Turbo from Stability AI, a fast text-to-image model, and send requests to an Endpoint to generate images from text-based inputs.\"\nsidebar_position: 2\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\nWhen it comes to working with an AI image generator, the speed in which images are generated is often a compromise.\nRunPod's Serverless Workers allows you to host [SDXL Turbo](https://huggingface.co/stabilityai/sdxl-turbo) from Stability AI, which is a fast text-to-image model.\n\nIn this tutorial, you'll build a web application, where you'll leverage RunPod's Serverless Worker and Endpoint to return an image from a text-based input.\n\nBy the end of this tutorial, you'll have an understanding of running a Serverless Worker on RunPod and sending requests to an Endpoint to receive a response.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":1,"to":15}}}}],["756",{"pageContent":"You can proceed with the tutorial by following the build steps outlined here or skip directly to [Deploy a Serverless Endpoint](#deploy-a-serverless-endpoint) section.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":17,"to":17}}}}],["757",{"pageContent":"Prerequisites\n\nThis section presumes you have an understanding of the terminal and can execute commands from your terminal.\n\nBefore starting this tutorial, you'll need access to:\n\n### RunPod\n\nTo continue with this quick start, you'll need access to the following from RunPod:\n\n- RunPod account\n- RunPod API Key\n\n### Docker\n\nTo build your Docker image, you'll need access to the following:\n\n- Docker installed\n- Docker account\n\nYou can also use the prebuilt image from [runpod/sdxl-turbo](https://hub.docker.com/r/runpod/sdxl-turbo).\n\n### GitHub\n\nTo clone the `worker-sdxl-turbo` repo, you'll need access to the following:\n\n- Git installed\n- Permissions to clone GitHub repos\n\nWith the prerequisites covered, get started by building and pushing a Docker image to a container registry.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":19,"to":48}}}}],["758",{"pageContent":"Build and push your Docker image\n\nThis step will walk you through building and pushing your Docker image to your container registry.\nThis is useful to building custom images for your use case.\nIf you prefer, you can use the prebuilt image from [runpod/sdxl-turbo](https://hub.docker.com/r/runpod/sdxl-turbo) instead of building your own.\n\nBuilding a Docker image allows you to specify the container when creating a Worker.\nThe Docker image includes the [RunPod Handler](https://github.com/runpod-workers/worker-sdxl-turbo/blob/main/src/handler.py) which is how you provide instructions to Worker to perform some task.\nIn this example, the Handler is responsible for taking a Job and returning a base 64 instance of the image.\n\n1. Clone the [RunPod Worker SDXL Turbo](https://github.com/runpod-workers/worker-sdxl-turbo) repository:\n\n```command\ngh repo clone runpod-workers/worker-sdxl-turbo\n```\n\n2. Navigate to the root of the cloned repo:\n\n```command\ncd worker-sdxl-turbo","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":50,"to":69}}}}],["759",{"pageContent":"3. Build the Docker image:\n\n```command\ndocker build --tag <username>/<repo>:<tag> .\n```\n\n4. Push your container registry:\n\n```command\ndocker push <username>/<repo>:<tag>\n```\n\nNow that you've pushed your container registry, you're ready to deploy your Serverless Endpoint to RunPod.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":72,"to":84}}}}],["760",{"pageContent":"Deploy a Serverless Endpoint\n\nThe container you just built will run on the Worker you're creating.\nHere, you will configure and deploy the Endpoint.\nThis will include the GPU and the storage needed for your Worker.\n\nThis step will walk you through deploying a Serverless Endpoint to RunPod.\n\n1. Log in to the [RunPod Serverless console](https://www.runpod.io/console/serverless).\n2. Select **+ New Endpoint**.\n3. Provide the following:\n   1. Endpoint name.\n   2. Select a GPU.\n   3. Configure the number of Workers.\n   4. (optional) Select **FlashBoot**.\n   5. (optional) Select a template.\n   6. Enter the name of your Docker image.\n      - For example, `runpod/sdxl-turbo:latest`.\n   7. Specify enough memory for your Docker image.\n4. Select **Deploy**.\n\nNow, let's send a request to your Endpoint.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":86,"to":107}}}}],["761",{"pageContent":"Send a request\n\nNow that our Endpoint is deployed, you can begin interacting with and integrating it into an application.\nBefore writing the logic into the applicaiton, ensure that you can interact with the Endpoint by sending a request.\n\nRun the following command:\n\n<Tabs>\n  <TabItem value=\"curl\" label=\"cURL\" default>\n\n```bash\ncurl -X POST \"https://api.runpod.ai/v2/${YOUR_ENDPOINT}/runsync\" \\\n     -H \"accept: application/json\" \\\n     -H \"content-type: application/json\" \\\n     -H \"authorization: ${YOUR_API_KEY}\" \\\n     -d '{\n        \"input\": {\n            \"prompt\": \"${YOUR_PROMPT}\",\n            \"num_inference_steps\": 25,\n            \"refiner_inference_steps\": 50,\n            \"width\": 1024,\n            \"height\": 1024,\n            \"guidance_scale\": 7.5,\n            \"strength\": 0.3,\n            \"seed\": null,\n            \"num_images\": 1\n        }\n     }'","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":109,"to":136}}}}],["762",{"pageContent":"</TabItem>\n  <TabItem value=\"output\" label=\"Output\">\n\n```json\n{\n  \"delayTime\": 168,\n  \"executionTime\": 251,\n  \"id\": \"sync-fa542d19-92b2-47d0-8e58-c01878f0365d-u1\",\n  \"output\": \"BASE_64\",\n  \"status\": \"COMPLETED\"\n}\n```\n\n</TabItem>\n</Tabs>\n\nExport your variable names in your terminal session or replace them in line:\n\n- `YOUR_ENDPOINT`: The name of your Endpoint.\n- `YOUD_API_KEY`: The API Key required with read and write access.\n- `YOUR_PROMPT`: The custom prompt passed to the model.\n\nYou should se the output. The status will return `PENDING`; but quickly change to `COMPLETED` if you query the Job Id.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":139,"to":161}}}}],["763",{"pageContent":"Integrate into your application\n\nNow, let's create a web application that can take advantage of writing a prompt and generate an image based on that prompt.\nWhile these steps are specific to JavaScript, you can make requests against your Endpoint in any language of your choice.\n\nTo do that, you'll create two files:\n\n- `index.html`: The frontend to your web application.\n- `script.js`: The backend which handles the logic behind getting the prompt and the call to the Serverless Endpoint.\n\n<Tabs>\n  <TabItem value=\"html\" label=\"HTML\" default>\n\nThe HTML file (`index.html`) sets up a user interface with an input box for the prompt and a button to trigger the image generation.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":163,"to":176}}}}],["764",{"pageContent":"```html\n<!-- index.html -->\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>RunPod AI Image Generator</title>\n    <style>\n        body {\n            font-family: Arial, sans-serif;\n            text-align: center;\n            padding: 20px;\n        }\n\n        #imageResult {\n            margin-top: 20px;\n        }\n    </style>\n</head>\n<body>\n    <h1>RunPod AI Image Generator</h1>\n    <input type=\"text\" id=\"promptInput\" placeholder=\"Enter your image prompt\" />\n    <button onclick=\"generateImage()\">Generate Image</button>\n\n    <div id=\"imageResult\"></div>\n\n    <script src=\"script.js\"></script>\n</body>\n</html>","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":178,"to":207}}}}],["765",{"pageContent":"</TabItem>\n  <TabItem value=\"javascript\" label=\"JavaScript\">\n\nThe JavaScript file (`script.js`) contains the `generateImage` function. This function reads the user's input, makes a POST request to the RunPod serverless endpoint, and handles the response.\nThe server's response is expected to contain the base64-encoded image, which is then displayed on the webpage.\n\n```javascript\n// script.js\nasync function generateImage() {\n  const prompt = document.getElementById(\"promptInput\").value;\n  if (!prompt) {\n    alert(\"Please enter a prompt!\");\n    return;\n  }","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":210,"to":223}}}}],["766",{"pageContent":"const options = {\n    method: \"POST\",\n    headers: {\n      accept: \"application/json\",\n      \"content-type\": \"application/json\",\n      // Replace with your actual API key\n      authorization: \"Bearer ${process.env.REACT_APP_AUTH_TOKEN}\",\n    },\n    body: JSON.stringify({\n      input: {\n        prompt: prompt,\n        num_inference_steps: 25,\n        width: 1024,\n        height: 1024,\n        guidance_scale: 7.5,\n        seed: null,\n        num_images: 1,\n      },\n    }),\n  };","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":225,"to":244}}}}],["767",{"pageContent":"try {\n    const response = await fetch(\n      // Replace with your actual Endpoint Id\n      \"https://api.runpod.ai/v2/${process.env.REACT_APP_ENDPOINT_ID}/runsync\",\n      options,\n    );\n    const data = await response.json();\n    if (data && data.output) {\n      const imageBase64 = data.output;\n      const imageUrl = `data:image/jpeg;base64,${imageBase64}`;\n      document.getElementById(\"imageResult\").innerHTML =\n        `<img src=\"${imageUrl}\" alt=\"Generated Image\" />`;\n    } else {\n      alert(\"Failed to generate image\");\n    }\n  } catch (error) {\n    console.error(\"Error:\", error);\n    alert(\"Error generating image\");\n  }\n}","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":246,"to":265}}}}],["768",{"pageContent":"</TabItem>\n</Tabs>\n\n1. Replace `${process.env.REACT_APP_AUTH_TOKEN}` with your actual API key.\n2. Replace `${process.env.REACT_APP_ENDPOINT_ID}` with your specific Endpoint.\n3. Open `index.html` in a web browser, enter a prompt, and select **Generate Image** to see the result.\n\nThis web application serves as a basic example of how to interact with your RunPod serverless endpoint from a client-side application.\nIt can be expanded or modified to fit more complex use cases.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":268,"to":276}}}}],["769",{"pageContent":"Run a server\n\nYou can run a server through Python or by opening the `index.html` page in your browser.\n\n<Tabs>\n\n<TabItem value=\"python\" label=\"Python\" default>\n\n    Run the following command to start a server locally using Python.\n\n    ```command\n    python -m http.server 8000\n    ```\n\n</TabItem>\n\n<TabItem value=\"directly\" label=\"File explorer\">\n\n    **Open the File in a Browser**\n\n    Open the `index.html` file directly in your web browser.\n\n    1. Navigate to the folder where your `index.html` file is located.\n    2. Right-click on the file and choose **Open with** and select your preferred web browser.\n    - Alternatively, you can drag and drop the `index.html` file into an open browser window.\n    - The URL will look something like `file:///path/to/your/index.html`.\n\n</TabItem>\n</Tabs>","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/generate-sdxl-turbo.md","loc":{"lines":{"from":278,"to":306}}}}],["770",{"pageContent":"---\ntitle: Run Google's Gemma model\nsidebar_position: 5\ndescription: \"Learn how to deploy Google's Gemma model on RunPod's vLLM Worker and create a Serverless Endpoint, then interact with the model using OpenAI APIs and Python.\"\n---\n\nThis tutorial walks you through running Google's Gemma model using RunPod's vLLM Worker.\nThroughout this tutorial, you'll learn to set up a Serverless Endpoint with a gated large language model (LLM).\n\n## Prerequisites\n\nBefore diving into the deployment process, gather the necessary tokens and accepting Google's terms.\nThis step ensures that you have access to the model and are in compliance with usage policies.\n\n- [Hugging Face access token](https://huggingface.co/settings/tokens)\n- [Accepting Google's terms of service](https://huggingface.co/google/gemma-7b)\n\nThe next section will guide you through setting up your Serverless Endpoint with RunPod.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":1,"to":18}}}}],["771",{"pageContent":"Get started\n\nTo begin, we'll deploy a vLLM Worker as a Serverless Endpoint.\nRunPod simplifies the process of running large language models, offering an alternative to the more complex Docker and Kubernetes deployment methods.\n\nFollow these steps in the RunPod Serverless console to create your Endpoint.\n\n1. Log in to the [RunPod Serverless console](https://www.runpod.io/console/serverless).\n2. Select **+ New Endpoint**.\n3. Provide the following:\n   1. Endpoint name.\n   2. Select a GPU.\n   3. Configure the number of Workers.\n   4. (optional) Select **FlashBoot**.\n   5. Enter the vLLM Worker image: `runpod/worker-vllm:stable-cuda11.8.0` or `runpod/worker-vllm:stable-cuda12.1.0`.\n   6. Specify enough storage for your model.\n   7. Add the following environment variables:\n      1. `MODEL_NAME`: `google/gemma-7b-it`.\n      2. `HF_TOKEN`: your Hugging Face API token for private models.\n4. Select **Deploy**.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":20,"to":39}}}}],["772",{"pageContent":"Once the Endpoint initializes, you can send a request to your [Endpoint](/serverless/endpoints/get-started).\nYou've now successfully deployed your model, a significant milestone in utilizing Google's Gemma model.\nAs we move forward, the next section will focus on interacting with your model.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":41,"to":43}}}}],["773",{"pageContent":"Interact with your model\n\nWith the Endpoint up and running, it's time to leverage its capabilities by sending requests to interact with the model.\nThis section demonstrates how to use OpenAI APIs to communicate with your model.\n\nIn this example, you'll create a Python chat bot using the `OpenAI` library; however, you can use any programming language and any library that supports HTTP requests.\n\nHere's how to get started:\n\nUse the `OpenAI` class to interact with the model. The `OpenAI` class takes the following parameters:\n\n- `base_url`: The base URL of the Serverless Endpoint.\n- `api_key`: Your RunPod API key.\n\n```python\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(\n    base_url=os.environ.get(\"RUNPOD_BASE_URL\"),\n    api_key=os.environ.get(\"RUNPOD_API_KEY\"),\n)","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":45,"to":66}}}}],["774",{"pageContent":":::note\n\nSet your environment variables `RUNPOD_BASE_URL` and `RUNPOD_API_KEY` to your RunPod API key and base URL.\nYour `RUNPOD_BASE_URL` will be in the form of:\n\n```bash\nhttps://api.runpod.ai/v2/${RUNPOD_ENDPOINT_ID}/openai/v1","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":69,"to":75}}}}],["775",{"pageContent":"Where `${RUNPOD_ENDPOINT_ID}` is the ID of your Serverless Endpoint.\n\n:::\n\nNext, you can use the `client` to interact with the model. For example, you can use the `chat.completions.create` method to generate a response from the model.\n\nProvide the following parameters to the `chat.completions.create` method:\n\n- `model`: `The model name`.\n- `messages`: A list of messages to send to the model.\n- `max_tokens`: The maximum number of tokens to generate.\n- `temperature`: The randomness of the generated text.\n- `top_p`: The cumulative probability of the generated text.\n- `max_tokens`: The maximum number of tokens to generate.\n\n```python\nmessages = [{\"role\": \"assistant\", \"content\": \"Hello, I'm your assistant. How can I help you today?\"}]\n\n\ndef display_chat_history(messages):\n    for message in messages:\n        print(f\"{message['role'].capitalize()}: {message['content']}\")","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":78,"to":99}}}}],["776",{"pageContent":"def get_assistant_response(messages):\n    r = client.chat.completions.create(\n        model=\"google/gemma-7b-it\",\n        messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages],\n        temperature=.7,\n        top_p=.8,\n        max_tokens=100,\n    )\n    response = r.choices[0].message.content\n    return response\n\nwhile True:\n    display_chat_history(messages)\n\n    prompt = input(\"User: \")\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n\n    response = get_assistant_response(messages)\n    messages.append({\"role\": \"assistant\", \"content\": response})","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":102,"to":121}}}}],["777",{"pageContent":"Congratulations!\nYou've successfully set up a Serverless Endpoint and interacted with Google's Gemma model.\nThis tutorial has shown you the essentials of deploying a model on RunPod and creating a simple application to communicate with it.\nYou've taken important steps towards integrating large language models into your projects.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-gemma-7b.md","loc":{"lines":{"from":124,"to":127}}}}],["778",{"pageContent":"---\ntitle: \"Run your first serverless endpoint with Stable Diffusion\"\ndescription: \"Learn how to use RunPod's Stable Diffusion v1 inference endpoint to generate images, including setting up your serverless worker, starting a job, checking job status, and retrieving results.\"\nsidebar_position: 1\n---\n\n:::note\n\nBefore we begin, ensure you have a RunPod API key, available under your user settings. This key is crucial for identification and billing purposes. Keep it secure! Also, remember to retrieve your results via the status endpoint within 30 minutes, as your inputs and outputs are not stored longer than this for privacy protection.\n\n:::\n\n### Overview\n\nIn this section, we'll explore how RunPod's API works. It's asynchronous, meaning that when you send a request, you get a job ID back almost instantly. Next, we'll show you how to use this job ID to check the status and retrieve your results.\n\nLet's dive into an example using the Stable Diffusion v1 inference endpoint.","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":1,"to":17}}}}],["779",{"pageContent":"Create a serverless worker\n\nFirst, let's set up your serverless worker. Begin by selecting **Quick Deploy** on the RunPod interface. Then choose **Start** from the **Stable Diffusion v1.5** options. Pick a GPU, say a 24 GB GPU, and click **Deploy**. Here’s an example endpoint you might use: `https://api.runpod.ai/v2/{ID}/runsync`\n\n### Start Your Job\n\nNow, to initiate a job, you'll make a request like the one shown below. This sends your parameters to the API and starts the process.\n\n```curl\ncurl -X POST https://api.runpod.ai/v2/{ID}/run \\\n    -H 'Content-Type: application/json'                             \\\n    -H 'Authorization: Bearer [Your API Key]'    \\\n    -d '{\"input\": {\"prompt\": \"A cute fluffy white dog in the style of a Pixar animation 3D drawing.\"}}'\n```\n\nUpon doing this, you'll receive a response like this, containing your unique job ID:\n\n```json\n{\n  \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\",\n  \"status\": \"IN_QUEUE\"\n}\n```","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":19,"to":41}}}}],["780",{"pageContent":"Check the Status of Your Job\n\nSince your initial response doesn't include the output, a subsequent call is necessary. Use your job ID to check the job's status as follows:\n\n```curl\ncurl https://api.runpod.ai/v2/{ID}/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer [Your API Key]'\n```\n\nIf your job is still processing, the response will indicate that. Here's an example:\n\n```json\n{\n  \"delayTime\": 2624,\n  \"id\": \"c80ffee4-f315-4e25-a146-0f3d98cf024b\",\n  \"input\": {\n    \"prompt\": \"A cute fluffy white dog in the style of a Pixar animation 3D drawing.\"\n  },\n  \"status\": \"IN_PROGRESS\"\n}\n```","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":43,"to":64}}}}],["781",{"pageContent":"Get Completed Job Status\n\nOnce your job is complete, you'll receive a final response like this:\n\n```json\n{\n  \"delayTime\": 17158,\n  \"executionTime\": 4633,\n  \"id\": \"fb5a249d-12c7-48e5-a0e4-b813c3381262-22\",\n  \"output\": [\n    {\n      \"image\": \"base64image\",\n      \"seed\": 40264\n    }\n  ],\n  \"status\": \"COMPLETED\"\n}\n```\n\nTo save the output, use the following command:\n\n```json\ncurl https://api.runpod.ai/v2/{ID}/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer [Your API Key]' | jq . > output.json\n```\n\n:::note\n\nRemember, you have up to 1 hour to retrieve your results via the status endpoint for privacy reasons.\n\n:::","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":66,"to":97}}}}],["782",{"pageContent":"Get Your Results\n\nFinally, to view your results, decode the base64 image from the output. Here's how you can do it in Python:\n\n```python\nimport json\nimport base64\n\n\ndef decode_and_save_image(json_file_path, output_image_path):\n    try:\n        # Reading the JSON file\n        with open(json_file_path, \"r\") as file:\n            data = json.load(file)\n\n        # Extracting the base64 encoded image data\n        base64_image = data[\"output\"][0][\"image\"]\n\n        # Decode the Base64 string\n        decoded_image_data = base64.b64decode(base64_image)\n\n        # Writing the decoded data to an image file\n        with open(output_image_path, \"wb\") as image_file:\n            image_file.write(decoded_image_data)\n\n        print(f\"Image successfully decoded and saved as '{output_image_path}'.\")","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":99,"to":124}}}}],["783",{"pageContent":"except FileNotFoundError:\n        print(\n            \"File not found. Please ensure the JSON file exists in the specified path.\"\n        )\n    except KeyError as e:\n        print(f\"Error in JSON structure: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\n# Usage\njson_file_path = \"output.json\"  # Path to your JSON file\noutput_image_path = \"decoded_image.png\"  # Desired path for the output image\n\ndecode_and_save_image(json_file_path, output_image_path)","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":126,"to":140}}}}],["784",{"pageContent":"Congratulations! You've now successfully used RunPod's Stable Diffusion API to generate images.\n\n![](decoded_image.png)","metadata":{"source":"/runpod-docs/docs/tutorials/serverless/gpu/run-your-first.md","loc":{"lines":{"from":143,"to":145}}}}],["785",{"pageContent":"# Website\n\nThis website is built using [Docusaurus](https://docusaurus.io/), a modern static website generator.\n\n### Installation\n\n```\n$ yarn\n```\n\n### Local Development\n\n```\n$ yarn start\n```\n\nThis command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.\n\n### Build\n\n```\n$ yarn build\n```\n\nThis command generates static content into the `build` directory and can be served using any static contents hosting service.\n\n### Deployment\n\nUsing SSH:\n\n```\n$ USE_SSH=true yarn deploy\n```\n\nNot using SSH:\n\n```\n$ GIT_USER=<Your GitHub username> yarn deploy\n```\n\nIf you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.\n\n## Linting\n\nTo lint a specific folder or file, run:\n\n```command\nvale path/to/docs/\n# or\nvale path/to/*.md\n```\n\nTo lint the entire repo, run:\n\n```command\nyarn lint\n```","metadata":{"source":"/runpod-docs/README.md","loc":{"lines":{"from":1,"to":57}}}}],["786",{"pageContent":"Format Python code examples\n\nInstall `blacken-docs`.\n\n```bash\npython -m pip install blacken-docs\n```\n\nRun the formatter.\n\n```bash\ngit ls-files -z -- '*.md' | xargs -0 blacken-docs\n```","metadata":{"source":"/runpod-docs/README.md","loc":{"lines":{"from":59,"to":71}}}}]]